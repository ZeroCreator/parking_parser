import asyncio
import random
import re
import time
from typing import List, Dict, Any, Optional, Set
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse, parse_qs

from .base_parser import BaseParser


class YandexParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∫–æ–≤–æ–∫ –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ"""

    @property
    def source_name(self) -> str:
        return "yandex"

    async def parse(self) -> List[Dict[str, Any]]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç"""
        print("=" * 80)
        print("üöÄ –ü–ê–†–°–ï–† –Ø–ù–î–ï–ö–° –ö–ê–†–¢ - –°–ê–ù–ö–¢-–ü–ï–¢–ï–†–ë–£–†–ì")
        print("=" * 80)

        self.start_time = time.time()
        self.results = []
        self.all_urls.clear()

        if not await self.init_browser():
            return []

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–µ—Ç–∫—É –≤–º–µ—Å—Ç–æ —Ä—É—á–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
            search_areas = self.generate_grid_z14()

            # 1. –ü–∞—Ä—Å–∏–º –≤—Å–µ –æ–±–ª–∞—Å—Ç–∏ –≥–æ—Ä–æ–¥–∞
            print(f"\nüéØ –ù–ê–ß–ò–ù–ê–ï–ú –ü–ê–†–°–ò–ù–ì {len(search_areas)} –ê–í–¢–û–ó–û–ù (z=14)...")

            for i, area in enumerate(search_areas, 1):
                urls_before = len(self.all_urls)

                print(f"\nüìç –ó–æ–Ω–∞ {i}/{len(search_areas)}: {area['name']}")
                print(f"   –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã: {area['coords'][1]:.4f}¬∞N, {area['coords'][0]:.4f}¬∞E")
                print(f"   URL: {area['url']}")

                page = await self.browser.get(area['url'])
                await asyncio.sleep(4)

                # –ö–ª–∏–∫–∞–µ–º –∫–Ω–æ–ø–∫—É "–ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã", –µ—Å–ª–∏ –µ—Å—Ç—å
                button = await page.query_selector('span.search-command-view__show-results-button')
                if button:
                    print("‚úÖ –ö–Ω–æ–ø–∫–∞ –Ω–∞–π–¥–µ–Ω–∞, –∫–ª–∏–∫–∞–µ–º...")
                    await button.click()
                    await asyncio.sleep(3)
                    print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

                # –°–∫—Ä–∞–ø–∏–º —ç—Ç—É –æ–±–ª–∞—Å—Ç—å
                await self._scroll_and_collect_urls(page)

                new_urls = len(self.all_urls) - urls_before
                print(f"‚úÖ –í –æ–±–ª–∞—Å—Ç–∏ –Ω–∞–π–¥–µ–Ω–æ –ø–∞—Ä–∫–æ–≤–æ–∫: {new_urls}")
                print(f"\n‚úÖ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫–∏: {len(self.all_urls)}")

                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –æ–±–ª–∞—Å—Ç—è–º–∏
                if i < len(search_areas):
                    await asyncio.sleep(random.uniform(5, 8))

            if not self.all_urls:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Å—ã–ª–∫–∏")
                return []

            print(f"\n‚úÖ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫–∏: {len(self.all_urls)}")

            # 2. –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é –ø–∞—Ä–∫–æ–≤–∫—É
            print("\nüè¢ –ü–ê–†–°–ò–ú –î–ê–ù–ù–´–ï –ü–ê–†–ö–û–í–û–ö...")
            urls_list = list(self.all_urls)

            for i, url in enumerate(urls_list, 1):
                print(f"   {i}/{len(urls_list)}: {url}")
                data = await self.parse_parking_page(url)
                if data:
                    self.results.append(data)
                    print(f"      ‚úÖ –ü–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: {data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä–∫–æ–≤–∫–∏', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")
                else:
                    print(f"      ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")

                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if i < len(urls_list):
                    await asyncio.sleep(random.uniform(3, 5))

            # 3. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            self._remove_duplicates()

            # 4. –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._print_final_stats(len(self.all_urls))

            return self.results

        except Exception as e:
            print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            import traceback
            traceback.print_exc()
            return self.results
        finally:
            await self.close()

    def generate_grid_z14(self) -> List[Dict[str, str]]:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–µ—Ç–∫—É –∑–æ–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (z=14).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –¥–ª—è –ø–æ–∏—Å–∫–∞, –ø–æ–∫—Ä—ã–≤–∞—é—â–∏—Ö –≤–µ—Å—å –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥.
        """
        # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–∞
        # (—à–∏—Ä–æ—Ç–∞ lat, –¥–æ–ª–≥–æ—Ç–∞ lon)
        # –≠—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –Ω–µ–º–Ω–æ–≥–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ö–≤–∞—Ç–∞
        LAT_MIN, LAT_MAX = 59.80, 60.05  # –°–µ–≤–µ—Ä-–Æ–≥
        LON_MIN, LON_MAX = 29.60, 30.70  # –ó–∞–ø–∞–¥-–í–æ—Å—Ç–æ–∫

        # 2. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —à–∞–≥ —Å–µ—Ç–∫–∏ –¥–ª—è z=14
        # –ü—Ä–∏ z=14 sspn ~0.04-0.05 –≥—Ä–∞–¥—É—Å–∞, –¥–µ–ª–∞–µ–º —à–∞–≥ –Ω–µ–º–Ω–æ–≥–æ –º–µ–Ω—å—à–µ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        # –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏—Ç –ø–æ–ª–Ω—ã–π –æ—Ö–≤–∞—Ç –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        LAT_STEP = 0.04  # ~4.4 –∫–º
        LON_STEP = 0.06  # ~3.8 –∫–º –Ω–∞ —à–∏—Ä–æ—Ç–µ –°–ü–±

        zones = []
        zone_counter = 1

        # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å–µ—Ç–∫–∏
        lat = LAT_MIN
        while lat < LAT_MAX:
            lon = LON_MIN
            while lon < LON_MAX:
                # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∫–æ–≤–æ–∫ –≤ —ç—Ç–æ–π –∑–æ–Ω–µ
                url = (f"https://yandex.ru/maps/2/saint-petersburg/search/–ø–∞—Ä–∫–æ–≤–∫–∏/"
                       f"?l=carparks&ll={lon:.6f}%2C{lat:.6f}&z=14")

                zones.append({
                    "name": f"–ê–≤—Ç–æ–∑–æ–Ω–∞ {zone_counter}",
                    "url": url,
                    "coords": (lon, lat)  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏
                })

                zone_counter += 1
                lon += LON_STEP
            lat += LAT_STEP

        print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(zones)} –∑–æ–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (z=14)")
        print(f"üìê –®–∞–≥ —Å–µ—Ç–∫–∏: {LON_STEP:.3f}¬∞ (–¥–æ–ª–≥–æ—Ç–∞) √ó {LAT_STEP:.3f}¬∞ (—à–∏—Ä–æ—Ç–∞)")
        print(f"üìç –û—Ö–≤–∞—Ç—ã–≤–∞–µ–º–∞—è –æ–±–ª–∞—Å—Ç—å: {LAT_MIN}-{LAT_MAX}¬∞N, {LON_MIN}-{LON_MAX}¬∞E")

        return zones

    async def _scroll_and_collect_urls(self, page):
        """–°–∫—Ä–æ–ª–ª–∏–Ω–≥ –∏ —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏"""
        max_scrolls = 30
        no_new_count = 0
        previous_count = len(self.all_urls)

        for scroll_num in range(1, max_scrolls + 1):
            print(f"   üìç –°–∫—Ä–æ–ª–ª {scroll_num}/{max_scrolls}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            current_count_before = len(self.all_urls)

            # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ –¥–ª—è –Ø–Ω–¥–µ–∫—Å
            await self._yandex_specific_scroll(page)
            await asyncio.sleep(random.uniform(1.5, 2.5))

            # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏
            html_content = await page.evaluate("document.documentElement.outerHTML")
            urls_before = len(self.all_urls)
            self._extract_urls_from_html(html_content)
            new_urls = len(self.all_urls) - urls_before

            if new_urls > 0:
                print(f"   üì• –ù–æ–≤—ã—Ö –ø–∞—Ä–∫–æ–≤–æ–∫: {new_urls}")
                no_new_count = 0
            else:
                no_new_count += 1
                print(f"   üì≠ –ù–æ–≤—ã—Ö –ø–∞—Ä–∫–æ–≤–æ–∫ –Ω–µ—Ç ({no_new_count}/3)")

                if no_new_count >= 3:
                    print("   üèÅ –ó–∞–≤–µ—Ä—à–∞–µ–º —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏")
                    break

            # –ï—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è 3 —Ä–∞–∑–∞ –ø–æ–¥—Ä—è–¥ - –≤—ã—Ö–æ–¥–∏–º
            if len(self.all_urls) == previous_count:
                no_new_count += 1
            else:
                no_new_count = 0

            previous_count = len(self.all_urls)

            # –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞
            await asyncio.sleep(random.uniform(0.5, 1))

    async def _yandex_specific_scroll(self, page):
        """–°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ –¥–ª—è –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç"""
        try:
            await page.evaluate("""
                (function() {
                    const selectors = [
                        '.scroll__container_width_narrow',
                        '.scroll__container',
                        '.sidebar-view__panel',
                        '.search-list-view__list-container',
                        '.search-list-view__list'
                    ];

                    let scrolled = false;
                    for (const selector of selectors) {
                        const container = document.querySelector(selector);
                        if (container && container.scrollHeight > container.clientHeight) {
                            container.scrollTop = container.scrollHeight;
                            scrolled = true;
                            break;
                        }
                    }

                    window.scrollBy({
                        top: 800,
                        behavior: 'smooth'
                    });

                    return { containerScrolled: scrolled };
                })();
            """)
        except Exception as e:
            print(f"   ‚ö† –û—à–∏–±–∫–∞ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞: {e}")

    def _normalize_url(self, url: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫—É"""
        if not url:
            return ""

        # –°–ø–∏—Å–æ–∫ –≤–∫–ª–∞–¥–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –æ–±—Ä–µ–∑–∞—Ç—å
        tabs_to_remove = ['/reviews', '/photos', '/gallery', '/menu']

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–º–µ–Ω –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if url.startswith('//'):
            url = f"https:{url}"
        elif url.startswith('/'):
            url = f"https://yandex.ru{url}"
        elif not url.startswith('http'):
            return ""

        # –£–¥–∞–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∏ —è–∫–æ—Ä—è
        url = url.split('?')[0].split('#')[0].strip()

        # –û–±—Ä–µ–∑–∞–µ–º –≤–∫–ª–∞–¥–∫–∏ (reviews, photos, gallery, menu)
        for tab in tabs_to_remove:
            if tab in url:
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –≤–∫–ª–∞–¥–∫–∏ –∏ –æ–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω–µ—ë
                tab_index = url.find(tab)
                if tab_index != -1:
                    url = url[:tab_index]

        # –£–¥–∞–ª—è–µ–º –∫–æ–Ω–µ—á–Ω—ã–µ —Å–ª–µ—à–∏
        url = url.rstrip('/')

        return url

    def _extract_urls_from_html(self, html_content: str):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫–∏ –∏–∑ HTML"""
        try:
            urls_before = len(self.all_urls)

            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
            org_pattern = r'href="(/maps/org/[^"]+)"'
            all_link_matches = re.findall(org_pattern, html_content)

            for link in all_link_matches:
                full_url = f"https://yandex.ru{link}"
                clean_url = self._normalize_url(full_url)
                if clean_url:
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                    if not any(exclude in clean_url.lower() for exclude in ['/reviews/', '/photos/', '/gallery/', '/menu/']):
                        self.all_urls.add(clean_url)

            # –ò—â–µ–º –≤ –∫–∞—Ä—Ç–æ—á–∫–∞—Ö
            snippet_pattern = r'<li[^>]*class="[^"]*search-snippet-view[^"]*"[^>]*>.*?</li>'
            snippets = re.findall(snippet_pattern, html_content, re.DOTALL)

            for snippet in snippets:
                link_match = re.search(org_pattern, snippet)
                if link_match:
                    link = link_match.group(1)
                    full_url = f"https://yandex.ru{link}"
                    clean_url = self._normalize_url(full_url)
                    if clean_url and not any(exclude in clean_url.lower() for exclude in ['/reviews/', '/photos/', '/gallery/', '/menu/']):
                        self.all_urls.add(clean_url)

            new_urls = len(self.all_urls) - urls_before
            if new_urls > 0:
                print(f"   üì• –ò–∑–≤–ª–µ—á–µ–Ω–æ {new_urls} –Ω–æ–≤—ã—Ö URL")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL: {e}")

    async def parse_parking_page(self, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∫–æ–≤–∫–∏"""
        try:
            print(f"      üìñ –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–∞—Ä–∫–æ–≤–∫–∏...")
            page = await self.browser.get(url)
            await asyncio.sleep(random.uniform(3, 4))

            # –ü–æ–ª—É—á–∞–µ–º HTML
            html_content = await page.evaluate("document.documentElement.outerHTML")
            soup = BeautifulSoup(html_content, 'html.parser')

            # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ
            data = self._extract_page_data(url, soup, html_content)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–∞—Ä–∫–æ–≤–∫–∞ –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ
            address = data.get('–ê–¥—Ä–µ—Å', '')
            if address:
                address_lower = address.lower()
                spb_patterns = [
                    '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥',
                    '—Å–ø–±',
                    '–≥.—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥',
                    '–≥. —Å–ø–±',
                    '–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥',
                    '–≥.–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥'
                ]

                is_spb = any(pattern in address_lower for pattern in spb_patterns)
                if not is_spb:
                    print(f"      üö´ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∫–æ–≤–∫—É (–Ω–µ –∏–∑ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–∞): {address}")
                    return None

            return data if data else None

        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return None

    def _extract_page_data(self, url: str, soup: BeautifulSoup, html: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∫–æ–≤–∫–∏ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –¥–ª—è –Ø–Ω–¥–µ–∫—Å)"""
        data = {
            'source': 'yandex',
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            '–°—Å—ã–ª–∫–∞': url,
            '–°—Å—ã–ª–∫–∞ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫—É': url
        }

        # 1. –ù–∞–∑–≤–∞–Ω–∏–µ
        title_selectors = [
            'h1',
            '.orgpage-header-view__header',
            '.business-title',
            '.card-title-view__title',
            '[itemprop="name"]'
        ]

        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞'] = text
                    data['–ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä–∫–æ–≤–∫–∏'] = text
                    break

        # 2. –ê–¥—Ä–µ—Å
        address_selectors = [
            '[itemprop="address"]',
            '.business-contacts-view__address',
            '.card-address-view__address',
            '.orgpage-address-view__address-text',
            'address'
        ]

        for selector in address_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–ê–¥—Ä–µ—Å'] = text
                    data['–ê–¥—Ä–µ—Å –ø–∞—Ä–∫–æ–≤–∫–∏'] = text
                    break

        # 3. –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        coords = self._extract_yandex_coordinates(url, soup)
        if coords:
            data['–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã'] = coords

        # 4. –¢–µ–ª–µ—Ñ–æ–Ω
        phones = []
        phone_links = soup.find_all('a', href=re.compile(r'^tel:'))
        for link in phone_links:
            phone = link.get('href', '').replace('tel:', '').strip()
            if phone:
                clean_phone = re.sub(r'[^\d+]', '', phone)
                if clean_phone and clean_phone not in phones:
                    phones.append(clean_phone)

        if phones:
            data['–¢–µ–ª–µ—Ñ–æ–Ω'] = ', '.join(phones)

        # 5. –°–∞–π—Ç
        site_selectors = [
            '.business-urls-view__link',
            '.card-website-view__link',
            '.orgpage-url-view__url'
        ]

        for selector in site_selectors:
            elem = soup.select_one(selector)
            if elem:
                href = elem.get('href', '')
                if href and 'yandex.ru' not in href:
                    data['–°–∞–π—Ç'] = href
                    break

        # 6. –¢–∏–ø –æ–±—ä–µ–∫—Ç–∞
        category_selectors = [
            '.business-categories-view__category',
            '.card-categories-view__category',
            '.orgpage-categories-view__category',
            '[itemprop="category"]'
        ]

        for selector in category_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–¢–∏–ø –æ–±—ä–µ–∫—Ç–∞'] = text
                    break

        # 7. –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã
        hours_selectors = [
            '.business-feature-view__schedule-days',
            '.card-schedule-view__schedule',
            '.orgpage-working-view__working-days',
            '[itemprop="openingHours"]'
        ]

        for selector in hours_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text and (':' in text or '—á–∞—Å' in text.lower()):
                    data['–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã'] = text
                    break

        # 8. –†–µ–π—Ç–∏–Ω–≥ –∏ –æ—Ü–µ–Ω–∫–∏
        rating_selectors = [
            '.business-rating-badge-view__rating-text',
            '.card-rating-view__rating',
            '.orgpage-rating-view__rating',
            '[itemprop="ratingValue"]'
        ]

        for selector in rating_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–û—Ü–µ–Ω–∫–∞'] = text
                    break

        reviews_selectors = [
            '.business-rating-badge-view__rating-count',
            '.card-rating-view__reviews-count',
            '.orgpage-reviews-view__reviews-count',
            '[itemprop="reviewCount"]'
        ]

        for selector in reviews_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫'] = text
                    break

        # 9. –¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏
        parking_type = self._detect_yandex_parking_type(soup, data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', ''), html)
        data['–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏'] = parking_type

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø
        if '–∑–∞–∫—Ä—ã—Ç' in parking_type.lower() or '–æ—Ö—Ä–∞–Ω—è' in parking_type.lower():
            data['–î–æ—Å—Ç—É–ø'] = '–ó–∞–∫—Ä—ã—Ç—ã–π'
        else:
            data['–î–æ—Å—Ç—É–ø'] = '–û—Ç–∫—Ä—ã—Ç—ã–π'

        # 10. –¶–µ–Ω—ã
        page_text = soup.get_text()
        price_matches = re.findall(r'(\d+\s*—Ä—É–±|\d+\s*‚ÇΩ|\d+\s*–≤ —á–∞—Å|\d+\s*–≤ —Å—É—Ç–∫–∏)', page_text, re.IGNORECASE)
        if price_matches:
            data['–¶–µ–Ω—ã'] = price_matches[0]
            data['–¢–∞—Ä–∏—Ñ—ã'] = '; '.join(price_matches[:3])

        # 11. –í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
        capacity_match = re.search(r'(\d+)\s*–º–µ—Å—Ç|\b–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å\s*(\d+)', page_text, re.IGNORECASE)
        if capacity_match:
            capacity = capacity_match.group(1) or capacity_match.group(2)
            data['–í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å'] = capacity

        # 12. –û–ø–∏—Å–∞–Ω–∏–µ
        desc_selectors = [
            '.business-description-view__description',
            '.card-description-view__description',
            '.orgpage-description-view__description',
            '[itemprop="description"]'
        ]

        for selector in desc_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–û–ø–∏—Å–∞–Ω–∏–µ'] = text[:500]
                    break

        return data

    def _extract_yandex_coordinates(self, url: str, soup: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –¥–ª—è –Ø–Ω–¥–µ–∫—Å"""
        # –ò–∑ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤
        meta_coords = soup.find('meta', attrs={'name': 'coordinates'})
        if meta_coords:
            coords = meta_coords.get('content', '')
            if coords:
                return coords

        # –ò–∑ URL
        parsed = urlparse(url)
        if 'll=' in url:
            params = parse_qs(parsed.query)
            if 'll' in params:
                ll_parts = params['ll'][0].split('%2C')
                if len(ll_parts) == 2:
                    # –Ø–Ω–¥–µ–∫—Å: –¥–æ–ª–≥–æ—Ç–∞,—à–∏—Ä–æ—Ç–∞ -> –º–µ–Ω—è–µ–º –Ω–∞ —à–∏—Ä–æ—Ç–∞,–¥–æ–ª–≥–æ—Ç–∞
                    return f"{ll_parts[1]},{ll_parts[0]}"

        # –ò–∑ data-–∞—Ç—Ä–∏–±—É—Ç–æ–≤
        coord_elem = soup.find(attrs={'data-coordinates': True})
        if coord_elem:
            coords = coord_elem.get('data-coordinates')
            if coords:
                return coords

        return None

    def _detect_yandex_parking_type(self, soup: BeautifulSoup, name: str, html: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø–∞—Ä–∫–æ–≤–∫–∏ –¥–ª—è –Ø–Ω–¥–µ–∫—Å"""
        text = (name + ' ' + soup.get_text()).lower()
        type_info = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–ª–∞—Ç–Ω–æ—Å—Ç–∏
        if any(word in text for word in ['–ø–ª–∞—Ç–Ω', '–æ–ø–ª–∞—Ç', '—Ç–∞—Ä–∏—Ñ', '—Ü–µ–Ω–∞', '‚ÇΩ', '—Ä—É–±']):
            type_info.append('–ø–ª–∞—Ç–Ω–∞—è')
        elif any(word in text for word in ['–±–µ—Å–ø–ª–∞—Ç–Ω', 'free', 'gratis']):
            type_info.append('–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è')

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞
        if any(word in text for word in ['–∫—Ä—ã—Ç', '–∑–∞–∫—Ä—ã—Ç', '–æ—Ö—Ä–∞–Ω—è', '–ø–æ–¥–∑–µ–º–Ω']):
            type_info.append('–∫—Ä—ã—Ç–∞—è')
            type_info.append('–æ—Ö—Ä–∞–Ω—è–µ–º–∞—è')
        elif any(word in text for word in ['—É–ª–∏—á–Ω', '–æ—Ç–∫—Ä—ã—Ç', '–≥–æ—Å—Ç']):
            type_info.append('—É–ª–∏—á–Ω–∞—è')

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if any(word in text for word in ['—Ç–æ—Ä–≥–æ–≤', '—Ç—Ü', '–º–æ–ª–ª', '–≥–∞–ª–µ—Ä–µ—è']):
            type_info.append('–ø—Ä–∏ —Ç—Ü')
        elif any(word in text for word in ['–æ—Ñ–∏c', '–±–∏–∑–Ω–µ—Å', '—Ü–µ–Ω—Ç—Ä']):
            type_info.append('–±–∏–∑–Ω–µ—Å-—Ü–µ–Ω—Ç—Ä')

        return ", ".join(type_info) if type_info else "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

    def _remove_duplicates(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–º—É ID"""
        if not self.results:
            return

        unique_results = []
        seen = set()

        for item in self.results:
            # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Å—Å—ã–ª–∫–∏
            url = item.get('–°—Å—ã–ª–∫–∞', '').lower().strip()

            if url:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –∏–∑ URL
                # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ ID: /org/–Ω–∞–∑–≤–∞–Ω–∏–µ/ID/
                match = re.search(r'/(\d+)(?:/|$)', url)
                if match:
                    unique_id = match.group(1)
                    if unique_id not in seen:
                        seen.add(unique_id)
                        unique_results.append(item)
                elif url not in seen:
                    seen.add(url)
                    unique_results.append(item)
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç URL, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –∞–¥—Ä–µ—Å
                name = item.get('–ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä–∫–æ–≤–∫–∏', '').lower().strip()
                address = item.get('–ê–¥—Ä–µ—Å', '').lower().strip()

                if name and address:
                    key = f"{name}|{address}"
                    if key not in seen:
                        seen.add(key)
                        unique_results.append(item)
                elif name:
                    if name not in seen:
                        seen.add(name)
                        unique_results.append(item)
                elif address:
                    if address not in seen:
                        seen.add(address)
                        unique_results.append(item)
                else:
                    unique_results.append(item)

        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"üóë –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {removed}")

        self.results = unique_results

    def _print_final_stats(self, total_urls: int):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–±–æ—Ä–∞"""
        print("\n" + "=" * 80)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ë–û–†–ê")
        print("=" * 80)

        elapsed_time = time.time() - self.start_time
        minutes = int(elapsed_time // 60)
        seconds = int(elapsed_time % 60)

        print(f"‚è± –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {minutes} –º–∏–Ω {seconds} —Å–µ–∫")
        print(f"üîó –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {total_urls}")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(self.results)}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º
        phones_count = sum(1 for r in self.results if r.get('–¢–µ–ª–µ—Ñ–æ–Ω'))
        sites_count = sum(1 for r in self.results if r.get('–°–∞–π—Ç'))
        coords_count = sum(1 for r in self.results if r.get('–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã'))
        prices_count = sum(1 for r in self.results if r.get('–¶–µ–Ω—ã'))

        print(f"üìû –ü–∞—Ä–∫–æ–≤–æ–∫ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–æ–º: {phones_count}")
        print(f"üåê –ü–∞—Ä–∫–æ–≤–æ–∫ —Å —Å–∞–π—Ç–æ–º: {sites_count}")
        print(f"üìç –ü–∞—Ä–∫–æ–≤–æ–∫ —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {coords_count}")
        print(f"üí∞ –ü–∞—Ä–∫–æ–≤–æ–∫ —Å —Ü–µ–Ω–∞–º–∏: {prices_count}")

        # –¢–∏–ø—ã –ø–∞—Ä–∫–æ–≤–æ–∫
        types = {}
        for r in self.results:
            parking_type = r.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            types[parking_type] = types.get(parking_type, 0) + 1

        print("\nüè¢ –¢–ò–ü–´ –ü–ê–†–ö–û–í–û–ö:")
        for type_name, count in sorted(types.items(), key=lambda x: x[1], reverse=True):
            print(f"   {type_name}: {count}")

        print("\n" + "=" * 80)

    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤"""
        if not text:
            return ""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        return text
