import asyncio
import random
import re
import time
import json
from typing import List, Dict, Any, Optional, Set
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse, parse_qs

from .base_parser import BaseParser


class YandexParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∫–æ–≤–æ–∫ –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ"""

    def __init__(self, headless: bool = True):
        super().__init__(headless)
        self.start_time = None
        self.all_parking_urls: Set[str] = set()
        self.max_consecutive_no_new = 3  # –ú–∞–∫—Å–∏–º—É–º 3 –ø–æ–ø—ã—Ç–∫–∏ –±–µ–∑ –Ω–æ–≤—ã—Ö URL

    @property
    def source_name(self) -> str:
        return "yandex"

    async def parse(self) -> List[Dict[str, Any]]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç"""
        print("=" * 60)
        print("üöÄ –ó–ê–ü–£–°–ö –ü–ê–†–°–ï–†–ê –Ø–ù–î–ï–ö–° –ö–ê–†–¢")
        print("=" * 60)

        self.start_time = time.time()
        self.results = []
        self.all_parking_urls.clear()

        if not await self.init_browser():
            return []

        try:
            # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–∏—Å–∫–∞ –∏ –∫–ª–∏–∫–∞–µ–º –∫–Ω–æ–ø–∫—É
            print("\nüìÑ –ó–ê–ì–†–£–ó–ö–ê –°–¢–†–ê–ù–ò–¶–´ –ò –ö–õ–ò–ö –ö–ù–û–ü–ö–ò")
            print("-" * 50)

            search_url = "https://yandex.ru/maps/2/saint-petersburg/search/–ø–∞—Ä–∫–æ–≤–∫–∏/"
            print(f"üîó URL: {search_url}")

            page = await self.browser.get(search_url)
            await asyncio.sleep(3)

            # –ö–ª–∏–∫–∞–µ–º –∫–Ω–æ–ø–∫—É "–ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"
            button = await page.query_selector('span.search-command-view__show-results-button')
            if button:
                print("‚úÖ –ö–Ω–æ–ø–∫–∞ –Ω–∞–π–¥–µ–Ω–∞, –∫–ª–∏–∫–∞–µ–º...")
                await button.click()
                await asyncio.sleep(3)
                print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            else:
                print("‚ö†Ô∏è –ö–Ω–æ–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")

            # 2. –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
            print("\n‚è± –û–ñ–ò–î–ê–ù–ò–ï –ó–ê–ì–†–£–ó–ö–ò –î–ê–ù–ù–´–•")
            print("-" * 50)
            await asyncio.sleep(5)

            # 3. –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å–æ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–æ–º
            print("\nüîó –°–ë–û–† –°–°–´–õ–û–ö –°–û –°–ö–†–û–õ–õ–ò–ù–ì–û–ú")
            print("-" * 50)

            # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –±–µ–∑ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞
            html_content = await page.evaluate("document.documentElement.outerHTML")
            self._extract_urls_from_html(html_content)

            # –ó–∞—Ç–µ–º –≤—ã–ø–æ–ª–Ω—è–µ–º —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ –∏ —Å–æ–±–∏—Ä–∞–µ–º –±–æ–ª—å—à–µ —Å—Å—ã–ª–æ–∫
            await self._scroll_and_collect_urls(page)

            if not self.all_parking_urls:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Å—ã–ª–∫–∏")
                return []

            print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(self.all_parking_urls)}")

            # 4. –ü–∞—Ä—Å–∏–º –í–°–ï –ø–∞—Ä–∫–æ–≤–∫–∏
            print("\nüè¢ –î–ï–¢–ê–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì –ü–ê–†–ö–û–í–û–ö")
            print("-" * 50)

            urls_to_parse = list(self.all_parking_urls)
            print(f"üìä –ë—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å {len(urls_to_parse)} –ø–∞—Ä–∫–æ–≤–æ–∫")

            await self._parse_all_parking_pages(urls_to_parse)

            # 5. –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._print_final_stats()

            return self.results

        except Exception as e:
            print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            import traceback
            traceback.print_exc()
            return self.results
        finally:
            await self.close()

    async def _scroll_and_collect_urls(self, page):
        """–°–∫—Ä–æ–ª–ª–∏–Ω–≥ –ø–∞–Ω–µ–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫"""
        print("–ù–∞—á–∏–Ω–∞–µ–º —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö –ø–∞—Ä–∫–æ–≤–æ–∫...")

        max_scroll_attempts = 100
        consecutive_no_new = 0
        total_new_urls = 0
        last_new_urls_count = 0

        for attempt in range(1, max_scroll_attempts + 1):
            print(f"\n   üîÑ –ü–æ–ø—ã—Ç–∫–∞ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞ {attempt}/{max_scroll_attempts}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –¥–æ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞
            before_scroll_count = len(self.all_parking_urls)

            # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç—ã
            await self._yandex_specific_scroll(page)

            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—É–º–µ–Ω—å—à–∏–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è)
            wait_time = random.uniform(2, 3)
            print(f"   ‚è± –ñ–¥–µ–º {wait_time:.1f} —Å–µ–∫...")
            await asyncio.sleep(wait_time)

            # –ü–æ–ª—É—á–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π HTML
            html_content = await page.evaluate("document.documentElement.outerHTML")

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–≤—ã–µ URL
            new_urls_count_before = len(self.all_parking_urls)
            self._extract_urls_from_html(html_content)
            new_urls_count_after = len(self.all_parking_urls)
            new_urls_added = new_urls_count_after - new_urls_count_before

            print(f"   üìä URL –¥–æ: {before_scroll_count}, –¥–æ–±–∞–≤–ª–µ–Ω–æ: {new_urls_added}, –≤—Å–µ–≥–æ: {new_urls_count_after}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –Ω–æ–≤—ã–µ URL
            if new_urls_added > 0:
                consecutive_no_new = 0
                total_new_urls += new_urls_added
                last_new_urls_count = new_urls_added
                print(f"   ‚úÖ –î–æ–±–∞–≤–∏–ª–æ—Å—å {new_urls_added} –Ω–æ–≤—ã—Ö URL")

                # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–æ –º–∞–ª–æ URL, –≤–æ–∑–º–æ–∂–Ω–æ –∫–æ–Ω–µ—Ü –±–ª–∏–∑–æ–∫
                if new_urls_added < 5:
                    print(f"   ‚ö† –ú–∞–ª–æ –Ω–æ–≤—ã—Ö URL ({new_urls_added}), –≤–æ–∑–º–æ–∂–Ω–æ —Å–∫–æ—Ä–æ –∫–æ–Ω–µ—Ü")
            else:
                consecutive_no_new += 1
                print(f"   ‚ö† –ù–æ–≤—ã—Ö URL –Ω–µ—Ç (–ø–æ–ø—ã—Ç–æ–∫ –±–µ–∑ –Ω–æ–≤—ã—Ö: {consecutive_no_new}/{self.max_consecutive_no_new})")

                # –ï—Å–ª–∏ 3 —Ä–∞–∑–∞ –ø–æ–¥—Ä—è–¥ –Ω–µ—Ç –Ω–æ–≤—ã—Ö URL - –∑–∞–≤–µ—Ä—à–∞–µ–º
                if consecutive_no_new >= self.max_consecutive_no_new:
                    print(f"   üèÅ –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ - {self.max_consecutive_no_new} —Ä–∞–∑–∞ –ø–æ–¥—Ä—è–¥ –Ω–µ—Ç –Ω–æ–≤—ã—Ö URL")
                    break

            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ü–∞ (–±–µ–∑ –¥–æ–ª–≥–æ–≥–æ JavaScript)
            is_end = await self._quick_check_end(page)
            if is_end:
                print("   üèÅ –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü —Å–ø–∏—Å–∫–∞")
                break

            # –ï—Å–ª–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –ø–æ–ø—ã—Ç–∫–∏ –¥–æ–±–∞–≤–∏–ª–æ—Å—å –º–∞–ª–æ URL, –≤–æ–∑–º–æ–∂–Ω–æ –∫–æ–Ω–µ—Ü
            if attempt > 2 and last_new_urls_count < 3:
                print(f"   ‚ö† –í –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–ø—ã—Ç–∫–µ –º–∞–ª–æ –Ω–æ–≤—ã—Ö URL ({last_new_urls_count}), –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω–µ—Ü...")
                is_loading = await self._check_if_loading(page)
                if not is_loading:
                    print("   üèÅ –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ –∞–∫—Ç–∏–≤–Ω–∞ –∏ –º–∞–ª–æ –Ω–æ–≤—ã—Ö URL - –∑–∞–≤–µ—Ä—à–∞–µ–º")
                    break

            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å–∫—Ä–æ–ª–ª–∞–º–∏
            await asyncio.sleep(random.uniform(0.5, 1.5))

        print(f"\n‚úÖ –°–∫—Ä–æ–ª–ª–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω –ø–æ—Å–ª–µ {min(attempt, max_scroll_attempts)} –ø–æ–ø—ã—Ç–æ–∫")
        print(f"üìä –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö URL: {total_new_urls}")
        print(f"üìä –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫: {len(self.all_parking_urls)}")

    async def _yandex_specific_scroll(self, page):
        """–°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ –¥–ª—è –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç"""
        try:
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∏ —Å–∫—Ä–æ–ª–ª–∏—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
            await page.evaluate("""
                (function() {
                    // –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
                    const selectors = [
                        '.scroll__container_width_narrow',
                        '.scroll__container',
                        '.sidebar-view__panel',
                        '.search-list-view__list-container',
                        '.search-list-view__list'
                    ];

                    let scrolled = false;

                    for (const selector of selectors) {
                        const container = document.querySelector(selector);
                        if (container && container.scrollHeight > container.clientHeight) {
                            const oldScroll = container.scrollTop;
                            container.scrollTop = container.scrollHeight;

                            // –ü—Ä–æ–±—É–µ–º –ø–ª–∞–≤–Ω—ã–π —Å–∫—Ä–æ–ª–ª–∏–Ω–≥
                            setTimeout(() => {
                                container.scrollTo({
                                    top: container.scrollHeight,
                                    behavior: 'smooth'
                                });
                            }, 100);

                            scrolled = container.scrollTop > oldScroll;
                            if (scrolled) {
                                console.log('–°–∫—Ä–æ–ª–ª–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä:', selector);
                                break;
                            }
                        }
                    }

                    // –í—Å–µ–≥–¥–∞ —Å–∫—Ä–æ–ª–ª–∏–º –æ–∫–Ω–æ
                    const oldWindowScroll = window.pageYOffset;
                    window.scrollBy({
                        top: 800,
                        behavior: 'smooth'
                    });

                    return {
                        containerScrolled: scrolled,
                        windowScrolled: window.pageYOffset > oldWindowScroll
                    };
                })();
            """)

        except Exception as e:
            print(f"   ‚ö† –û—à–∏–±–∫–∞ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞: {e}")

    async def _quick_check_end(self, page):
        """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ü–∞ —Å–ø–∏—Å–∫–∞"""
        try:
            result = await page.evaluate("""
                (function() {
                    // –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ç–µ–∫—Å—Ç—É
                    const bodyText = document.body.innerText.toLowerCase();
                    const endKeywords = [
                        '–ø–æ–∫–∞–∑–∞–Ω—ã –≤—Å–µ',
                        '–±–æ–ª—å—à–µ –Ω–µ—Ç',
                        '–∫–æ–Ω–µ—Ü —Å–ø–∏—Å–∫–∞',
                        'all results shown',
                        'no more results'
                    ];

                    for (const keyword of endKeywords) {
                        if (bodyText.includes(keyword)) {
                            return true;
                        }
                    }

                    return false;
                })();
            """)

            return bool(result)

        except Exception as e:
            return False

    async def _check_if_loading(self, page):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–¥–µ—Ç –ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            result = await page.evaluate("""
                (function() {
                    // –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–∏–Ω–Ω–µ—Ä–æ–≤
                    const spinners = document.querySelectorAll('.spin2, .spinner, .loading, .loader');
                    for (const spinner of spinners) {
                        const style = getComputedStyle(spinner);
                        if (style.display !== 'none' && style.visibility !== 'hidden') {
                            return true;
                        }
                    }
                    return false;
                })();
            """)

            return bool(result)

        except Exception as e:
            print(f"   ‚ö† –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return False

    def _extract_urls_from_html(self, html_content: str):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫–∏ –∏–∑ HTML"""
        try:
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
            org_pattern = r'href="(/maps/org/[^"]+)"'
            snippet_pattern = r'<li[^>]*class="[^"]*search-snippet-view[^"]*"[^>]*>.*?</li>'

            # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏
            snippets = re.findall(snippet_pattern, html_content, re.DOTALL)

            urls_before = len(self.all_parking_urls)

            for snippet in snippets:
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É
                link_match = re.search(org_pattern, snippet)
                if link_match:
                    link = link_match.group(1)
                    if link:
                        full_url = f"https://yandex.ru{link}"
                        # –û—á–∏—â–∞–µ–º URL –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                        clean_url = full_url.split('?')[0].split('#')[0]
                        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                        if not any(exclude in clean_url.lower() for exclude in ['/reviews/', '/photos/', '/gallery/']):
                            self.all_parking_urls.add(clean_url)

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –≤–æ –≤—Å–µ–º HTML
            all_link_matches = re.findall(r'href="(/maps/org/[^/"]+/[^/"]*)"', html_content)
            for link in all_link_matches:
                full_url = f"https://yandex.ru{link}"
                clean_url = full_url.split('?')[0].split('#')[0]
                if not any(exclude in clean_url.lower() for exclude in ['/reviews/', '/photos/', '/gallery/']):
                    self.all_parking_urls.add(clean_url)

            urls_after = len(self.all_parking_urls)
            new_urls = urls_after - urls_before

            if new_urls > 0:
                print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {new_urls} –Ω–æ–≤—ã—Ö URL")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL: {e}")
            import traceback
            traceback.print_exc()

    # –£–î–ê–õ–ò–ú –ù–ï–ù–£–ñ–ù–´–ï –ú–ï–¢–û–î–´:
    # - _aggressive_yandex_scroll (—Å–ª–∏—à–∫–æ —Å–ª–æ–∂–Ω—ã–π)
    # - _fallback_scroll (—É–ø—Ä–æ—Å—Ç–∏–º)
    # - _check_end_of_results (—Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω—ã–π, –∑–∞–º–µ–Ω–∏–º –Ω–∞ –±—ã—Å—Ç—Ä—ã–π)

    # –û–°–¢–ê–í–ò–ú –¢–û–õ–¨–ö–û –†–ê–ë–û–ß–ò–ï –ú–ï–¢–û–î–´:

    async def _parse_all_parking_pages(self, urls: List[str]):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞—Ä–∫–æ–≤–æ–∫"""
        total = len(urls)
        success = 0
        failed = 0

        print(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {total} –ø–∞—Ä–∫–æ–≤–æ–∫...")

        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{total}] –ü–∞—Ä—Å–∏–º –ø–∞—Ä–∫–æ–≤–∫—É")
            print(f"   üîó {self._shorten_url(url)}")

            try:
                parking_data = await self._parse_single_parking_page(url)

                if parking_data:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
                    normalized = self.normalize_data(parking_data)
                    self.results.append(normalized)
                    success += 1

                    # –ö—Ä–∞—Ç–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                    name = parking_data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]
                    address = parking_data.get('–ê–¥—Ä–µ—Å', '')[:60]
                    print(f"   ‚úÖ {name}")
                    print(f"      üìç {address}")
                else:
                    print("   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å")
                    failed += 1

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {str(e)[:100]}")
                failed += 1

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < total:
                delay = random.uniform(2, 4)
                await asyncio.sleep(delay)

            # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if i % 10 == 0 or i == total:
                progress = (i / total) * 100
                elapsed = time.time() - self.start_time
                print(f"\nüìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{total} ({progress:.1f}%)")
                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {success} | ‚ùå –û—à–∏–±–æ–∫: {failed}")
                print(f"‚è± –ü—Ä–æ—à–ª–æ: {elapsed:.0f} —Å–µ–∫")

        print(f"\nüéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üìä –ò—Ç–æ–≥: –£—Å–ø–µ—à–Ω–æ {success}, –û—à–∏–±–æ–∫ {failed}")

    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã (parse_single_parking_page, extract_parking_data –∏ —Ç.–¥.)
    # –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ —Ä–∞–±–æ—Ç–∞—é—Ç

    async def _parse_single_parking_page(self, url: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∫–æ–≤–∫–∏"""
        max_retries = 2

        for attempt in range(1, max_retries + 1):
            try:
                if attempt > 1:
                    print(f"   üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}")
                    await asyncio.sleep(random.uniform(3, 5))

                # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–∞—Ä–∫–æ–≤–∫–∏
                parking_page = await self.browser.get(url)
                await asyncio.sleep(random.uniform(3, 4))

                # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –Ω–µ–º–Ω–æ–≥–æ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                await parking_page.evaluate("window.scrollBy(0, 300)")
                await asyncio.sleep(1)

                # –ü–æ–ª—É—á–∞–µ–º HTML
                html = await parking_page.evaluate("document.documentElement.outerHTML")

                if not html:
                    continue

                # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ
                soup = BeautifulSoup(html, 'lxml')
                data = self._extract_parking_data(url, soup, html)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                if data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞') or data.get('–ê–¥—Ä–µ—Å'):
                    return data
                else:
                    print(f"   ‚ö† –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")

            except Exception as e:
                error_msg = str(e)
                if "timeout" in error_msg.lower():
                    print(f"   ‚ö† –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ")
                else:
                    print(f"   ‚úó –û—à–∏–±–∫–∞: {error_msg[:50]}...")

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
            if attempt < max_retries:
                await asyncio.sleep(random.uniform(5, 8))

        return None

    def _extract_parking_data(self, url: str, soup: BeautifulSoup, html: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∫–æ–≤–∫–∏"""
        data = {
            'source': 'yandex',
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            '–°—Å—ã–ª–∫–∞': url,
            '–°—Å—ã–ª–∫–∞ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫—É': url
        }

        # 1. –ù–∞–∑–≤–∞–Ω–∏–µ
        title_selectors = [
            'h1',
            '.orgpage-header-view__header',
            '.business-title',
            '.card-title-view__title',
            '[itemprop="name"]'
        ]

        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞'] = text
                    data['–ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä–∫–æ–≤–∫–∏'] = text
                    break

        # 2. –ê–¥—Ä–µ—Å
        address_selectors = [
            '[itemprop="address"]',
            '.business-contacts-view__address',
            '.card-address-view__address',
            '.orgpage-address-view__address-text',
            'address'
        ]

        for selector in address_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–ê–¥—Ä–µ—Å'] = text
                    data['–ê–¥—Ä–µ—Å –ø–∞—Ä–∫–æ–≤–∫–∏'] = text
                    break

        # 3. –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        coords = self._extract_coordinates(url, soup)
        if coords:
            data['–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã'] = coords

        # 4. –¢–µ–ª–µ—Ñ–æ–Ω
        phone_links = soup.find_all('a', href=re.compile(r'^tel:'))
        phones = []
        for link in phone_links:
            phone = link.get('href', '').replace('tel:', '').strip()
            if phone:
                clean_phone = re.sub(r'[^\d+]', '', phone)
                if clean_phone and clean_phone not in phones:
                    phones.append(clean_phone)

        if phones:
            data['–¢–µ–ª–µ—Ñ–æ–Ω'] = ', '.join(phones)

        # 5. –°–∞–π—Ç
        site_selectors = [
            '.business-urls-view__link',
            '.card-website-view__link',
            '.orgpage-url-view__url'
        ]

        for selector in site_selectors:
            elem = soup.select_one(selector)
            if elem:
                href = elem.get('href', '')
                if href and 'yandex.ru' not in href:
                    data['–°–∞–π—Ç'] = href
                    break

        # 6. –¢–∏–ø –æ–±—ä–µ–∫—Ç–∞
        category_selectors = [
            '.business-categories-view__category',
            '.card-categories-view__category',
            '.orgpage-categories-view__category',
            '[itemprop="category"]'
        ]

        for selector in category_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–¢–∏–ø –æ–±—ä–µ–∫—Ç–∞'] = text
                    break

        # 7. –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã
        hours_selectors = [
            '.business-feature-view__schedule-days',
            '.card-schedule-view__schedule',
            '.orgpage-working-view__working-days',
            '[itemprop="openingHours"]'
        ]

        for selector in hours_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text and (':' in text or '—á–∞—Å' in text.lower()):
                    data['–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –ø–∞—Ä–∫–æ–≤–∫–∏'] = text
                    break

        # 8. –†–µ–π—Ç–∏–Ω–≥
        rating_selectors = [
            '.business-rating-badge-view__rating-text',
            '.card-rating-view__rating',
            '.orgpage-rating-view__rating',
            '[itemprop="ratingValue"]'
        ]

        for selector in rating_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–û—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∫–æ–≤–∫–∏'] = text
                    break

        # 9. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫
        reviews_selectors = [
            '.business-rating-badge-view__rating-count',
            '.card-rating-view__reviews-count',
            '.orgpage-reviews-view__reviews-count',
            '[itemprop="reviewCount"]'
        ]

        for selector in reviews_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫'] = text
                    break

        # 10. –¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏
        parking_type = self._detect_parking_type(soup, data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', ''), html)
        data['–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏'] = parking_type

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø
        if '–∑–∞–∫—Ä—ã—Ç' in parking_type.lower() or '–æ—Ö—Ä–∞–Ω—è' in parking_type.lower():
            data['–î–æ—Å—Ç—É–ø'] = '–ó–∞–∫—Ä—ã—Ç—ã–π'
        else:
            data['–î–æ—Å—Ç—É–ø'] = '–û—Ç–∫—Ä—ã—Ç—ã–π'

        # 11. –¶–µ–Ω—ã
        page_text = soup.get_text()
        price_matches = re.findall(r'(\d+\s*—Ä—É–±|\d+\s*‚ÇΩ|\d+\s*–≤ —á–∞—Å|\d+\s*–≤ —Å—É—Ç–∫–∏)', page_text, re.IGNORECASE)
        if price_matches:
            data['–¶–µ–Ω—ã'] = price_matches[0]
            data['–¢–∞—Ä–∏—Ñ—ã'] = '; '.join(price_matches[:3])

        # 12. –í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
        capacity_match = re.search(r'(\d+)\s*–º–µ—Å—Ç|\b–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å\s*(\d+)', page_text, re.IGNORECASE)
        if capacity_match:
            capacity = capacity_match.group(1) or capacity_match.group(2)
            data['–í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å'] = capacity

        # 13. –û–ø–∏—Å–∞–Ω–∏–µ
        desc_selectors = [
            '.business-description-view__description',
            '.card-description-view__description',
            '.orgpage-description-view__description',
            '[itemprop="description"]'
        ]

        for selector in desc_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–û–ø–∏—Å–∞–Ω–∏–µ'] = text[:500]
                    break

        return data

    def _extract_coordinates(self, url: str, soup: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç"""
        # –ò–∑ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤
        meta_coords = soup.find('meta', attrs={'name': 'coordinates'})
        if meta_coords:
            coords = meta_coords.get('content', '')
            if coords:
                return coords

        # –ò–∑ URL
        parsed = urlparse(url)
        if 'll=' in url:
            params = parse_qs(parsed.query)
            if 'll' in params:
                ll_parts = params['ll'][0].split('%2C')
                if len(ll_parts) == 2:
                    # –Ø–Ω–¥–µ–∫—Å: –¥–æ–ª–≥–æ—Ç–∞,—à–∏—Ä–æ—Ç–∞ -> –º–µ–Ω—è–µ–º –Ω–∞ —à–∏—Ä–æ—Ç–∞,–¥–æ–ª–≥–æ—Ç–∞
                    return f"{ll_parts[1]},{ll_parts[0]}"

        # –ò–∑ data-–∞—Ç—Ä–∏–±—É—Ç–æ–≤
        coord_elem = soup.find(attrs={'data-coordinates': True})
        if coord_elem:
            coords = coord_elem.get('data-coordinates')
            if coords:
                return coords

        return None

    def _detect_parking_type(self, soup: BeautifulSoup, name: str, html: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø–∞—Ä–∫–æ–≤–∫–∏"""
        text = (name + ' ' + soup.get_text()).lower()
        type_info = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–ª–∞—Ç–Ω–æ—Å—Ç–∏
        if any(word in text for word in ['–ø–ª–∞—Ç–Ω', '–æ–ø–ª–∞—Ç', '—Ç–∞—Ä–∏—Ñ', '—Ü–µ–Ω–∞', '‚ÇΩ', '—Ä—É–±']):
            type_info.append('–ø–ª–∞—Ç–Ω–∞—è')
        elif any(word in text for word in ['–±–µ—Å–ø–ª–∞—Ç–Ω', 'free', 'gratis']):
            type_info.append('–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è')

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞
        if any(word in text for word in ['–∫—Ä—ã—Ç', '–∑–∞–∫—Ä—ã—Ç', '–æ—Ö—Ä–∞–Ω—è', '–ø–æ–¥–∑–µ–º–Ω']):
            type_info.append('–∫—Ä—ã—Ç–∞—è')
            type_info.append('–æ—Ö—Ä–∞–Ω—è–µ–º–∞—è')
        elif any(word in text for word in ['—É–ª–∏—á–Ω', '–æ—Ç–∫—Ä—ã—Ç', '–≥–æ—Å—Ç']):
            type_info.append('—É–ª–∏—á–Ω–∞—è')

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if any(word in text for word in ['—Ç–æ—Ä–≥–æ–≤', '—Ç—Ü', '–º–æ–ª–ª', '–≥–∞–ª–µ—Ä–µ—è']):
            type_info.append('–ø—Ä–∏ —Ç—Ü')
        elif any(word in text for word in ['–æ—Ñ–∏c', '–±–∏–∑–Ω–µ—Å', '—Ü–µ–Ω—Ç—Ä']):
            type_info.append('–±–∏–∑–Ω–µ—Å-—Ü–µ–Ω—Ç—Ä')

        return ", ".join(type_info) if type_info else "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return ""
        text = ' '.join(text.split())
        return text.strip()

    def _shorten_url(self, url: str, max_length: int = 60) -> str:
        """–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ URL –¥–ª—è –≤—ã–≤–æ–¥–∞"""
        if len(url) <= max_length:
            return url
        return url[:max_length - 3] + "..."

    def _print_final_stats(self):
        """–í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        print("\n" + "=" * 60)
        print("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –Ø–ù–î–ï–ö–° –ö–ê–†–¢")
        print("=" * 60)

        elapsed = time.time() - self.start_time
        minutes = int(elapsed // 60)
        seconds = int(elapsed % 60)

        print(f"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è: {minutes} –º–∏–Ω {seconds} —Å–µ–∫")
        print(f"üîó –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ —Å–æ–±—Ä–∞–Ω–æ: {len(self.all_parking_urls)}")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ: {len(self.results)}")

        if self.all_parking_urls:
            success_rate = (len(self.results) / len(self.all_parking_urls)) * 100
            print(f"üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞—Ä—Å–∏–Ω–≥–∞: {success_rate:.1f}%")

        if self.results:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º
            closed_count = len([p for p in self.results if '–∑–∞–∫—Ä—ã—Ç' in p.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '').lower()])
            paid_count = len([p for p in self.results if '–ø–ª–∞—Ç–Ω' in p.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '').lower()])
            guarded_count = len([p for p in self.results if '–æ—Ö—Ä–∞–Ω—è' in p.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '').lower()])

            print(f"\nüöó –¢–ò–ü–´ –ü–ê–†–ö–û–í–û–ö:")
            print(f"   –ó–∞–∫—Ä—ã—Ç—ã—Ö: {closed_count}")
            print(f"   –û—Ö—Ä–∞–Ω—è–µ–º—ã—Ö: {guarded_count}")
            print(f"   –ü–ª–∞—Ç–Ω—ã—Ö: {paid_count}")

            # –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
            with_coords = len([p for p in self.results if p.get('–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã')])
            with_address = len([p for p in self.results if p.get('–ê–¥—Ä–µ—Å')])
            with_phone = len([p for p in self.results if p.get('–¢–µ–ª–µ—Ñ–æ–Ω')])

            print(f"\nüìä –ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–•:")
            print(
                f"   –° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {with_coords}/{len(self.results)} ({with_coords / len(self.results) * 100:.1f}%)")
            print(f"   –° –∞–¥—Ä–µ—Å–∞–º–∏: {with_address}/{len(self.results)} ({with_address / len(self.results) * 100:.1f}%)")
            print(f"   –° —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏: {with_phone}/{len(self.results)} ({with_phone / len(self.results) * 100:.1f}%)")

            # –ü—Ä–∏–º–µ—Ä—ã
            print(f"\nüèÜ –ü–†–ò–ú–ï–†–´ –ù–ê–ô–î–ï–ù–ù–´–• –ü–ê–†–ö–û–í–û–ö:")
            for i, item in enumerate(self.results[:3], 1):
                name = item.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:40]
                address = item.get('–ê–¥—Ä–µ—Å', '')[:50]
                parking_type = item.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                print(f"   {i}. {name}")
                print(f"      üìç {address}")
                print(f"      üöó {parking_type}")

        print("=" * 60)


# –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
async def test_yandex_parser():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞"""
    print("üß™ –¢–ï–°–¢–ò–†–£–ï–ú –ü–ê–†–°–ï–† –Ø–ù–î–ï–ö–° –ö–ê–†–¢")
    parser = YandexParser(headless=False)

    try:
        results = await parser.parse()

        print(f"\nüéâ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω! –ù–∞–π–¥–µ–Ω–æ {len(results)} –ø–∞—Ä–∫–æ–≤–æ–∫")

        if results:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            with open("yandex_test_results.json", "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print("üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ yandex_test_results.json")

            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            print(f"   –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ —Å–æ–±—Ä–∞–Ω–æ: {len(parser.all_parking_urls)}")
            print(f"   –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ: {len(results)}")

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await parser.close()


if __name__ == "__main__":
    asyncio.run(test_yandex_parser())
