import asyncio
import random
import re
import time
import json
from typing import List, Dict, Any, Optional, Set
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse, parse_qs

from .base_parser import BaseParser


class YandexParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∫–æ–≤–æ–∫ –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ"""

    def __init__(self, headless: bool = True):
        super().__init__(headless)
        self.start_time = None
        self.all_parking_urls: Set[str] = set()
        self.max_scroll_attempts_without_new = 5  # –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –±–µ–∑ –Ω–æ–≤—ã—Ö URL

    @property
    def source_name(self) -> str:
        return "yandex"

    async def parse(self, max_scrolls: int = 50, max_parkings: int = 200) -> List[Dict[str, Any]]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç"""
        print("=" * 60)
        print("üöÄ –ó–ê–ü–£–°–ö –ü–ê–†–°–ï–†–ê –Ø–ù–î–ï–ö–° –ö–ê–†–¢")
        print("=" * 60)

        self.start_time = time.time()
        self.results = []
        self.all_parking_urls.clear()

        if not await self.init_browser():
            return []

        try:
            # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–∏—Å–∫–∞ –∏ –∫–ª–∏–∫–∞–µ–º –∫–Ω–æ–ø–∫—É
            print("\nüìÑ –ó–ê–ì–†–£–ó–ö–ê –°–¢–†–ê–ù–ò–¶–´ –ò –ö–õ–ò–ö –ö–ù–û–ü–ö–ò")
            print("-" * 50)

            search_url = "https://yandex.ru/maps/2/saint-petersburg/search/–ø–∞—Ä–∫–æ–≤–∫–∏/"
            print(f"üîó URL: {search_url}")

            page = await self.browser.get(search_url)
            await asyncio.sleep(5)

            # –ö–ª–∏–∫–∞–µ–º –∫–Ω–æ–ø–∫—É "–ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"
            button = await page.query_selector('span.search-command-view__show-results-button')
            if button:
                print("‚úÖ –ö–Ω–æ–ø–∫–∞ –Ω–∞–π–¥–µ–Ω–∞, –∫–ª–∏–∫–∞–µ–º...")
                await button.click()
                await asyncio.sleep(5)
                print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            else:
                print("‚ö†Ô∏è –ö–Ω–æ–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")

            # 2. –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Å—Å—ã–ª–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞
            print("\nüìú –°–ë–û–† –í–°–ï–• –°–°–´–õ–û–ö –°–û –°–ö–†–û–õ–õ–ò–ù–ì–û–ú")
            print("-" * 50)

            await self._collect_all_urls_with_scrolling(page, max_scrolls)

            if not self.all_parking_urls:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Å—ã–ª–∫–∏")
                return []

            print(f"\n‚úÖ –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(self.all_parking_urls)}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ URL –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            self._save_urls_list()

            # 3. –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é –ø–∞—Ä–∫–æ–≤–∫—É (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ)
            print("\nüè¢ –î–ï–¢–ê–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì –ü–ê–†–ö–û–í–û–ö")
            print("-" * 50)

            urls_to_parse = list(self.all_parking_urls)[:max_parkings]
            print(f"üìä –ë—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å {len(urls_to_parse)} –ø–∞—Ä–∫–æ–≤–æ–∫")

            await self._parse_all_parking_pages(urls_to_parse)

            # 4. –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._print_final_stats(urls_to_parse)

            return self.results

        except Exception as e:
            print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            import traceback
            traceback.print_exc()
            return self.results
        finally:
            await self.close()

    async def _collect_all_urls_with_scrolling(self, page, max_scrolls: int):
        """–°–±–æ—Ä –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞"""
        print("–ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ —Å–æ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–æ–º...")

        # –î–µ–±–∞–≥ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        await self._debug_page_structure(page)

        scroll_attempt = 0
        attempts_without_new_urls = 0

        while scroll_attempt < max_scrolls and attempts_without_new_urls < self.max_scroll_attempts_without_new:
            scroll_attempt += 1
            print(f"\n   üîÑ –ü–æ–ø—ã—Ç–∫–∞ {scroll_attempt}/{max_scrolls}")

            # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            current_urls = await self._extract_all_urls_from_page(page)
            previous_count = len(self.all_parking_urls)
            self.all_parking_urls.update(current_urls)
            current_count = len(self.all_parking_urls)
            new_urls = current_count - previous_count

            print(f"   üìé –í—Å–µ–≥–æ URL: {current_count} (+{new_urls} –Ω–æ–≤—ã—Ö)")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –Ω–æ–≤—ã–µ URL
            if new_urls > 0:
                attempts_without_new_urls = 0
                print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {new_urls} –Ω–æ–≤—ã—Ö URL")
            else:
                attempts_without_new_urls += 1
                print(
                    f"   ‚ö† –ù–æ–≤—ã—Ö URL –Ω–µ—Ç (–ø–æ–ø—ã—Ç–æ–∫: {attempts_without_new_urls}/{self.max_scroll_attempts_without_new})")

            # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ –Ω–µ—Ç –Ω–æ–≤—ã—Ö URL, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
            if attempts_without_new_urls >= self.max_scroll_attempts_without_new:
                print("   üèÅ –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º - —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ –Ω–µ—Ç –Ω–æ–≤—ã—Ö URL")
                break

            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
            await self._perform_scroll_actions(page)

            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            await asyncio.sleep(random.uniform(3, 5))

            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if scroll_attempt % 5 == 0:
                print(f"   üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {current_count} —Å—Å—ã–ª–æ–∫")

        print(f"\n‚úÖ –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω –ø–æ—Å–ª–µ {scroll_attempt} –ø–æ–ø—ã—Ç–æ–∫")
        print(f"üìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {len(self.all_parking_urls)}")

    async def _perform_scroll_actions(self, page):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞"""
        try:
            print("   üéØ –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞...")

            # –°–ø–æ—Å–æ–± 1: –ò—â–µ–º –∫–Ω–æ–ø–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º "–ü–æ–∫–∞–∑–∞—Ç—å –µ—â—ë", "–ï—â—ë", "–ó–∞–≥—Ä—É–∑–∏—Ç—å –µ—â—ë"
            show_more_texts = ['–ø–æ–∫–∞–∑–∞—Ç—å', '–µ—â–µ', '–µ—â—ë', '–∑–∞–≥—Ä—É–∑–∏—Ç—å', 'show more', 'load more']

            all_buttons = await page.query_selector_all('button, [role="button"], [class*="button"], [class*="btn"]')

            clicked = False
            for button in all_buttons:
                try:
                    button_text = await button.text()
                    if button_text:
                        button_text_lower = button_text.lower()
                        if any(text in button_text_lower for text in show_more_texts):
                            print(f"   üñ±Ô∏è –ù–∞–π–¥–µ–Ω–∞ –∫–Ω–æ–ø–∫–∞: '{button_text}', –∫–ª–∏–∫–∞–µ–º...")
                            await button.click()
                            await asyncio.sleep(3)
                            clicked = True
                            break
                except:
                    continue

            if clicked:
                return

            # –°–ø–æ—Å–æ–± 2: –ò—Å–ø–æ–ª—å–∑—É–µ–º JavaScript –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –∫–ª–∏–∫–∞ –ø–æ –∫–Ω–æ–ø–∫–∞–º
            print("   üìú –ò—Å–ø–æ–ª—å–∑—É–µ–º JavaScript –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–Ω–æ–ø–æ–∫...")

            scroll_result = await page.evaluate("""
                (function() {
                    let clicked = false;

                    // –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–Ω–æ–ø–∫–∞–º–∏
                    const possibleButtons = document.querySelectorAll('button, [role="button"], [class*="button"], [class*="btn"], [onclick]');
                    const showMoreKeywords = ['–ø–æ–∫–∞–∑–∞—Ç—å', '–µ—â–µ', '–µ—â—ë', '–∑–∞–≥—Ä—É–∑–∏—Ç—å', 'show', 'more', 'load'];

                    for (const element of possibleButtons) {
                        const text = (element.textContent || element.innerText || '').toLowerCase().trim();
                        const title = (element.getAttribute('title') || '').toLowerCase();
                        const ariaLabel = (element.getAttribute('aria-label') || '').toLowerCase();

                        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —ç–ª–µ–º–µ–Ω—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                        const allText = text + ' ' + title + ' ' + ariaLabel;

                        if (showMoreKeywords.some(keyword => allText.includes(keyword))) {
                            console.log(`–ù–∞–π–¥–µ–Ω–∞ –∫–Ω–æ–ø–∫–∞: ${text}`);
                            element.click();
                            clicked = true;
                            break;
                        }
                    }

                    // –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–Ω–æ–ø–∫—É, –ø—Ä–æ–±—É–µ–º –ø—Ä–æ–∫—Ä—É—Ç–∏—Ç—å
                    if (!clicked) {
                        // –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
                        const containers = [
                            document.querySelector('.sidebar-view__panel'),
                            document.querySelector('.scroll__container'),
                            document.querySelector('.search-list-view__list-container'),
                            document.body
                        ].filter(c => c);

                        for (const container of containers) {
                            const oldScroll = container.scrollTop || window.pageYOffset;
                            const scrollAmount = 1000;

                            if (container === document.body) {
                                window.scrollBy(0, scrollAmount);
                            } else {
                                container.scrollTop += scrollAmount;
                            }

                            console.log(`–ü—Ä–æ–∫—Ä—É—á–µ–Ω ${container === document.body ? 'window' : 'container'} –Ω–∞ ${scrollAmount}px`);
                            break;
                        }
                    }

                    return { clicked: clicked };
                })();
            """)

            if scroll_result.get('clicked'):
                print("   ‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω –∫–ª–∏–∫ —á–µ—Ä–µ–∑ JavaScript")
            else:
                print("   üìú –í—ã–ø–æ–ª–Ω–µ–Ω–∞ –ø—Ä–æ–∫—Ä—É—Ç–∫–∞ —á–µ—Ä–µ–∑ JavaScript")

            # –°–ø–æ—Å–æ–± 3: –ò–º–∏—Ç–∞—Ü–∏—è –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            print("   üë§ –ò–º–∏—Ç–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è...")
            await self._emulate_user_scrolling(page)

            # –ñ–¥–µ–º –Ω–µ–º–Ω–æ–≥–æ –ø–æ—Å–ª–µ –≤—Å–µ—Ö –¥–µ–π—Å—Ç–≤–∏–π
            await asyncio.sleep(2)

        except Exception as e:
            print(f"   ‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–µ: {e}")

    async def _emulate_user_scrolling(self, page):
        """–ò–º–∏—Ç–∞—Ü–∏—è –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            # 1. –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –≤–Ω–∏–∑
            await page.evaluate("""
                // –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –æ–∫–Ω–æ –≤–Ω–∏–∑
                window.scrollTo(0, document.body.scrollHeight);

                // –¢–∞–∫–∂–µ –ø—Ä–æ–±—É–µ–º –ø—Ä–æ–∫—Ä—É—Ç–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
                const containers = [
                    '.sidebar-view__panel',
                    '.scroll__container',
                    '.search-list-view__list-container',
                    '.scrollable-container',
                    '.search-list-view__list'
                ];

                containers.forEach(selector => {
                    const container = document.querySelector(selector);
                    if (container && container.scrollHeight > container.clientHeight) {
                        container.scrollTop = container.scrollHeight;
                    }
                });
            """)
            await asyncio.sleep(1)

            # 2. –ù–µ–±–æ–ª—å—à–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞ –≤–≤–µ—Ä—Ö –∏ –≤–Ω–∏–∑
            await page.evaluate("window.scrollBy(0, -200)")
            await asyncio.sleep(0.5)
            await page.evaluate("window.scrollBy(0, 400)")
            await asyncio.sleep(0.5)

            # 3. –ò–º–∏—Ç–∏—Ä—É–µ–º –¥–≤–∏–∂–µ–Ω–∏–µ –º—ã—à–∏
            await page.evaluate("""
                // –î–≤–∏–∂–µ–Ω–∏–µ –º—ã—à–∏ –ø–æ —ç–∫—Ä–∞–Ω—É
                const moveEvent = new MouseEvent('mousemove', {
                    clientX: window.innerWidth / 2,
                    clientY: window.innerHeight / 2,
                    bubbles: true
                });
                document.dispatchEvent(moveEvent);

                // –ö–ª–∏–∫ –≤ —Ü–µ–Ω—Ç—Ä–µ —ç–∫—Ä–∞–Ω–∞
                const clickEvent = new MouseEvent('click', {
                    clientX: window.innerWidth / 2,
                    clientY: window.innerHeight / 2,
                    bubbles: true
                });
                document.dispatchEvent(clickEvent);
            """)

            await asyncio.sleep(1)

            print("   üìú –í—ã–ø–æ–ª–Ω–µ–Ω–∞ –∏–º–∏—Ç–∞—Ü–∏—è –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")

        except Exception as e:
            print(f"   ‚ö† –û—à–∏–±–∫–∞ –∏–º–∏—Ç–∞—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π: {e}")

    async def _scroll_with_javascript(self, page):
        """–°–∫—Ä–æ–ª–ª–∏–Ω–≥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º JavaScript"""
        try:
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∏ –ø—Ä–æ–∫—Ä—É—Ç–∏—Ç—å —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            scroll_result = await page.evaluate("""
                (function() {
                    let scrolled = false;

                    // 1. –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    const resultCards = document.querySelectorAll('.search-snippet-view, .search-list-view__list-item');
                    if (resultCards.length > 0) {
                        // –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∫–∞—Ä—Ç–æ—á–∫–µ
                        const lastCard = resultCards[resultCards.length - 1];
                        lastCard.scrollIntoView({ behavior: 'smooth', block: 'end' });
                        scrolled = true;
                    }

                    // 2. –ò—â–µ–º –∫–Ω–æ–ø–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏
                    const loadButtons = document.querySelectorAll('button, [role="button"], [class*="button"]');
                    for (const button of loadButtons) {
                        const text = button.textContent || button.innerText || '';
                        if (text.toLowerCase().includes('–ø–æ–∫–∞–∑–∞—Ç—å') || 
                            text.toLowerCase().includes('–µ—â–µ') ||
                            text.toLowerCase().includes('–∑–∞–≥—Ä—É–∑–∏—Ç—å')) {
                            button.click();
                            scrolled = true;
                            break;
                        }
                    }

                    // 3. –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
                    const pagination = document.querySelector('.pagination, .pager, [class*="page"]');
                    if (pagination) {
                        const nextButton = pagination.querySelector('[rel="next"], .next, [class*="next"]');
                        if (nextButton) {
                            nextButton.click();
                            scrolled = true;
                        }
                    }

                    return { success: true, scrolled: scrolled, elementsFound: resultCards.length };
                })();
            """)

            if scroll_result.get('success'):
                print(f"   üìú –ù–∞–π–¥–µ–Ω–æ {scroll_result['elementsFound']} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                if scroll_result['scrolled']:
                    print("   ‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ —á–µ—Ä–µ–∑ JavaScript")
                else:
                    print("   ‚ö† –°–∫—Ä–æ–ª–ª–∏–Ω–≥ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω")

        except Exception as e:
            print(f"   ‚ö† –û—à–∏–±–∫–∞ JavaScript —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞: {e}")

    async def _extract_all_urls_from_page(self, page) -> Set[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –í–°–ï–• URL –ø–∞—Ä–∫–æ–≤–æ–∫ —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ URL
            urls = await page.evaluate("""
                (function() {
                    const urls = new Set();

                    // 1. –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
                    const allLinks = document.querySelectorAll('a[href*="/org/"]');
                    console.log(`–ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ /org/: ${allLinks.length}`);

                    for (const link of allLinks) {
                        let href = link.getAttribute('href');
                        if (href) {
                            // –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                            if (href.startsWith('/')) {
                                href = 'https://yandex.ru' + href;
                            } else if (href.startsWith('./')) {
                                href = 'https://yandex.ru' + href.substring(1);
                            }

                            // –û—á–∏—â–∞–µ–º URL
                            const cleanUrl = href.split('?')[0].split('#')[0];

                            // –§–∏–ª—å—Ç—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                            if (!cleanUrl.includes('/reviews/') && 
                                !cleanUrl.includes('/photos/') && 
                                !cleanUrl.includes('/gallery/')) {
                                urls.add(cleanUrl);
                            }
                        }
                    }

                    // 2. –ò—â–µ–º –≤ data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö
                    const elementsWithId = document.querySelectorAll('[data-id], [data-bem]');
                    console.log(`–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å data-id/data-bem: ${elementsWithId.length}`);

                    for (const element of elementsWithId) {
                        const dataId = element.getAttribute('data-id') || '';
                        const dataBem = element.getAttribute('data-bem') || '';

                        // –ò—â–µ–º ID –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
                        const idMatch = dataId.match(/id(\\d+)/) || dataBem.match(/"id":"(\\d+)"/);
                        if (idMatch) {
                            const orgId = idMatch[1];
                            urls.add(`https://yandex.ru/maps/org/${orgId}/`);
                        }
                    }

                    // 3. –ò—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    const pageText = document.body.innerText;
                    const orgPattern = /yandex\\.ru\\/maps\\/org\\/[^\\s)]+/g;
                    const matches = pageText.match(orgPattern);
                    if (matches) {
                        console.log(`–ù–∞–π–¥–µ–Ω–æ URL –≤ —Ç–µ–∫—Å—Ç–µ: ${matches.length}`);
                        matches.forEach(match => {
                            const cleanUrl = match.split('?')[0].split('#')[0];
                            urls.add(cleanUrl.startsWith('http') ? cleanUrl : 'https://' + cleanUrl);
                        });
                    }

                    // 4. –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ (—á–∞—Å—Ç–æ —ç—Ç–æ –∫–∞—Ä—Ç–æ—á–∫–∏ –ø–∞—Ä–∫–æ–≤–æ–∫)
                    const coordElements = document.querySelectorAll('[data-coordinates]');
                    console.log(`–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: ${coordElements.length}`);

                    for (const element of coordElements) {
                        // –ò—â–µ–º –±–ª–∏–∂–∞–π—à—É—é —Å—Å—ã–ª–∫—É
                        const link = element.closest('a[href*="/org/"]') || element.querySelector('a[href*="/org/"]');
                        if (link) {
                            let href = link.getAttribute('href');
                            if (href) {
                                if (href.startsWith('/')) {
                                    href = 'https://yandex.ru' + href;
                                }
                                const cleanUrl = href.split('?')[0].split('#')[0];
                                if (!cleanUrl.includes('/reviews/') && 
                                    !cleanUrl.includes('/photos/') && 
                                    !cleanUrl.includes('/gallery/')) {
                                    urls.add(cleanUrl);
                                }
                            }
                        }
                    }

                    // 5. –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    const snippetElements = document.querySelectorAll('.search-snippet-view, .search-list-view__list-item');
                    console.log(`–ù–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: ${snippetElements.length}`);

                    for (const element of snippetElements) {
                        const link = element.querySelector('a[href*="/org/"]');
                        if (link) {
                            let href = link.getAttribute('href');
                            if (href) {
                                if (href.startsWith('/')) {
                                    href = 'https://yandex.ru' + href;
                                }
                                const cleanUrl = href.split('?')[0].split('#')[0];
                                if (!cleanUrl.includes('/reviews/') && 
                                    !cleanUrl.includes('/photos/') && 
                                    !cleanUrl.includes('/gallery/')) {
                                    urls.add(cleanUrl);
                                }
                            }
                        }
                    }

                    console.log(`–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: ${urls.size}`);
                    return Array.from(urls);
                })();
            """)

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –≤ set (–∏—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É)
            url_set = set(urls) if urls else set()

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ URL –ø–∞—Ä–∫–æ–≤–æ–∫
            filtered_urls = {url for url in url_set if self._is_parking_url(url)}

            print(f"   üîç –ù–∞–π–¥–µ–Ω–æ {len(filtered_urls)} URL –ø–∞—Ä–∫–æ–≤–æ–∫")

            # –î–ª—è –¥–µ–±–∞–≥–∞ –≤—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 5 URL
            if filtered_urls:
                print(f"   üìã –ü—Ä–∏–º–µ—Ä—ã URL:")
                for i, url in enumerate(list(filtered_urls)[:3]):
                    print(f"      {i + 1}. {self._shorten_url(url, 70)}")

            return filtered_urls

        except Exception as e:
            print(f"   ‚ö† –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL: {e}")
            import traceback
            traceback.print_exc()
            return set()

    def _is_parking_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL —Å—Å—ã–ª–∫–æ–π –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫—É"""
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ URL, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫—É
        parking_keywords = [
            'parkovka',
            'parking',
            'avtoparkovka',
            'avtomobilnaya_parkovka',
            'sto',  # –°–¢–û —á–∞—Å—Ç–æ –∏–º–µ—é—Ç –ø–∞—Ä–∫–æ–≤–∫–∏
            'parking_lot',
            'car_park'
        ]

        url_lower = url.lower()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        for keyword in parking_keywords:
            if keyword in url_lower:
                return True

        # –ï—Å–ª–∏ URL —Å–æ–¥–µ—Ä–∂–∏—Ç /org/, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ —ç—Ç–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è (–≤–æ–∑–º–æ–∂–Ω–æ –ø–∞—Ä–∫–æ–≤–∫–∞)
        if '/org/' in url_lower:
            # –ò—Å–∫–ª—é—á–∞–µ–º —è–≤–Ω–æ –Ω–µ –ø–∞—Ä–∫–æ–≤–æ—á–Ω—ã–µ URL
            non_parking = ['/reviews/', '/photos/', '/gallery/', '/attraction/', '/hotel/']
            if not any(np in url_lower for np in non_parking):
                return True

        return False

    def _save_urls_list(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ URL –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"yandex_urls_{timestamp}.txt"

            with open(filename, 'w', encoding='utf-8') as f:
                for url in sorted(self.all_parking_urls):
                    f.write(f"{url}\n")

            print(f"üíæ –°–ø–∏—Å–æ–∫ URL —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filename}")

        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è URL: {e}")

    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π (–æ–Ω–∏ —É–∂–µ —Ä–∞–±–æ—á–∏–µ)
    async def _parse_all_parking_pages(self, urls: List[str]):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞—Ä–∫–æ–≤–æ–∫"""
        total = len(urls)
        success = 0
        failed = 0

        print(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {total} –ø–∞—Ä–∫–æ–≤–æ–∫...")

        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{total}] –ü–∞—Ä—Å–∏–º –ø–∞—Ä–∫–æ–≤–∫—É")
            print(f"   üîó {self._shorten_url(url)}")

            try:
                parking_data = await self._parse_single_parking_page(url)

                if parking_data:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
                    normalized = self.normalize_data(parking_data)
                    self.results.append(normalized)
                    success += 1

                    # –ö—Ä–∞—Ç–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                    name = parking_data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]
                    address = parking_data.get('–ê–¥—Ä–µ—Å', '')[:60]
                    print(f"   ‚úÖ {name}")
                    print(f"      üìç {address}")
                else:
                    print("   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å")
                    failed += 1

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {str(e)[:100]}")
                failed += 1

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < total:
                delay = random.uniform(2, 4)
                await asyncio.sleep(delay)

            # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if i % 5 == 0 or i == total:
                progress = (i / total) * 100
                elapsed = time.time() - self.start_time
                print(f"\nüìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{total} ({progress:.1f}%)")
                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {success} | ‚ùå –û—à–∏–±–æ–∫: {failed}")
                print(f"‚è± –ü—Ä–æ—à–ª–æ: {elapsed:.0f} —Å–µ–∫")

        print(f"\nüéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üìä –ò—Ç–æ–≥: –£—Å–ø–µ—à–Ω–æ {success}, –û—à–∏–±–æ–∫ {failed}")

    async def _parse_single_parking_page(self, url: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∫–æ–≤–∫–∏"""
        max_retries = 2

        for attempt in range(1, max_retries + 1):
            try:
                if attempt > 1:
                    print(f"   üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}")
                    await asyncio.sleep(random.uniform(3, 5))

                # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–∞—Ä–∫–æ–≤–∫–∏
                parking_page = await self.browser.get(url)
                await asyncio.sleep(random.uniform(3, 4))

                # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –Ω–µ–º–Ω–æ–≥–æ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                await parking_page.evaluate("window.scrollBy(0, 300)")
                await asyncio.sleep(1)

                # –ü–æ–ª—É—á–∞–µ–º HTML
                html = await parking_page.evaluate("document.documentElement.outerHTML")

                if not html:
                    continue

                # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ
                soup = BeautifulSoup(html, 'lxml')
                data = self._extract_parking_data(url, soup, html)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                if data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞') or data.get('–ê–¥—Ä–µ—Å'):
                    return data
                else:
                    print(f"   ‚ö† –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")

            except Exception as e:
                error_msg = str(e)
                if "timeout" in error_msg.lower():
                    print(f"   ‚ö† –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ")
                else:
                    print(f"   ‚úó –û—à–∏–±–∫–∞: {error_msg[:50]}...")

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
            if attempt < max_retries:
                await asyncio.sleep(random.uniform(5, 8))

        return None

    def _extract_parking_data(self, url: str, soup: BeautifulSoup, html: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∫–æ–≤–∫–∏"""
        data = {
            'source': 'yandex',
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            '–°—Å—ã–ª–∫–∞': url,
            '–°—Å—ã–ª–∫–∞ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫—É': url
        }

        # 1. –ù–∞–∑–≤–∞–Ω–∏–µ
        title_selectors = [
            'h1',
            '.orgpage-header-view__header',
            '.business-title',
            '.card-title-view__title',
            '[itemprop="name"]'
        ]

        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞'] = text
                    data['–ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä–∫–æ–≤–∫–∏'] = text
                    break

        # 2. –ê–¥—Ä–µ—Å
        address_selectors = [
            '[itemprop="address"]',
            '.business-contacts-view__address',
            '.card-address-view__address',
            '.orgpage-address-view__address-text',
            'address'
        ]

        for selector in address_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–ê–¥—Ä–µ—Å'] = text
                    data['–ê–¥—Ä–µ—Å –ø–∞—Ä–∫–æ–≤–∫–∏'] = text
                    break

        # 3. –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        coords = self._extract_coordinates(url, soup)
        if coords:
            data['–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã'] = coords

        # 4. –¢–µ–ª–µ—Ñ–æ–Ω
        phone_links = soup.find_all('a', href=re.compile(r'^tel:'))
        phones = []
        for link in phone_links:
            phone = link.get('href', '').replace('tel:', '').strip()
            if phone:
                clean_phone = re.sub(r'[^\d+]', '', phone)
                if clean_phone and clean_phone not in phones:
                    phones.append(clean_phone)

        if phones:
            data['–¢–µ–ª–µ—Ñ–æ–Ω'] = ', '.join(phones)

        # 5. –°–∞–π—Ç
        site_selectors = [
            '.business-urls-view__link',
            '.card-website-view__link',
            '.orgpage-url-view__url'
        ]

        for selector in site_selectors:
            elem = soup.select_one(selector)
            if elem:
                href = elem.get('href', '')
                if href and 'yandex.ru' not in href:
                    data['–°–∞–π—Ç'] = href
                    break

        # 6. –¢–∏–ø –æ–±—ä–µ–∫—Ç–∞
        category_selectors = [
            '.business-categories-view__category',
            '.card-categories-view__category',
            '.orgpage-categories-view__category',
            '[itemprop="category"]'
        ]

        for selector in category_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–¢–∏–ø –æ–±—ä–µ–∫—Ç–∞'] = text
                    break

        # 7. –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã
        hours_selectors = [
            '.business-feature-view__schedule-days',
            '.card-schedule-view__schedule',
            '.orgpage-working-view__working-days',
            '[itemprop="openingHours"]'
        ]

        for selector in hours_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text and (':' in text or '—á–∞—Å' in text.lower()):
                    data['–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –ø–∞—Ä–∫–æ–≤–∫–∏'] = text
                    break

        # 8. –†–µ–π—Ç–∏–Ω–≥
        rating_selectors = [
            '.business-rating-badge-view__rating-text',
            '.card-rating-view__rating',
            '.orgpage-rating-view__rating',
            '[itemprop="ratingValue"]'
        ]

        for selector in rating_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–û—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∫–æ–≤–∫–∏'] = text
                    break

        # 9. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫
        reviews_selectors = [
            '.business-rating-badge-view__rating-count',
            '.card-rating-view__reviews-count',
            '.orgpage-reviews-view__reviews-count',
            '[itemprop="reviewCount"]'
        ]

        for selector in reviews_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫'] = text
                    break

        # 10. –¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏
        parking_type = self._detect_parking_type(soup, data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', ''), html)
        data['–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏'] = parking_type

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø
        if '–∑–∞–∫—Ä—ã—Ç' in parking_type.lower() or '–æ—Ö—Ä–∞–Ω—è' in parking_type.lower():
            data['–î–æ—Å—Ç—É–ø'] = '–ó–∞–∫—Ä—ã—Ç—ã–π'
        else:
            data['–î–æ—Å—Ç—É–ø'] = '–û—Ç–∫—Ä—ã—Ç—ã–π'

        # 11. –¶–µ–Ω—ã
        page_text = soup.get_text()
        price_matches = re.findall(r'(\d+\s*—Ä—É–±|\d+\s*‚ÇΩ|\d+\s*–≤ —á–∞—Å|\d+\s*–≤ —Å—É—Ç–∫–∏)', page_text, re.IGNORECASE)
        if price_matches:
            data['–¶–µ–Ω—ã'] = price_matches[0]
            data['–¢–∞—Ä–∏—Ñ—ã'] = '; '.join(price_matches[:3])

        # 12. –í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
        capacity_match = re.search(r'(\d+)\s*–º–µ—Å—Ç|\b–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å\s*(\d+)', page_text, re.IGNORECASE)
        if capacity_match:
            capacity = capacity_match.group(1) or capacity_match.group(2)
            data['–í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å'] = capacity

        # 13. –û–ø–∏—Å–∞–Ω–∏–µ
        desc_selectors = [
            '.business-description-view__description',
            '.card-description-view__description',
            '.orgpage-description-view__description',
            '[itemprop="description"]'
        ]

        for selector in desc_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–û–ø–∏—Å–∞–Ω–∏–µ'] = text[:500]
                    break

        return data

    def _extract_coordinates(self, url: str, soup: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç"""
        # –ò–∑ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤
        meta_coords = soup.find('meta', attrs={'name': 'coordinates'})
        if meta_coords:
            coords = meta_coords.get('content', '')
            if coords:
                return coords

        # –ò–∑ URL
        parsed = urlparse(url)
        if 'll=' in url:
            params = parse_qs(parsed.query)
            if 'll' in params:
                ll_parts = params['ll'][0].split('%2C')
                if len(ll_parts) == 2:
                    # –Ø–Ω–¥–µ–∫—Å: –¥–æ–ª–≥–æ—Ç–∞,—à–∏—Ä–æ—Ç–∞ -> –º–µ–Ω—è–µ–º –Ω–∞ —à–∏—Ä–æ—Ç–∞,–¥–æ–ª–≥–æ—Ç–∞
                    return f"{ll_parts[1]},{ll_parts[0]}"

        # –ò–∑ data-–∞—Ç—Ä–∏–±—É—Ç–æ–≤
        coord_elem = soup.find(attrs={'data-coordinates': True})
        if coord_elem:
            coords = coord_elem.get('data-coordinates')
            if coords:
                return coords

        return None

    def _detect_parking_type(self, soup: BeautifulSoup, name: str, html: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø–∞—Ä–∫–æ–≤–∫–∏"""
        text = (name + ' ' + soup.get_text()).lower()
        type_info = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–ª–∞—Ç–Ω–æ—Å—Ç–∏
        if any(word in text for word in ['–ø–ª–∞—Ç–Ω', '–æ–ø–ª–∞—Ç', '—Ç–∞—Ä–∏—Ñ', '—Ü–µ–Ω–∞', '‚ÇΩ', '—Ä—É–±']):
            type_info.append('–ø–ª–∞—Ç–Ω–∞—è')
        elif any(word in text for word in ['–±–µ—Å–ø–ª–∞—Ç–Ω', 'free', 'gratis']):
            type_info.append('–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è')

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞
        if any(word in text for word in ['–∫—Ä—ã—Ç', '–∑–∞–∫—Ä—ã—Ç', '–æ—Ö—Ä–∞–Ω—è', '–ø–æ–¥–∑–µ–º–Ω']):
            type_info.append('–∫—Ä—ã—Ç–∞—è')
            type_info.append('–æ—Ö—Ä–∞–Ω—è–µ–º–∞—è')
        elif any(word in text for word in ['—É–ª–∏—á–Ω', '–æ—Ç–∫—Ä—ã—Ç', '–≥–æ—Å—Ç']):
            type_info.append('—É–ª–∏—á–Ω–∞—è')

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if any(word in text for word in ['—Ç–æ—Ä–≥–æ–≤', '—Ç—Ü', '–º–æ–ª–ª', '–≥–∞–ª–µ—Ä–µ—è']):
            type_info.append('–ø—Ä–∏ —Ç—Ü')
        elif any(word in text for word in ['–æ—Ñ–∏c', '–±–∏–∑–Ω–µ—Å', '—Ü–µ–Ω—Ç—Ä']):
            type_info.append('–±–∏–∑–Ω–µ—Å-—Ü–µ–Ω—Ç—Ä')

        return ", ".join(type_info) if type_info else "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return ""
        text = ' '.join(text.split())
        return text.strip()

    def _shorten_url(self, url: str, max_length: int = 60) -> str:
        """–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ URL –¥–ª—è –≤—ã–≤–æ–¥–∞"""
        if len(url) <= max_length:
            return url
        return url[:max_length - 3] + "..."

    def _print_final_stats(self, urls_to_parse: List[str]):
        """–í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        print("\n" + "=" * 60)
        print("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –Ø–ù–î–ï–ö–° –ö–ê–†–¢")
        print("=" * 60)

        elapsed = time.time() - self.start_time
        minutes = int(elapsed // 60)
        seconds = int(elapsed % 60)

        print(f"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è: {minutes} –º–∏–Ω {seconds} —Å–µ–∫")
        print(f"üîó –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ —Å–æ–±—Ä–∞–Ω–æ: {len(self.all_parking_urls)}")
        print(f"üîó –ü–∞—Ä—Å–∏–ª–æ—Å—å: {len(urls_to_parse)}")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ: {len(self.results)}")

        if urls_to_parse:
            success_rate = (len(self.results) / len(urls_to_parse)) * 100
            print(f"üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞—Ä—Å–∏–Ω–≥–∞: {success_rate:.1f}%")

        if self.results:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º
            closed_count = len([p for p in self.results if '–∑–∞–∫—Ä—ã—Ç' in p.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '').lower()])
            paid_count = len([p for p in self.results if '–ø–ª–∞—Ç–Ω' in p.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '').lower()])
            guarded_count = len([p for p in self.results if '–æ—Ö—Ä–∞–Ω—è' in p.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '').lower()])

            print(f"\nüöó –¢–ò–ü–´ –ü–ê–†–ö–û–í–û–ö:")
            print(f"   –ó–∞–∫—Ä—ã—Ç—ã—Ö: {closed_count}")
            print(f"   –û—Ö—Ä–∞–Ω—è–µ–º—ã—Ö: {guarded_count}")
            print(f"   –ü–ª–∞—Ç–Ω—ã—Ö: {paid_count}")

            # –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
            with_coords = len([p for p in self.results if p.get('–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã')])
            with_address = len([p for p in self.results if p.get('–ê–¥—Ä–µ—Å')])
            with_phone = len([p for p in self.results if p.get('–¢–µ–ª–µ—Ñ–æ–Ω')])

            print(f"\nüìä –ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–•:")
            print(
                f"   –° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {with_coords}/{len(self.results)} ({with_coords / len(self.results) * 100:.1f}%)")
            print(f"   –° –∞–¥—Ä–µ—Å–∞–º–∏: {with_address}/{len(self.results)} ({with_address / len(self.results) * 100:.1f}%)")
            print(f"   –° —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏: {with_phone}/{len(self.results)} ({with_phone / len(self.results) * 100:.1f}%)")

            # –ü—Ä–∏–º–µ—Ä—ã
            print(f"\nüèÜ –ü–†–ò–ú–ï–†–´ –ù–ê–ô–î–ï–ù–ù–´–• –ü–ê–†–ö–û–í–û–ö:")
            for i, item in enumerate(self.results[:3], 1):
                name = item.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:40]
                address = item.get('–ê–¥—Ä–µ—Å', '')[:50]
                parking_type = item.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                print(f"   {i}. {name}")
                print(f"      üìç {address}")
                print(f"      üöó {parking_type}")

        print("=" * 60)

    async def _debug_page_structure(self, page):
        """–î–µ–±–∞–≥ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        print("\nüîç –î–ï–ë–ê–ì –°–¢–†–£–ö–¢–£–†–´ –°–¢–†–ê–ù–ò–¶–´:")
        print("-" * 40)

        try:
            structure = await page.evaluate("""
                (function() {
                    const results = {};

                    // 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
                    const containers = [
                        '.sidebar-view__panel',
                        '.scroll__container',
                        '.search-list-view__list-container',
                        '.search-list-view__list',
                        '.search-list-view__items',
                        '.search-snippet-view',
                        '[data-coordinates]',
                        'a[href*="/org/"]'
                    ];

                    containers.forEach(selector => {
                        const elements = document.querySelectorAll(selector);
                        results[selector] = {
                            count: elements.length,
                            firstExists: elements.length > 0
                        };

                        // –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                        if (elements.length > 0 && (selector.includes('panel') || selector.includes('container'))) {
                            const firstEl = elements[0];
                            results[selector].scrollHeight = firstEl.scrollHeight;
                            results[selector].clientHeight = firstEl.clientHeight;
                            results[selector].scrollable = firstEl.scrollHeight > firstEl.clientHeight;
                        }
                    });

                    // 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–Ω–æ–ø–∫–∏
                    const buttons = document.querySelectorAll('button, [role="button"]');
                    results['buttons'] = {
                        count: buttons.length,
                        texts: Array.from(buttons).map(btn => btn.textContent?.trim() || btn.innerText?.trim() || '').filter(t => t)
                    };

                    // 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                    const bodyText = document.body.innerText || '';
                    results['textStats'] = {
                        length: bodyText.length,
                        containsParking: bodyText.toLowerCase().includes('–ø–∞—Ä–∫–æ–≤'),
                        containsShowMore: bodyText.toLowerCase().includes('–ø–æ–∫–∞–∑–∞—Ç—å') || bodyText.toLowerCase().includes('–µ—â–µ')
                    };

                    return results;
                })();
            """)

            # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–¢–†–ê–ù–ò–¶–´:")
            for selector, info in structure.items():
                if selector == 'textStats':
                    print(f"   üìù –¢–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã:")
                    print(f"      –î–ª–∏–Ω–∞: {info['length']} —Å–∏–º–≤–æ–ª–æ–≤")
                    print(f"      –°–æ–¥–µ—Ä–∂–∏—Ç '–ø–∞—Ä–∫–æ–≤': {info['containsParking']}")
                    print(f"      –°–æ–¥–µ—Ä–∂–∏—Ç '–ø–æ–∫–∞–∑–∞—Ç—å/–µ—â–µ': {info['containsShowMore']}")
                elif selector == 'buttons':
                    print(f"   üñ±Ô∏è –ö–Ω–æ–ø–∫–∏: {info['count']} —à—Ç.")
                    if info['texts']:
                        unique_texts = list(set(info['texts']))[:5]
                        print(f"      –¢–µ–∫—Å—Ç—ã: {', '.join(unique_texts)}")
                else:
                    if info['count'] > 0:
                        print(f"   {selector}: {info['count']} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                        if 'scrollable' in info:
                            print(f"      –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º—ã–π: {info['scrollable']}")
                            print(
                                f"      –í—ã—Å–æ—Ç–∞: {info.get('scrollHeight', 'N/A')} / {info.get('clientHeight', 'N/A')}")

            print("-" * 40)

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –¥–µ–±–∞–≥–∞: {e}")


# –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
async def test_yandex_scrolling():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞ —Å–æ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–æ–º"""
    print("üß™ –¢–ï–°–¢–ò–†–£–ï–ú –ü–ê–†–°–ï–† –°–û –°–ö–†–û–õ–õ–ò–ù–ì–û–ú")
    parser = YandexParser(headless=False)

    try:
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –ª–∏–º–∏—Ç–∞–º–∏
        results = await parser.parse(max_scrolls=15, max_parkings=20)

        print(f"\nüéâ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω! –ù–∞–π–¥–µ–Ω–æ {len(results)} –ø–∞—Ä–∫–æ–≤–æ–∫")

        if results:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            with open("yandex_scrolling_results.json", "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print("üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ yandex_scrolling_results.json")

            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–æ–±—Ä–∞–Ω–Ω—ã–º URL
            print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –°–°–´–õ–ö–ê–ú:")
            print(f"   –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ —Å–æ–±—Ä–∞–Ω–æ: {len(parser.all_parking_urls)}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ URL
            import csv
            with open("yandex_all_urls.csv", "w", encoding="utf-8", newline='') as f:
                writer = csv.writer(f)
                writer.writerow(["URL"])
                for url in parser.all_parking_urls:
                    writer.writerow([url])
            print("üíæ –°–ø–∏—Å–æ–∫ URL —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ yandex_all_urls.csv")



    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await parser.close()


# –í –∫–æ–Ω—Ü–µ —Ñ–∞–π–ª–∞, –ø–µ—Ä–µ–¥ if __name__ == "__main__":

async def debug_scrolling_only():
    """–¢–æ–ª—å–∫–æ –¥–µ–±–∞–≥ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞ –±–µ–∑ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    print("üß™ –î–ï–ë–ê–ì –°–ö–†–û–õ–õ–ò–ù–ì–ê –Ø–ù–î–ï–ö–°.–ö–ê–†–¢")

    parser = YandexParser(headless=False)

    try:
        await parser.init_browser()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–∏—Å–∫–∞
        search_url = "https://yandex.ru/maps/2/saint-petersburg/search/–ø–∞—Ä–∫–æ–≤–∫–∏/"
        print(f"üîó –ó–∞–≥—Ä—É–∂–∞–µ–º: {search_url}")

        page = await parser.browser.get(search_url)
        await asyncio.sleep(5)

        # –ö–ª–∏–∫–∞–µ–º –∫–Ω–æ–ø–∫—É "–ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"
        button = await page.query_selector('span.search-command-view__show-results-button')
        if button:
            print("‚úÖ –ö–ª–∏–∫–∞–µ–º –∫–Ω–æ–ø–∫—É '–ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã'...")
            await button.click()
            await asyncio.sleep(5)

        # –î–µ–ª–∞–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç –î–û —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞
        await page.save_screenshot("debug_before_scroll.png")
        print("üíæ –°–∫—Ä–∏–Ω—à–æ—Ç –î–û —Å–æ—Ö—Ä–∞–Ω–µ–Ω: debug_before_scroll.png")

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –î–û —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞
        print("\nüìã –°–ë–ò–†–ê–ï–ú –°–°–´–õ–ö–ò –î–û –°–ö–†–û–õ–õ–ò–ù–ì–ê:")
        urls_before = await parser._extract_all_urls_from_page(page)
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(urls_before)}")

        # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑
        print("\nüìú –í–´–ü–û–õ–ù–Ø–ï–ú –°–ö–†–û–õ–õ–ò–ù–ì (5 –ø–æ–ø—ã—Ç–æ–∫):")
        for i in range(5):
            print(f"\n   üîÑ –ü–æ–ø—ã—Ç–∫–∞ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞ {i + 1}/5")

            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞
            await parser._perform_scroll_actions(page)

            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏
            await asyncio.sleep(4)

            # –î–µ–ª–∞–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–∏
            await page.save_screenshot(f"debug_scroll_attempt_{i + 1}.png")
            print(f"   üíæ –°–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: debug_scroll_attempt_{i + 1}.png")

            # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ—Å–ª–µ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞
            current_urls = await parser._extract_all_urls_from_page(page)
            print(f"   –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(current_urls)}")

            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º
            new_urls = len(current_urls) - len(urls_before)
            if new_urls > 0:
                print(f"   ‚úÖ –î–æ–±–∞–≤–∏–ª–æ—Å—å –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫: +{new_urls}")
            else:
                print(f"   ‚ö† –ù–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ—Ç")

        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–±–æ—Ä –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫
        print("\nüìä –§–ò–ù–ê–õ–¨–ù–´–ô –°–ë–û–† –°–°–´–õ–û–ö:")
        all_urls = await parser._extract_all_urls_from_page(page)
        print(f"   –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫: {len(all_urls)}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ URL
        with open("debug_scroll_urls.txt", "w", encoding="utf-8") as f:
            for url in sorted(all_urls):
                f.write(f"{url}\n")
        print("üíæ –°–ø–∏—Å–æ–∫ URL —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ debug_scroll_urls.txt")

        print("\nüéâ –î–µ–±–∞–≥ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print("üìÅ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã:")
        print("   - debug_before_scroll.png")
        print("   - debug_scroll_attempt_*.png")
        print("   - debug_scroll_urls.txt")

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –¥–µ–±–∞–≥–∞: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await parser.close()

if __name__ == "__main__":
    print("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:")
    print("1. –ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (—Å–æ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–æ–º)")
    print("2. –¢–æ–ª—å–∫–æ –¥–µ–±–∞–≥ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞")

    choice = input("–í–≤–µ–¥–∏—Ç–µ 1 –∏–ª–∏ 2: ").strip()

    if choice == "1":
        asyncio.run(test_yandex_scrolling())
    else:
        asyncio.run(debug_scrolling_only())
