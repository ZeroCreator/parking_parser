import asyncio
import random
import re
import time
from typing import List, Dict, Any, Optional, Set
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse, parse_qs

from .base_parser import BaseParser


class YandexParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∫–æ–≤–æ–∫ –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ"""

    @property
    def source_name(self) -> str:
        return "yandex"

    async def parse(self) -> List[Dict[str, Any]]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç"""
        print("=" * 60)
        print("üöÄ –ó–ê–ü–£–°–ö –ü–ê–†–°–ï–†–ê –Ø–ù–î–ï–ö–° –ö–ê–†–¢")
        print("=" * 60)

        self.start_time = time.time()
        self.results = []
        self.all_urls.clear()

        if not await self.init_browser():
            return []

        try:
            # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–∏—Å–∫–∞
            print("\nüìÑ –ó–ê–ì–†–£–ó–ö–ê –°–¢–†–ê–ù–ò–¶–´ –ü–û–ò–°–ö–ê")
            print("-" * 50)

            search_url = "https://yandex.ru/maps/2/saint-petersburg/search/–ø–∞—Ä–∫–æ–≤–∫–∏/"
            print(f"üîó URL: {search_url}")

            page = await self.browser.get(search_url)
            await asyncio.sleep(3)

            # –ö–ª–∏–∫–∞–µ–º –∫–Ω–æ–ø–∫—É "–ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã", –µ—Å–ª–∏ –µ—Å—Ç—å
            button = await page.query_selector('span.search-command-view__show-results-button')
            if button:
                print("‚úÖ –ö–Ω–æ–ø–∫–∞ –Ω–∞–π–¥–µ–Ω–∞, –∫–ª–∏–∫–∞–µ–º...")
                await button.click()
                await asyncio.sleep(3)
                print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

            # 2. –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏
            print("\nüîó –°–ë–û–† –°–°–´–õ–û–ö –°–û –°–ö–†–û–õ–õ–ò–ù–ì–û–ú")
            print("-" * 50)

            await self._scroll_and_collect_urls(page)

            if not self.all_urls:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Å—ã–ª–∫–∏")
                return []

            print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(self.all_urls)}")

            # 3. –ü–∞—Ä—Å–∏–º –ø–∞—Ä–∫–æ–≤–∫–∏
            print("\nüè¢ –î–ï–¢–ê–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì –ü–ê–†–ö–û–í–û–ö")
            print("-" * 50)

            urls_to_parse = list(self.all_urls)
            print(f"üìä –ë—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å {len(urls_to_parse)} –ø–∞—Ä–∫–æ–≤–æ–∫")

            await self._parse_all_parking_pages(urls_to_parse)

            # 4. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._remove_duplicates()
            self._print_final_stats(len(self.all_urls))

            return self.results

        except Exception as e:
            print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            import traceback
            traceback.print_exc()
            return self.results
        finally:
            await self.close()

    async def _scroll_and_collect_urls(self, page):
        """–°–∫—Ä–æ–ª–ª–∏–Ω–≥ –∏ —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –¥–ª—è –Ø–Ω–¥–µ–∫—Å)"""
        print("–ù–∞—á–∏–Ω–∞–µ–º —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö –ø–∞—Ä–∫–æ–≤–æ–∫...")

        max_scroll_attempts = 100
        consecutive_no_new = 0
        total_new_urls = 0

        for attempt in range(1, max_scroll_attempts + 1):
            print(f"\n   üîÑ –ü–æ–ø—ã—Ç–∫–∞ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞ {attempt}/{max_scroll_attempts}")

            before_scroll_count = len(self.all_urls)

            # –°–∫—Ä–æ–ª–ª–∏–Ω–≥ –¥–ª—è –Ø–Ω–¥–µ–∫—Å
            await self._yandex_specific_scroll(page)

            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏
            await asyncio.sleep(random.uniform(2, 3))

            # –ü–æ–ª—É—á–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π HTML –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º URL
            html_content = await page.evaluate("document.documentElement.outerHTML")
            new_urls_count_before = len(self.all_urls)
            self._extract_urls_from_html(html_content)
            new_urls_added = len(self.all_urls) - new_urls_count_before

            print(f"   üìä URL –¥–æ: {before_scroll_count}, –¥–æ–±–∞–≤–ª–µ–Ω–æ: {new_urls_added}, –≤—Å–µ–≥–æ: {len(self.all_urls)}")

            if new_urls_added > 0:
                consecutive_no_new = 0
                total_new_urls += new_urls_added
            else:
                consecutive_no_new += 1
                if consecutive_no_new >= self.max_consecutive_no_new:
                    print(f"   üèÅ –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ - {self.max_consecutive_no_new} —Ä–∞–∑–∞ –ø–æ–¥—Ä—è–¥ –Ω–µ—Ç –Ω–æ–≤—ã—Ö URL")
                    break

            await asyncio.sleep(random.uniform(0.5, 1.5))

        print(f"\n‚úÖ –°–∫—Ä–æ–ª–ª–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω")
        print(f"üìä –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö URL: {total_new_urls}")

    async def _yandex_specific_scroll(self, page):
        """–°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ –¥–ª—è –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç"""
        try:
            await page.evaluate("""
                (function() {
                    const selectors = [
                        '.scroll__container_width_narrow',
                        '.scroll__container',
                        '.sidebar-view__panel',
                        '.search-list-view__list-container',
                        '.search-list-view__list'
                    ];

                    let scrolled = false;
                    for (const selector of selectors) {
                        const container = document.querySelector(selector);
                        if (container && container.scrollHeight > container.clientHeight) {
                            container.scrollTop = container.scrollHeight;
                            scrolled = true;
                            break;
                        }
                    }

                    window.scrollBy({
                        top: 800,
                        behavior: 'smooth'
                    });

                    return { containerScrolled: scrolled };
                })();
            """)
        except Exception as e:
            print(f"   ‚ö† –û—à–∏–±–∫–∞ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞: {e}")

    def _extract_urls_from_html(self, html_content: str):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫–∏ –∏–∑ HTML (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –¥–ª—è –Ø–Ω–¥–µ–∫—Å)"""
        try:
            urls_before = len(self.all_urls)

            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
            org_pattern = r'href="(/maps/org/[^"]+)"'
            all_link_matches = re.findall(org_pattern, html_content)

            for link in all_link_matches:
                full_url = f"https://yandex.ru{link}"
                clean_url = full_url.split('?')[0].split('#')[0]
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                if not any(exclude in clean_url.lower() for exclude in ['/reviews/', '/photos/', '/gallery/']):
                    self.all_urls.add(clean_url)

            # –ò—â–µ–º –≤ –∫–∞—Ä—Ç–æ—á–∫–∞—Ö
            snippet_pattern = r'<li[^>]*class="[^"]*search-snippet-view[^"]*"[^>]*>.*?</li>'
            snippets = re.findall(snippet_pattern, html_content, re.DOTALL)

            for snippet in snippets:
                link_match = re.search(org_pattern, snippet)
                if link_match:
                    link = link_match.group(1)
                    full_url = f"https://yandex.ru{link}"
                    clean_url = full_url.split('?')[0].split('#')[0]
                    if not any(exclude in clean_url.lower() for exclude in ['/reviews/', '/photos/', '/gallery/']):
                        self.all_urls.add(clean_url)

            new_urls = len(self.all_urls) - urls_before
            if new_urls > 0:
                print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {new_urls} –Ω–æ–≤—ã—Ö URL")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL: {e}")

    # –í–ê–ñ–ù–û: –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –º–µ—Ç–æ–¥ —Å _extract_parking_data –Ω–∞ _extract_page_data
    def _extract_page_data(self, url: str, soup: BeautifulSoup, html: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∫–æ–≤–∫–∏ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –¥–ª—è –Ø–Ω–¥–µ–∫—Å)"""
        data = {
            'source': 'yandex',
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            '–°—Å—ã–ª–∫–∞': url,
            '–°—Å—ã–ª–∫–∞ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫—É': url
        }

        # 1. –ù–∞–∑–≤–∞–Ω–∏–µ
        title_selectors = [
            'h1',
            '.orgpage-header-view__header',
            '.business-title',
            '.card-title-view__title',
            '[itemprop="name"]'
        ]

        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞'] = text
                    data['–ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä–∫–æ–≤–∫–∏'] = text
                    break

        # 2. –ê–¥—Ä–µ—Å
        address_selectors = [
            '[itemprop="address"]',
            '.business-contacts-view__address',
            '.card-address-view__address',
            '.orgpage-address-view__address-text',
            'address'
        ]

        for selector in address_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–ê–¥—Ä–µ—Å'] = text
                    data['–ê–¥—Ä–µ—Å –ø–∞—Ä–∫–æ–≤–∫–∏'] = text
                    break

        # 3. –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        coords = self._extract_yandex_coordinates(url, soup)
        if coords:
            data['–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã'] = coords

        # 4. –¢–µ–ª–µ—Ñ–æ–Ω
        phones = []
        phone_links = soup.find_all('a', href=re.compile(r'^tel:'))
        for link in phone_links:
            phone = link.get('href', '').replace('tel:', '').strip()
            if phone:
                clean_phone = re.sub(r'[^\d+]', '', phone)
                if clean_phone and clean_phone not in phones:
                    phones.append(clean_phone)

        if phones:
            data['–¢–µ–ª–µ—Ñ–æ–Ω'] = ', '.join(phones)

        # 5. –°–∞–π—Ç
        site_selectors = [
            '.business-urls-view__link',
            '.card-website-view__link',
            '.orgpage-url-view__url'
        ]

        for selector in site_selectors:
            elem = soup.select_one(selector)
            if elem:
                href = elem.get('href', '')
                if href and 'yandex.ru' not in href:
                    data['–°–∞–π—Ç'] = href
                    break

        # 6. –¢–∏–ø –æ–±—ä–µ–∫—Ç–∞
        category_selectors = [
            '.business-categories-view__category',
            '.card-categories-view__category',
            '.orgpage-categories-view__category',
            '[itemprop="category"]'
        ]

        for selector in category_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–¢–∏–ø –æ–±—ä–µ–∫—Ç–∞'] = text
                    break

        # 7. –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã
        hours_selectors = [
            '.business-feature-view__schedule-days',
            '.card-schedule-view__schedule',
            '.orgpage-working-view__working-days',
            '[itemprop="openingHours"]'
        ]

        for selector in hours_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text and (':' in text or '—á–∞—Å' in text.lower()):
                    data['–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã'] = text
                    break

        # 8. –†–µ–π—Ç–∏–Ω–≥ –∏ –æ—Ü–µ–Ω–∫–∏
        rating_selectors = [
            '.business-rating-badge-view__rating-text',
            '.card-rating-view__rating',
            '.orgpage-rating-view__rating',
            '[itemprop="ratingValue"]'
        ]

        for selector in rating_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–û—Ü–µ–Ω–∫–∞'] = text
                    break

        reviews_selectors = [
            '.business-rating-badge-view__rating-count',
            '.card-rating-view__reviews-count',
            '.orgpage-reviews-view__reviews-count',
            '[itemprop="reviewCount"]'
        ]

        for selector in reviews_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫'] = text
                    break

        # 9. –¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏
        parking_type = self._detect_yandex_parking_type(soup, data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', ''), html)
        data['–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏'] = parking_type

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø
        if '–∑–∞–∫—Ä—ã—Ç' in parking_type.lower() or '–æ—Ö—Ä–∞–Ω—è' in parking_type.lower():
            data['–î–æ—Å—Ç—É–ø'] = '–ó–∞–∫—Ä—ã—Ç—ã–π'
        else:
            data['–î–æ—Å—Ç—É–ø'] = '–û—Ç–∫—Ä—ã—Ç—ã–π'

        # 10. –¶–µ–Ω—ã
        page_text = soup.get_text()
        price_matches = re.findall(r'(\d+\s*—Ä—É–±|\d+\s*‚ÇΩ|\d+\s*–≤ —á–∞—Å|\d+\s*–≤ —Å—É—Ç–∫–∏)', page_text, re.IGNORECASE)
        if price_matches:
            data['–¶–µ–Ω—ã'] = price_matches[0]
            data['–¢–∞—Ä–∏—Ñ—ã'] = '; '.join(price_matches[:3])

        # 11. –í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
        capacity_match = re.search(r'(\d+)\s*–º–µ—Å—Ç|\b–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å\s*(\d+)', page_text, re.IGNORECASE)
        if capacity_match:
            capacity = capacity_match.group(1) or capacity_match.group(2)
            data['–í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å'] = capacity

        # 12. –û–ø–∏—Å–∞–Ω–∏–µ
        desc_selectors = [
            '.business-description-view__description',
            '.card-description-view__description',
            '.orgpage-description-view__description',
            '[itemprop="description"]'
        ]

        for selector in desc_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = self._clean_text(elem.get_text())
                if text:
                    data['–û–ø–∏—Å–∞–Ω–∏–µ'] = text[:500]
                    break

        return data

    def _extract_yandex_coordinates(self, url: str, soup: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –¥–ª—è –Ø–Ω–¥–µ–∫—Å"""
        # –ò–∑ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤
        meta_coords = soup.find('meta', attrs={'name': 'coordinates'})
        if meta_coords:
            coords = meta_coords.get('content', '')
            if coords:
                return coords

        # –ò–∑ URL
        parsed = urlparse(url)
        if 'll=' in url:
            params = parse_qs(parsed.query)
            if 'll' in params:
                ll_parts = params['ll'][0].split('%2C')
                if len(ll_parts) == 2:
                    # –Ø–Ω–¥–µ–∫—Å: –¥–æ–ª–≥–æ—Ç–∞,—à–∏—Ä–æ—Ç–∞ -> –º–µ–Ω—è–µ–º –Ω–∞ —à–∏—Ä–æ—Ç–∞,–¥–æ–ª–≥–æ—Ç–∞
                    return f"{ll_parts[1]},{ll_parts[0]}"

        # –ò–∑ data-–∞—Ç—Ä–∏–±—É—Ç–æ–≤
        coord_elem = soup.find(attrs={'data-coordinates': True})
        if coord_elem:
            coords = coord_elem.get('data-coordinates')
            if coords:
                return coords

        return None

    def _detect_yandex_parking_type(self, soup: BeautifulSoup, name: str, html: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø–∞—Ä–∫–æ–≤–∫–∏ –¥–ª—è –Ø–Ω–¥–µ–∫—Å"""
        text = (name + ' ' + soup.get_text()).lower()
        type_info = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–ª–∞—Ç–Ω–æ—Å—Ç–∏
        if any(word in text for word in ['–ø–ª–∞—Ç–Ω', '–æ–ø–ª–∞—Ç', '—Ç–∞—Ä–∏—Ñ', '—Ü–µ–Ω–∞', '‚ÇΩ', '—Ä—É–±']):
            type_info.append('–ø–ª–∞—Ç–Ω–∞—è')
        elif any(word in text for word in ['–±–µ—Å–ø–ª–∞—Ç–Ω', 'free', 'gratis']):
            type_info.append('–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è')

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞
        if any(word in text for word in ['–∫—Ä—ã—Ç', '–∑–∞–∫—Ä—ã—Ç', '–æ—Ö—Ä–∞–Ω—è', '–ø–æ–¥–∑–µ–º–Ω']):
            type_info.append('–∫—Ä—ã—Ç–∞—è')
            type_info.append('–æ—Ö—Ä–∞–Ω—è–µ–º–∞—è')
        elif any(word in text for word in ['—É–ª–∏—á–Ω', '–æ—Ç–∫—Ä—ã—Ç', '–≥–æ—Å—Ç']):
            type_info.append('—É–ª–∏—á–Ω–∞—è')

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if any(word in text for word in ['—Ç–æ—Ä–≥–æ–≤', '—Ç—Ü', '–º–æ–ª–ª', '–≥–∞–ª–µ—Ä–µ—è']):
            type_info.append('–ø—Ä–∏ —Ç—Ü')
        elif any(word in text for word in ['–æ—Ñ–∏c', '–±–∏–∑–Ω–µ—Å', '—Ü–µ–Ω—Ç—Ä']):
            type_info.append('–±–∏–∑–Ω–µ—Å-—Ü–µ–Ω—Ç—Ä')

        return ", ".join(type_info) if type_info else "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
