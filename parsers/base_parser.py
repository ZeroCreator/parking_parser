import asyncio
import random
import re
import time
from abc import ABC, abstractmethod
from typing import Optional, List, Dict, Any, Set
from datetime import datetime

import nodriver
from bs4 import BeautifulSoup


class BaseParser(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤"""

    def __init__(self, headless: bool = True):
        self.headless = headless
        self.browser: Optional[nodriver.Browser] = None
        self.results: List[Dict[str, Any]] = []
        self.start_time = None
        self.all_urls: Set[str] = set()
        self.max_consecutive_no_new = 3  # –ú–∞–∫—Å–∏–º—É–º 3 –ø–æ–ø—ã—Ç–∫–∏ –±–µ–∑ –Ω–æ–≤—ã—Ö URL

    # === –û–ë–©–ò–ï –ú–ï–¢–û–î–´ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–ò ===

    async def init_browser(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±—Ä–∞—É–∑–µ—Ä–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–∞—Å–∫–∏—Ä–æ–≤–∫–æ–π"""
        try:
            print(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –±—Ä–∞—É–∑–µ—Ä (headless={self.headless})...")

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—Ö–æ–¥–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏
            args = [
                "--disable-blink-features=AutomationControlled",
                "--disable-features=IsolateOrigins,site-per-process",
                "--disable-web-security",
                "--disable-dev-shm-usage",
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-gpu",
                "--disable-software-rasterizer",
                "--disable-extensions",
                "--disable-background-networking",
                "--disable-default-apps",
                "--disable-sync",
                "--disable-translate",
                "--metrics-recording-only",
                "--no-first-run",
                "--mute-audio",
                "--hide-scrollbars",
                "--disable-notifications",
                "--disable-popup-blocking",
                "--disable-background-timer-throttling",
                "--disable-backgrounding-occluded-windows",
                "--disable-breakpad",
                "--disable-component-extensions-with-background-pages",
                "--disable-features=TranslateUI",
                "--disable-ipc-flooding-protection",
                "--disable-renderer-backgrounding",
                "--enable-automation",
                "--password-store=basic",
                "--use-mock-keychain",
                f"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            ]

            self.browser = await nodriver.start(
                headless=self.headless,
                window_size=(1200, 900),
                disable_features=[],
                args=args
            )

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: —Å–∫—Ä—ã–≤–∞–µ–º WebDriver —Ñ–ª–∞–≥–∏ —á–µ—Ä–µ–∑ JavaScript
            page = await self.browser.get('about:blank')
            await page.evaluate("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });

                window.chrome = {
                    runtime: {}
                };
            """)

            print("‚úÖ –ë—Ä–∞—É–∑–µ—Ä –∑–∞–ø—É—â–µ–Ω")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            return False

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞"""
        print("\nüîÑ –ó–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É –ø–∞—Ä—Å–µ—Ä–∞...")
        if self.browser:
            try:
                self.browser = None
                print("‚úÖ –†–µ—Å—É—Ä—Å—ã –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω—ã")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏: {e}")
                self.browser = None

    # === –û–ë–©–ò–ï –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ ===

    async def random_delay(self, min_seconds: float = 1, max_seconds: float = 3):
        """–°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
        delay = random.uniform(min_seconds, max_seconds)
        await asyncio.sleep(delay)

    def _shorten_url(self, url: str, max_length: int = 60) -> str:
        """–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ URL –¥–ª—è –≤—ã–≤–æ–¥–∞"""
        if len(url) <= max_length:
            return url
        return url[:max_length - 3] + "..."

    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return ""
        text = ' '.join(text.split())
        return text.strip()

    def _safe_get_text(self, element, default: str = "") -> str:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ BeautifulSoup —ç–ª–µ–º–µ–Ω—Ç–∞"""
        if element:
            text = element.get_text(' ', strip=True)
            return ' '.join(text.split())
        return default

    def _is_loading_element_visible(self, soup: BeautifulSoup) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∏–¥–∏–º–æ—Å—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∑–∞–≥—Ä—É–∑–∫–∏"""
        loading_selectors = [
            '.spin2', '.spinner', '.loading', '.loader',
            '[class*="loading"]', '[class*="spinner"]'
        ]

        for selector in loading_selectors:
            if soup.select_one(selector):
                return True
        return False

    # === –ú–ï–¢–û–î–´ –ü–ê–†–°–ò–ù–ì–ê –°–¢–†–ê–ù–ò–¶ –û–ë–™–ï–ö–¢–û–í ===

    async def _parse_all_parking_pages(self, urls: List[str]) -> None:
        """–û–±—â–∏–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –æ–±—ä–µ–∫—Ç–æ–≤"""
        print(f"\nüè¢ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {len(urls)} –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ {self.source_name}...")

        success_count = 0
        fail_count = 0

        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] –û–±—ä–µ–∫—Ç {i}")
            print(f"   üîó {self._shorten_url(url, 60)}")

            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            data = await self._parse_single_page(url)

            if data:
                normalized_data = self.normalize_data(data)
                self.results.append(normalized_data)
                success_count += 1

                # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                name = normalized_data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:40]
                address = normalized_data.get('–ê–¥—Ä–µ—Å', '')[:50]
                parking_type = normalized_data.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')

                print(f"   ‚úÖ {name}")
                print(f"      üìç {address}")
                print(f"      üöó –¢–∏–ø: {parking_type}")
            else:
                fail_count += 1
                print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if i % 10 == 0 or i == len(urls):
                progress = (i / len(urls)) * 100
                elapsed = time.time() - self.start_time
                estimated_total = (elapsed / max(1, i)) * len(urls)
                remaining = max(0, estimated_total - elapsed)

                print(f"\nüìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(urls)} ({progress:.1f}%)")
                print(f"‚è± –ü—Ä–æ—à–ª–æ: {elapsed:.0f}—Å | –û—Å—Ç–∞–ª–æ—Å—å: {remaining:.0f}—Å")
                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {success_count} | ‚ùå –û—à–∏–±–æ–∫: {fail_count}")

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < len(urls):
                delay = random.uniform(4, 7)
                print(f"   ‚è≥ –ó–∞–¥–µ—Ä–∂–∫–∞ {delay:.1f}—Å...")
                await asyncio.sleep(delay)

        print(f"\nüéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üìä –ò—Ç–æ–≥: –£—Å–ø–µ—à–Ω–æ {success_count}, –û—à–∏–±–æ–∫ {fail_count}")

    async def _parse_single_page(self, url: str) -> Optional[Dict[str, Any]]:
        """–û–±—â–∏–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—ä–µ–∫—Ç–∞"""
        max_retries = 2

        for attempt in range(1, max_retries + 1):
            try:
                if attempt > 1:
                    print(f"   üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}")
                    await asyncio.sleep(random.uniform(3, 5))

                # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –Ω–æ–≤–æ–º —Ç–∞–±–µ
                tab = await self.browser.get(url)

                # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏
                await asyncio.sleep(random.uniform(3, 4))

                # –ü–æ–ª—É—á–∞–µ–º HTML
                html = await tab.get_content()

                # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ
                soup = BeautifulSoup(str(html), 'lxml')

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
                data = self._extract_page_data(url, soup, str(html))

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                if data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞') or data.get('–ê–¥—Ä–µ—Å'):
                    return data
                else:
                    print(f"   ‚ö† –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")

            except Exception as e:
                error_msg = str(e)
                print(f"   ‚úó –û—à–∏–±–∫–∞: {error_msg[:50]}...")

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
            if attempt < max_retries:
                retry_delay = random.uniform(5, 8)
                await asyncio.sleep(retry_delay)

        return None

    # === –ú–ï–¢–û–î–´ –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–ò –ò –û–ë–†–ê–ë–û–¢–ö–ò ===

    def normalize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –µ–¥–∏–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        normalized = {
            '–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞': data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', ''),
            '–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã': data.get('–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã', ''),
            '–ê–¥—Ä–µ—Å': data.get('–ê–¥—Ä–µ—Å', ''),
            '–¢–µ–ª–µ—Ñ–æ–Ω': data.get('–¢–µ–ª–µ—Ñ–æ–Ω', ''),
            '–°–∞–π—Ç': data.get('–°–∞–π—Ç', ''),
            '–¢–∏–ø –æ–±—ä–µ–∫—Ç–∞': data.get('–¢–∏–ø –æ–±—ä–µ–∫—Ç–∞', ''),
            '–°—Å—ã–ª–∫–∞': data.get('–°—Å—ã–ª–∫–∞', ''),
            '–ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä–∫–æ–≤–∫–∏': data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä–∫–æ–≤–∫–∏', ''),
            '–°—Å—ã–ª–∫–∞ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫—É': data.get('–°—Å—ã–ª–∫–∞ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫—É', ''),
            '–ê–¥—Ä–µ—Å –ø–∞—Ä–∫–æ–≤–∫–∏': data.get('–ê–¥—Ä–µ—Å –ø–∞—Ä–∫–æ–≤–∫–∏', ''),
            '–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏': data.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', ''),
            '–î–æ—Å—Ç—É–ø': data.get('–î–æ—Å—Ç—É–ø', ''),
            '–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã': data.get('–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', ''),
            '–¢–∞—Ä–∏—Ñ—ã': data.get('–¢–∞—Ä–∏—Ñ—ã', ''),
            '–¶–µ–Ω—ã': data.get('–¶–µ–Ω—ã', ''),
            '–í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å': data.get('–í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å', ''),
            '–û—Ü–µ–Ω–∫–∞': data.get('–û—Ü–µ–Ω–∫–∞', ''),
            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫': data.get('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫', ''),
            '–û–ø–∏—Å–∞–Ω–∏–µ': data.get('–û–ø–∏—Å–∞–Ω–∏–µ', ''),
            'source': self.source_name,
            'timestamp': data.get('timestamp', datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        }

        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        for key, value in normalized.items():
            if isinstance(value, str):
                value = ' '.join(value.split())
                normalized[key] = value

        return normalized

    def _remove_duplicates(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not self.results:
            return

        unique_results = []
        seen_keys = set()

        for item in self.results:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á
            name = item.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '').strip()
            address = item.get('–ê–¥—Ä–µ—Å', '').strip()
            url = item.get('–°—Å—ã–ª–∫–∞ –Ω–∞ –æ–±—ä–µ–∫—Ç') or item.get('–°—Å—ã–ª–∫–∞', '')

            if url:
                key = f"{self.source_name}_{url}"
            elif name and address:
                key = f"{self.source_name}_{name[:30]}_{address[:30]}"
            else:
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—ä–µ–∫—Ç—ã –±–µ–∑ –∫–ª—é—á–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

            if key not in seen_keys:
                seen_keys.add(key)
                unique_results.append(item)

        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"üóë –£–¥–∞–ª–µ–Ω–æ {removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ {self.source_name}")

        self.results = unique_results

    def _print_final_stats(self, urls_collected: int = None):
        """–û–±—â–∏–π –º–µ—Ç–æ–¥ –≤—ã–≤–æ–¥–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        print("\n" + "=" * 60)
        print(f"üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê {self.source_name.upper()}")
        print("=" * 60)

        if self.start_time:
            elapsed = time.time() - self.start_time
            minutes = int(elapsed // 60)
            seconds = int(elapsed % 60)
            print(f"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è: {minutes} –º–∏–Ω {seconds} —Å–µ–∫")

        if urls_collected:
            print(f"üîó –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {urls_collected}")

        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ: {len(self.results)}")

        if urls_collected and len(self.results) > 0:
            efficiency = len(self.results) / max(1, urls_collected) * 100
            print(f"üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞—Ä—Å–∏–Ω–≥–∞: {efficiency:.1f}%")

        if self.results:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –ø–∞—Ä–∫–æ–≤–æ–∫
            closed_count = len([p for p in self.results if '–∑–∞–∫—Ä—ã—Ç' in p.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '').lower()])
            paid_count = len([p for p in self.results if '–ø–ª–∞—Ç–Ω' in p.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '').lower()])
            guarded_count = len([p for p in self.results if '–æ—Ö—Ä–∞–Ω—è' in p.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '').lower()])

            print(f"\nüöó –¢–ò–ü–´ –ü–ê–†–ö–û–í–û–ö:")
            print(f"   –ó–∞–∫—Ä—ã—Ç—ã—Ö/–æ—Ö—Ä–∞–Ω—è–µ–º—ã—Ö: {closed_count}")
            print(f"   –ü–ª–∞—Ç–Ω—ã—Ö: {paid_count}")

            # –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
            with_coords = len([p for p in self.results if p.get('–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã')])
            with_address = len([p for p in self.results if p.get('–ê–¥—Ä–µ—Å')])
            with_phone = len([p for p in self.results if p.get('–¢–µ–ª–µ—Ñ–æ–Ω')])

            print(f"\nüìä –ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–•:")
            print(
                f"   –° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {with_coords}/{len(self.results)} ({with_coords / len(self.results) * 100:.1f}%)")
            print(f"   –° –∞–¥—Ä–µ—Å–∞–º–∏: {with_address}/{len(self.results)} ({with_address / len(self.results) * 100:.1f}%)")
            print(f"   –° —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏: {with_phone}/{len(self.results)} ({with_phone / len(self.results) * 100:.1f}%)")

        print("=" * 60)

    # === –ê–ë–°–¢–†–ê–ö–¢–ù–´–ï –ú–ï–¢–û–î–´ ===

    @property
    @abstractmethod
    def source_name(self) -> str:
        """–ò–º—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        pass

    @abstractmethod
    async def parse(self) -> List[Dict[str, Any]]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        pass

    @abstractmethod
    def _extract_page_data(self, url: str, soup: BeautifulSoup, html: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—ä–µ–∫—Ç–∞"""
        pass
