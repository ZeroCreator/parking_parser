import asyncio
import random
import re
import hashlib
import time
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
from urllib.parse import urlparse, parse_qs, urlencode

from bs4 import BeautifulSoup
import nodriver

from .base_parser import BaseParser


class TwoGisParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä 2–ì–ò–° —Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –Ω–∞ –∑–æ–Ω—ã."""

    def __init__(self, headless: bool = True):
        super().__init__(headless)
        self.processed_ids: Set[str] = set()
        self.session_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
        }

    @property
    def source_name(self) -> str:
        return "2gis"

    async def parse(self, max_pages: int = 30) -> List[Dict[str, Any]]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ 2–ì–ò–° —Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –Ω–∞ –∑–æ–Ω—ã"""
        print("=" * 60)
        print("üöÄ –ó–ê–ü–£–°–ö –ü–ê–†–°–ï–†–ê 2–ì–ò–° (–ó–û–ù–ò–†–û–í–ê–ù–ù–´–ô)")
        print("=" * 60)

        self.start_time = time.time()
        self.results = []

        if not await self.init_browser():
            return []

        try:
            # 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–æ–Ω—ã –¥–ª—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–∞
            print(f"\nüéØ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ó–û–ù –î–õ–Ø –°–ê–ù–ö–¢-–ü–ï–¢–ï–†–ë–£–†–ì–ê")
            print("-" * 50)

            search_areas = self.generate_grid_z14()
            print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∑–æ–Ω: {len(search_areas)}")

            # 2. –ü–∞—Ä—Å–∏–º –≤—Å–µ –∑–æ–Ω—ã –≥–æ—Ä–æ–¥–∞
            print(f"\nüìÑ –≠–¢–ê–ü 1: –°–ë–û–† –í–°–ï–• –°–°–´–õ–û–ö –ù–ê –ü–ê–†–ö–û–í–ö–ò –ü–û –ó–û–ù–ê–ú")
            print("-" * 50)

            for i, area in enumerate(search_areas, 1):
                urls_before = len(self.all_urls)

                print(f"\nüìç –ó–æ–Ω–∞ {i}/{len(search_areas)}: {area['name']}")
                print(f"   –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã: {area['coords'][1]:.4f}¬∞N, {area['coords'][0]:.4f}¬∞E")
                print(f"   –ú–∞—Å—à—Ç–∞–±: z={area['zoom']}")
                print(f"   URL: {area['url']}")

                # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∑–æ–Ω—ã
                await self._collect_urls_from_zone(
                    area['url'],
                    area['name'],
                    area['coords'],
                    area['zoom']
                )

                new_urls = len(self.all_urls) - urls_before
                print(f"‚úÖ –í –∑–æ–Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–∞—Ä–∫–æ–≤–æ–∫: {new_urls}")
                print(f"üìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Å—Å—ã–ª–æ–∫: {len(self.all_urls)}")

                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–æ–Ω–∞–º–∏
                if i < len(search_areas):
                    await asyncio.sleep(random.uniform(5, 8))

            if not self.all_urls:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫–∏")
                return []

            print(f"\n‚úÖ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(self.all_urls)}")

            # 3. –ü–∞—Ä—Å–∏–º –≤—Å–µ —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –ø–∞—Ä–∫–æ–≤–∫–∏
            print("\nüè¢ –≠–¢–ê–ü 2: –ü–ê–†–°–ò–ù–ì –í–°–ï–• –°–û–ë–†–ê–ù–ù–´–• –ü–ê–†–ö–û–í–û–ö")
            print("-" * 50)

            urls_list = list(self.all_urls)
            await self._parse_all_parking_pages(urls_list)

            # 4. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._remove_duplicates()
            self._print_final_stats(len(self.all_urls))

            return self.results

        except Exception as e:
            print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            import traceback
            traceback.print_exc()
            return self.results
        finally:
            await self.close()

    def generate_grid_z14(self) -> List[Dict[str, Any]]:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–µ—Ç–∫—É –∑–æ–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (z=14).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –¥–ª—è –ø–æ–∏—Å–∫–∞, –ø–æ–∫—Ä—ã–≤–∞—é—â–∏—Ö –≤–µ—Å—å –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥.
        """
        # –ì—Ä–∞–Ω–∏—Ü—ã –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–∞ –¥–ª—è 2GIS
        LAT_MIN, LAT_MAX = 59.85, 60.05  # –ù–µ–º–Ω–æ–≥–æ —Ä–∞—Å—à–∏—Ä—è–µ–º –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ö–≤–∞—Ç–∞
        LON_MIN, LON_MAX = 30.15, 30.70  # –ó–∞–ø–∞–¥-–í–æ—Å—Ç–æ–∫

        # –®–∞–≥ —Å–µ—Ç–∫–∏ –¥–ª—è z=14
        LAT_STEP = 0.04  # ~4.4 –∫–º
        LON_STEP = 0.06  # ~3.8 –∫–º –Ω–∞ —à–∏—Ä–æ—Ç–µ –°–ü–±
        ZOOM = 14  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—Å—à—Ç–∞–±

        zones = []
        zone_counter = 1

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å–µ—Ç–∫–∏
        lat = LAT_MIN
        while lat < LAT_MAX:
            lon = LON_MIN
            while lon < LON_MAX:
                # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∫–æ–≤–æ–∫ –≤ —ç—Ç–æ–π –∑–æ–Ω–µ
                # 2GIS –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã m –¥–ª—è –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: lon,lat,zoom
                url = f"https://2gis.ru/spb/search/parking/?m={lon:.6f}%2C{lat:.6f}%2F{ZOOM}"

                zones.append({
                    "name": f"–ó–æ–Ω–∞ {zone_counter}",
                    "url": url,
                    "coords": (lon, lat),
                    "zoom": ZOOM
                })

                zone_counter += 1
                lon += LON_STEP
            lat += LAT_STEP

        print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(zones)} –∑–æ–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (z={ZOOM})")
        print(f"üìê –®–∞–≥ —Å–µ—Ç–∫–∏: {LON_STEP:.3f}¬∞ (–¥–æ–ª–≥–æ—Ç–∞) √ó {LAT_STEP:.3f}¬∞ (—à–∏—Ä–æ—Ç–∞)")
        print(f"üìç –û—Ö–≤–∞—Ç—ã–≤–∞–µ–º–∞—è –æ–±–ª–∞—Å—Ç—å: {LAT_MIN}-{LAT_MAX}¬∞N, {LON_MIN}-{LON_MAX}¬∞E")

        return zones

    async def _collect_urls_from_zone(self, zone_url: str, zone_name: str, coords: tuple, zoom: int) -> bool:
        """–°–±–æ—Ä URL –ø–∞—Ä–∫–æ–≤–æ–∫ –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–æ–Ω—ã"""
        try:
            print(f"   üîç –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ –≤ –∑–æ–Ω–µ: {zone_name}")

            # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∑–æ–Ω—ã
            tab = await self.browser.get(zone_url)
            await asyncio.sleep(random.uniform(4, 6))

            # –ö–ª–∏–∫–∞–µ–º –ø–æ –ø–æ–∏—Å–∫–æ–≤–æ–π –≤—ã–¥–∞—á–µ, –µ—Å–ª–∏ –µ—Å—Ç—å
            await self._click_search_results_if_needed(tab)

            # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            print("   üì• –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
            initial_urls = await self._get_urls_from_current_page(tab)
            if initial_urls:
                self.all_urls.update(initial_urls)
                print(f"   üìä –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {len(initial_urls)} URL")
            else:
                print("   ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫–∏ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã")

            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            print("   üìú –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–∫—Ä—É—Ç–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
            await self._scroll_2gis_to_bottom(tab)

            # –°–æ–±–∏—Ä–∞–µ–º –í–°–ï URL –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏
            current_urls = await self._get_urls_from_current_page(tab)
            if current_urls:
                previous_count = len(self.all_urls)
                self.all_urls.update(current_urls)
                new_urls = len(self.all_urls) - previous_count
                print(f"   üìé –í—Å–µ–≥–æ URL –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏: {len(self.all_urls)} (+{new_urls} –Ω–æ–≤—ã—Ö)")

            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–Ω–æ–ø–∫—É –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ (–ø–µ—Ä–µ–¥–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–æ–Ω—ã)
            print("   üîç –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–Ω–æ–ø–∫—É –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏...")
            await self._try_find_2gis_pagination_after_scroll(
                tab,
                coords,
                zoom,
                current_page=1
            )

            print(f"   ‚úÖ –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –≤ –∑–æ–Ω–µ {zone_name} –∑–∞–≤–µ—Ä—à–µ–Ω")
            return True

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –≤ –∑–æ–Ω–µ {zone_name}: {str(e)[:100]}")
            return False

    async def _click_search_results_if_needed(self, tab):
        """–ö–ª–∏–∫–∞–µ—Ç –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ø–æ–∏—Å–∫–∞, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å"""
        try:
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            await asyncio.sleep(2)

            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–Ω–æ–ø–∫—É –∏–ª–∏ —ç–ª–µ–º–µ–Ω—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            selectors = [
                '.searchResults',
                '.listContainer',
                '.searchResults__list',
                '.searchResults__container',
                '[data-qa="search-results"]',
                '.searchTab__content'
            ]

            for selector in selectors:
                element = await tab.query_selector(selector)
                if element:
                    print("   üñ± –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∫–ª–∏–∫–∞–µ–º...")
                    await element.click()
                    await asyncio.sleep(2)
                    break

            # –¢–∞–∫–∂–µ –ø—Ä–æ–±—É–µ–º –∫–ª–∏–∫–Ω—É—Ç—å –ø–æ –ø–µ—Ä–≤–æ–π –∫–∞—Ä—Ç–æ—á–∫–µ
            first_card = await tab.query_selector('.minicard')
            if first_card:
                await first_card.click()
                await asyncio.sleep(1)

        except Exception as e:
            print(f"   ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∫–ª–∏–∫–Ω—É—Ç—å –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º: {str(e)[:50]}")

    async def _scroll_2gis_to_bottom(self, tab):
        """–ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ—Ç –í–°–ï —Å–∫—Ä–æ–ª–ª–∏—Ä—É–µ–º—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ 2–ì–ò–°"""
        print("   üìú –°–ö–†–û–õ–õ–ò–ú –í–°–ï –ö–û–ù–¢–ï–ô–ù–ï–†–´...")

        try:
            # 1. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø—Ä–æ–∫—Ä—É—Ç–∏—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            await tab.evaluate("""
                (function() {
                    const mainContainers = [
                        '.searchResults__list',
                        '.listContainer',
                        '.searchResults__container',
                        '.scroll__container',
                        '[data-scroll]'
                    ];

                    for (const selector of mainContainers) {
                        const container = document.querySelector(selector);
                        if (container && container.scrollHeight > container.clientHeight) {
                            container.scrollTop = container.scrollHeight;
                            return { scrolled: true, selector: selector };
                        }
                    }
                    return { scrolled: false };
                })()
            """)
            await asyncio.sleep(random.uniform(1, 2))

            # 2. –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –æ–∫–Ω–æ –±—Ä–∞—É–∑–µ—Ä–∞
            await tab.evaluate("""
                window.scrollBy({
                    top: 800,
                    behavior: 'smooth'
                });
            """)
            await asyncio.sleep(random.uniform(1, 2))

            # 3. –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –≤—Å–µ —Å–∫—Ä–æ–ª–ª–∏—Ä—É–µ–º—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
            container_count = await tab.evaluate("""
                document.querySelectorAll('[data-scroll], [tabindex], [overflow="auto"], [overflow="scroll"]').length
            """)

            for i in range(container_count):
                await tab.evaluate(f"""
                    (function() {{
                        const containers = document.querySelectorAll('[data-scroll], [tabindex], [overflow="auto"], [overflow="scroll"]');
                        if (containers[{i}]) {{
                            const container = containers[{i}];
                            if (container.scrollHeight > container.clientHeight) {{
                                container.scrollTop = container.scrollHeight;
                            }}
                        }}
                    }})()
                """)
                await asyncio.sleep(0.3)

            await asyncio.sleep(random.uniform(2, 3))

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞: {str(e)[:100]}")

    async def _try_find_2gis_pagination_after_scroll(self, tab, coords: tuple, zoom: int, current_page: int = 1):
        """–ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –∫–Ω–æ–ø–∫–∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–æ–Ω—ã"""
        try:
            html = await tab.get_content()
            soup = BeautifulSoup(html, 'lxml')

            next_page_num = current_page + 1
            found_next_page = False

            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            for link in soup.find_all('a', href=True):
                href = link['href']

                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ 2GIS
                patterns = [
                    r'/page/(\d+)',
                    r'page=(\d+)',
                    r'pagination=(\d+)'
                ]

                for pattern in patterns:
                    match = re.search(pattern, href)
                    if match:
                        page_num = int(match.group(1))
                        if page_num == next_page_num:
                            print(f"   üñ± –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {next_page_num}")

                            # –í–ê–ñ–ù–û: –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ü–ê–†–ê–ú–ï–¢–†–ê–ú–ò –ó–û–ù–´
                            lon, lat = coords
                            base_url = f"https://2gis.ru/spb/search/parking/"

                            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–æ–Ω—ã (–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏ –º–∞—Å—à—Ç–∞–±)
                            params = {
                                'm': f"{lon:.6f},{lat:.6f}/{zoom}"
                            }

                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                            if '/page/' in href:
                                # –§–æ—Ä–º–∞—Ç: /page/2/
                                page_url = f"{base_url}page/{next_page_num}/?{urlencode(params)}"
                            else:
                                # –§–æ—Ä–º–∞—Ç: ?page=2
                                params['page'] = next_page_num
                                page_url = f"{base_url}?{urlencode(params)}"

                            print(f"   üìç URL —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∑–æ–Ω—ã: {page_url}")

                            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                            await tab.get(page_url)
                            await asyncio.sleep(random.uniform(4, 6))

                            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                            await self._scroll_2gis_to_bottom(tab)

                            # –°–æ–±–∏—Ä–∞–µ–º URL —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                            urls_page = await self._get_urls_from_current_page(tab)
                            if urls_page:
                                before = len(self.all_urls)
                                self.all_urls.update(urls_page)
                                new_count = len(self.all_urls) - before
                                print(f"   üìä +{new_count} –Ω–æ–≤—ã—Ö URL")

                            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏—â–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                            await self._try_find_2gis_pagination_after_scroll(
                                tab,
                                coords,
                                zoom,
                                next_page_num
                            )
                            found_next_page = True
                            break

                if found_next_page:
                    break

            if not found_next_page:
                # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–Ω–æ–ø–∫–∏ "–î–∞–ª—å—à–µ" –∏–ª–∏ "–°–ª–µ–¥—É—é—â–∞—è"
                next_buttons = soup.find_all(['button', 'a'], string=re.compile(r'–¥–∞–ª—å—à–µ|—Å–ª–µ–¥—É—é—â|next', re.I))

                for button in next_buttons:
                    print(f"   üñ± –ù–∞–π–¥–µ–Ω–∞ –∫–Ω–æ–ø–∫–∞ '–î–∞–ª—å—à–µ', –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {next_page_num}")

                    # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∑–æ–Ω—ã
                    lon, lat = coords
                    base_url = f"https://2gis.ru/spb/search/parking/"

                    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
                    formats_to_try = [
                        f"{base_url}page/{next_page_num}/?m={lon:.6f}%2C{lat:.6f}%2F{zoom}",
                        f"{base_url}?page={next_page_num}&m={lon:.6f}%2C{lat:.6f}%2F{zoom}"
                    ]

                    for page_url in formats_to_try:
                        try:
                            await tab.get(page_url)
                            await asyncio.sleep(random.uniform(4, 6))

                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å –ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                            current_url = await tab.evaluate("window.location.href")
                            if "parking" in current_url:
                                print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ—à–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {next_page_num}")

                                # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                                await self._scroll_2gis_to_bottom(tab)

                                # –°–æ–±–∏—Ä–∞–µ–º URL —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                                urls_page = await self._get_urls_from_current_page(tab)
                                if urls_page:
                                    before = len(self.all_urls)
                                    self.all_urls.update(urls_page)
                                    new_count = len(self.all_urls) - before
                                    print(f"   üìä +{new_count} –Ω–æ–≤—ã—Ö URL")

                                # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏—â–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                                await self._try_find_2gis_pagination_after_scroll(
                                    tab,
                                    coords,
                                    zoom,
                                    next_page_num
                                )
                                found_next_page = True
                                break

                        except Exception as e:
                            print(f"   ‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –ø–æ URL {page_url}: {str(e)[:50]}")
                            continue

                    if found_next_page:
                        break

            if not found_next_page:
                print(f"   ‚ö† –ù–µ—Ç –±–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ —ç—Ç–æ–π –∑–æ–Ω–µ (–¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page})")

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: {str(e)[:60]}")

    async def _get_urls_from_current_page(self, tab) -> Set[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ URL –ø–∞—Ä–∫–æ–≤–æ–∫ —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (2–ì–ò–°)"""
        try:
            await asyncio.sleep(1)
            html = await tab.get_content()

            urls = self._extract_2gis_urls_from_html(html)

            filtered_urls = set()
            for url in urls:
                if self._is_valid_2gis_url(url):
                    clean_url = self._clean_2gis_url(url)
                    if clean_url:
                        filtered_urls.add(clean_url)

            return filtered_urls

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ URL: {str(e)[:50]}")
            return set()

    def _extract_2gis_urls_from_html(self, html: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL –ø–∞—Ä–∫–æ–≤–æ–∫ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞ (2–ì–ò–°)"""
        soup = BeautifulSoup(html, 'lxml')
        urls = []

        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∏—Ä–º—ã
        firm_links = soup.select('a[href*="/firm/"]')

        for link in firm_links:
            href = link.get('href', '')
            if href and '/firm/' in href:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                if href.startswith('//'):
                    full_url = f"https:{href}"
                elif href.startswith('/'):
                    full_url = f"https://2gis.ru{href}"
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue

                clean_url = self._clean_2gis_url(full_url)
                if clean_url:
                    urls.append(clean_url)

        # –¢–∞–∫–∂–µ –∏—â–µ–º –≤ data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        for elem in soup.select('[data-id]'):
            data_id = elem.get('data-id', '')
            if data_id and data_id.startswith('firm_'):
                firm_id = data_id.replace('firm_', '')
                url = f"https://2gis.ru/spb/firm/{firm_id}"
                if url not in urls:
                    urls.append(url)

        # –ò—â–µ–º –≤ –º–∏–Ω–∏-–∫–∞—Ä—Ç–æ—á–∫–∞—Ö
        minicards = soup.select('.minicard')
        for card in minicards:
            link = card.select_one('a[href*="/firm/"]')
            if link:
                href = link.get('href', '')
                if href:
                    if href.startswith('//'):
                        full_url = f"https:{href}"
                    elif href.startswith('/'):
                        full_url = f"https://2gis.ru{href}"
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        continue

                    clean_url = self._clean_2gis_url(full_url)
                    if clean_url and clean_url not in urls:
                        urls.append(clean_url)

        return list(set(urls))

    def _is_valid_2gis_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL –ø–∞—Ä–∫–æ–≤–∫–∏ (2–ì–ò–°)"""
        if '/firm/' not in url:
            return False

        # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ URL
        exclude_patterns = [
            '/reviews',
            '/gallery',
            '/photos',
            '/menu',
            '/contacts',
            '/search/',
            'tab=',
            '#',
            'reviewTab',
            'photoTab'
        ]

        for pattern in exclude_patterns:
            if pattern in url:
                return False

        return True

    def _clean_2gis_url(self, url: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ URL –ø–∞—Ä–∫–æ–≤–∫–∏ (2–ì–ò–°)"""
        url = url.split('?')[0].split('#')[0]
        url = url.rstrip('/')

        if url.startswith('//'):
            url = f"https:{url}"
        elif url.startswith('/'):
            url = f"https://2gis.ru{url}"
        elif not url.startswith('http'):
            url = f"https://{url}"

        return url

    def _extract_page_data(self, url: str, soup: BeautifulSoup, html: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã 2–ì–ò–°"""
        data = {}

        # –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è
        data['–°—Å—ã–ª–∫–∞'] = url
        data['–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã'] = self.extract_coordinates(url) or ""

        # –ù–∞–∑–≤–∞–Ω–∏–µ
        title_selectors = [
            'h1',
            '[itemprop="name"]',
            '.firm-card__title',
            '.business-card-title',
            'h1[data-qa="firm-card-header-name"]'
        ]

        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(' ', strip=True)
                if text and len(text) > 2:
                    data['–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞'] = text
                    break

        # –ê–¥—Ä–µ—Å
        address_selectors = [
            'address',
            '[itemprop="address"]',
            '.address',
            '.firm-card__address',
            '[data-qa="firm-card-address"]'
        ]

        for selector in address_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(' ', strip=True)
                if text and len(text) > 5:
                    data['–ê–¥—Ä–µ—Å'] = text
                    break

        # –¢–µ–ª–µ—Ñ–æ–Ω—ã
        phones = []
        phone_selectors = [
            'a[href^="tel:"]',
            '[data-qa="phone"]',
            '.contact__phone',
            '.phone',
            '.firm-card__phone'
        ]

        for selector in phone_selectors:
            for link in soup.select(selector):
                href = link.get('href', '')
                text = link.get_text(strip=True)

                if href.startswith('tel:'):
                    phone = href.replace('tel:', '').strip()
                elif text:
                    phone = text
                else:
                    continue

                if phone:
                    clean_phone = re.sub(r'[^\d\+\(\)\s\-]', '', phone)
                    clean_phone = ' '.join(clean_phone.split())
                    if clean_phone and clean_phone not in phones:
                        phones.append(clean_phone)

        data['–¢–µ–ª–µ—Ñ–æ–Ω'] = ', '.join(phones) if phones else ""

        # –°–∞–π—Ç
        site_selectors = [
            'a[href^="http://"]:not([href*="2gis.ru"])',
            'a[href^="https://"]:not([href*="2gis.ru"])',
            '.contact__website',
            '.website',
            '[data-qa="website"]'
        ]

        for selector in site_selectors:
            for link in soup.select(selector):
                href = link.get('href', '')
                if href and '2gis.ru' not in href:
                    data['–°–∞–π—Ç'] = href
                    break

        # –¢–∏–ø –æ–±—ä–µ–∫—Ç–∞
        type_selectors = [
            '[itemprop="category"]',
            '.category',
            '.firm-card__category',
            '.business-card-category'
        ]

        for selector in type_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                if text:
                    data['–¢–∏–ø –æ–±—ä–µ–∫—Ç–∞'] = text
                    break

        # –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã
        hours_selectors = [
            '[itemprop="openingHours"]',
            '.working-hours',
            '.schedule',
            '.hours',
            '[data-qa="opening-hours"]'
        ]

        for selector in hours_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(' ', strip=True)
                if ':' in text or '—á–∞—Å' in text.lower() or '–æ—Ç–∫—Ä—ã—Ç' in text.lower():
                    data['–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã'] = text
                    break

        # –†–µ–π—Ç–∏–Ω–≥
        rating_selectors = [
            '[itemprop="ratingValue"]',
            '.rating',
            '.business-rating-badge',
            '[data-qa="rating"]'
        ]

        for selector in rating_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                if text:
                    try:
                        rating_match = re.search(r'[\d\.]+', text)
                        if rating_match:
                            data['–û—Ü–µ–Ω–∫–∞'] = rating_match.group(0)
                    except:
                        data['–û—Ü–µ–Ω–∫–∞'] = text
                    break

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤
        reviews_selectors = [
            '[itemprop="reviewCount"]',
            '.reviews-count',
            '.review-count',
            '[data-qa="reviews-count"]'
        ]

        for selector in reviews_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                if text:
                    reviews_match = re.search(r'\d+', text)
                    if reviews_match:
                        data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫'] = reviews_match.group(0)
                    break

        # –¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏
        data['–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏'] = self.detect_parking_type(html, data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', ''))

        # –¶–µ–Ω—ã –∏ —Ç–∞—Ä–∏—Ñ—ã
        price_text = soup.get_text(' ', strip=True)
        price_patterns = [
            r'(\d+[\s\u00A0]*—Ä—É–±[–ª–µ–π\.]*)',
            r'(\d+[\s\u00A0]*‚ÇΩ)',
            r'(\d+[\s\u00A0]*—Ä\.)',
            r'(\d+[\s\u00A0]*–≤ —á–∞—Å)',
            r'(\d+[\s\u00A0]*–≤ —Å—É—Ç–∫–∏)',
            r'(\d+[\s\u00A0]*–≤ –º–µ—Å—è—Ü)',
        ]

        prices = []
        for pattern in price_patterns:
            matches = re.findall(pattern, price_text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    price = match[0]
                else:
                    price = match

                if price and price not in prices:
                    prices.append(price.strip())

        if prices:
            data['–¢–∞—Ä–∏—Ñ—ã'] = '; '.join(prices[:3])
            data['–¶–µ–Ω—ã'] = prices[0] if prices else ""

        # –í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
        capacity_patterns = [
            r'(\d+)[\s\u00A0]*–º–µ—Å—Ç[–∞-—è]*',
            r'(\d+)[\s\u00A0]*–º–∞—à–∏–Ω–æ–º–µ—Å—Ç',
            r'–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å[\s:]*(\d+)',
            r'(\d+)[\s\u00A0]*–∞–≤—Ç–æ–º–æ–±–∏–ª[–µ–π—è]'
        ]

        for pattern in capacity_patterns:
            match = re.search(pattern, price_text, re.IGNORECASE)
            if match:
                data['–í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å'] = match.group(1)
                break

        # –û–ø–∏—Å–∞–Ω–∏–µ
        desc_selectors = [
            '.firm-card__description',
            '[itemprop="description"]',
            '.description',
            '.firm-description'
        ]

        for selector in desc_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(' ', strip=True)
                if text and len(text) > 20:
                    data['–û–ø–∏—Å–∞–Ω–∏–µ'] = text[:200]
                    break

        # –ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä–∫–æ–≤–∫–∏ (–¥—É–±–ª–∏—Ä—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞)
        data['–ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä–∫–æ–≤–∫–∏'] = data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '–ü–∞—Ä–∫–æ–≤–∫–∞')

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫—É
        if data['–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏'] == '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ':
            page_text = soup.get_text(' ', strip=True).lower()
            name_text = data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '').lower()

            parking_keywords = ['–ø–∞—Ä–∫–æ–≤–∫', '—Å—Ç–æ—è–Ω–∫', 'parking', '–∞–≤—Ç–æ—Å—Ç–æ—è–Ω–∫', '–ø–∞—Ä–∫–∏–Ω–≥']
            for keyword in parking_keywords:
                if keyword in page_text or keyword in name_text:
                    data['–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏'] = '–ø–∞—Ä–∫–æ–≤–∫–∞'
                    break

        return data

    def extract_coordinates(self, url: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∏–∑ URL (2–ì–ò–°)"""
        patterns = [
            r'@([\d\.]+),([\d\.]+)',
            r'll=([\d\.]+)%2C([\d\.]+)',
            r'/([\d\.]+)%2C([\d\.]+)/',
            r'm=([\d\.]+)%2C([\d\.]+)'
        ]

        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                lon, lat = match.groups()
                return f"{lon},{lat}"

        return None

    def detect_parking_type(self, html: str, name: str = "") -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø–∞—Ä–∫–æ–≤–∫–∏ (2–ì–ò–°)"""
        text = (html + " " + name).lower()
        type_info = []

        if any(word in text for word in ['–ø–ª–∞—Ç–Ω', '–æ–ø–ª–∞—Ç', '—Ç–∞—Ä–∏—Ñ', '—Ü–µ–Ω–∞', '‚ÇΩ', '—Ä—É–±']):
            type_info.append('–ø–ª–∞—Ç–Ω–∞—è')
        elif any(word in text for word in ['–±–µ—Å–ø–ª–∞—Ç–Ω', 'free', 'gratis']):
            type_info.append('–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è')

        if any(word in text for word in ['–∫—Ä—ã—Ç', '–∑–∞–∫—Ä—ã—Ç', '–æ—Ö—Ä–∞–Ω—è', '–ø–æ–¥–∑–µ–º–Ω']):
            type_info.append('–∫—Ä—ã—Ç–∞—è')
            type_info.append('–æ—Ö—Ä–∞–Ω—è–µ–º–∞—è')
        elif any(word in text for word in ['—É–ª–∏—á–Ω', '–æ—Ç–∫—Ä—ã—Ç', '–≥–æ—Å—Ç']):
            type_info.append('—É–ª–∏—á–Ω–∞—è')

        return ", ".join(type_info) if type_info else "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

    def _generate_parking_id(self, url: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID –¥–ª—è –ø–∞—Ä–∫–æ–≤–∫–∏"""
        match = re.search(r'/firm/(\d+)', url)
        if match:
            return f"2gis_{match.group(1)}"

        url_hash = hashlib.md5(url.encode()).hexdigest()[:10]
        return f"2gis_{url_hash}"

    def _print_final_stats(self, total_urls: int):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–±–æ—Ä–∞"""
        print("\n" + "=" * 80)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ë–û–†–ê 2–ì–ò–°")
        print("=" * 80)

        elapsed_time = time.time() - self.start_time
        minutes = int(elapsed_time // 60)
        seconds = int(elapsed_time % 60)

        print(f"‚è± –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {minutes} –º–∏–Ω {seconds} —Å–µ–∫")
        print(f"üîó –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {total_urls}")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(self.results)}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º
        phones_count = sum(1 for r in self.results if r.get('–¢–µ–ª–µ—Ñ–æ–Ω'))
        sites_count = sum(1 for r in self.results if r.get('–°–∞–π—Ç'))
        coords_count = sum(1 for r in self.results if r.get('–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã'))
        prices_count = sum(1 for r in self.results if r.get('–¶–µ–Ω—ã'))

        print(f"üìû –ü–∞—Ä–∫–æ–≤–æ–∫ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–æ–º: {phones_count}")
        print(f"üåê –ü–∞—Ä–∫–æ–≤–æ–∫ —Å —Å–∞–π—Ç–æ–º: {sites_count}")
        print(f"üìç –ü–∞—Ä–∫–æ–≤–æ–∫ —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {coords_count}")
        print(f"üí∞ –ü–∞—Ä–∫–æ–≤–æ–∫ —Å —Ü–µ–Ω–∞–º–∏: {prices_count}")

        # –¢–∏–ø—ã –ø–∞—Ä–∫–æ–≤–æ–∫
        types = {}
        for r in self.results:
            parking_type = r.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            types[parking_type] = types.get(parking_type, 0) + 1

        print("\nüè¢ –¢–ò–ü–´ –ü–ê–†–ö–û–í–û–ö:")
        for type_name, count in sorted(types.items(), key=lambda x: x[1], reverse=True):
            print(f"   {type_name}: {count}")

        print("\n" + "=" * 80)