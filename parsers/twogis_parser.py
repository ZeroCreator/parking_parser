"2Gis parser"

import asyncio
import random
import time
import re
import hashlib
from typing import List, Dict, Any, Optional, Set
from datetime import datetime

from bs4 import BeautifulSoup
import nodriver

from .base_parser import BaseParser


class TwoGisParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä 2–ì–ò–°."""

    def __init__(self, headless: bool = True):
        super().__init__(headless)
        self.processed_ids: Set[str] = set()
        self.start_time = None
        self.all_parking_urls: Set[str] = set()
        self.session_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
        }

    @property
    def source_name(self) -> str:
        return "2gis"

    #
    # –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ 2–ì–ò–°
    #

    async def parse(self, max_pages: int = 30) -> List[Dict[str, Any]]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ 2–ì–ò–°."""
        print("=" * 60)
        print("üöÄ –ó–ê–ü–£–°–ö –ü–ê–†–°–ï–†–ê 2–ì–ò–°")
        print("=" * 60)

        self.start_time = time.time()
        self.results = []

        if not await self.init_browser():
            return []

        try:
            # –≠–¢–ê–ü 1: –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Å—Å—ã–ª–∫–∏
            print(f"\nüìÑ –≠–¢–ê–ü 1: –°–ë–û–† –í–°–ï–• –°–°–´–õ–û–ö –ù–ê –ü–ê–†–ö–û–í–ö–ò")
            print("-" * 50)

            await self._collect_all_parking_urls_by_scroll_simple()

            if not self.all_parking_urls:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫–∏")
                return []

            print(f"\n‚úÖ –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫–∏: {len(self.all_parking_urls)}")

            # –≠–¢–ê–ü 2: –ü–∞—Ä—Å–∏–º –í–°–ï —Å–æ–±—Ä–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫–∏
            print("\nüè¢ –≠–¢–ê–ü 2: –ü–ê–†–°–ò–ù–ì –í–°–ï–• –°–û–ë–†–ê–ù–ù–´–• –ü–ê–†–ö–û–í–û–ö")
            print("-" * 50)

            urls_list = list(self.all_parking_urls)
            await self._parse_all_parking_pages(urls_list)

            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            self._print_final_stats(urls_list)

            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            self._remove_duplicates()

            return self.results

        except Exception as e:
            print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            import traceback
            traceback.print_exc()
            return self.results
        finally:
            await self.close()

    #
    # –°–±–æ—Ä –í–°–ï–• URL –ø–∞—Ä–∫–æ–≤–æ–∫
    #

    async def _collect_all_parking_urls_by_scroll_simple(self) -> bool:
        """–°–±–æ—Ä –í–°–ï–• URL –ø–∞—Ä–∫–æ–≤–æ–∫."""
        print("üîç –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫...")

        # –ù–∞—á–∏–Ω–∞–µ–º —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        start_url = "https://2gis.ru/spb/search/parking"

        print(f"üìç –ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {start_url}")

        tab = await self.browser.get(start_url)
        await asyncio.sleep(random.uniform(5, 7))

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        print("   üì• –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
        initial_urls = await self._get_urls_from_current_page(tab)
        if initial_urls:
            self.all_parking_urls.update(initial_urls)
            print(f"   üìä –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {len(initial_urls)} URL")
        else:
            print("   ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫–∏ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
            return False

        # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø–æ–¥–≥—Ä—É–∑–∫–∏ –Ω–æ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        print("   üìú –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–∫—Ä—É—Ç–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")

        # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ –¥–æ –∫–æ–Ω—Ü–∞
        await self._scroll_to_bottom(tab)

        # –°–æ–±–∏—Ä–∞–µ–º –í–°–ï URL –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏
        current_urls = await self._get_urls_from_current_page(tab)
        if current_urls:
            previous_count = len(self.all_parking_urls)
            self.all_parking_urls.update(current_urls)
            new_urls = len(self.all_parking_urls) - previous_count
            print(f"      üìé –í—Å–µ–≥–æ URL –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏: {len(self.all_parking_urls)} (+{new_urls} –Ω–æ–≤—ã—Ö)")

        # –ü–û–°–õ–ï –ü–†–û–ö–†–£–¢–ö–ò –î–û –ö–û–ù–¶–ê - –ü–†–û–ë–£–ï–ú –ù–ê–ô–¢–ò –ö–ù–û–ü–ö–£ –ü–ê–ì–ò–ù–ê–¶–ò–ò
        print("      üîç –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–Ω–æ–ø–∫—É –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏...")
        await self._try_find_pagination_after_scroll(tab)

        print(f"\n‚úÖ –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        print(f"üìä –ò—Ç–æ–≥: {len(self.all_parking_urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL")

        return len(self.all_parking_urls) > 0

    async def _scroll_to_bottom(self, tab):
        """–ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ—Ç –í–°–ï —Å–∫—Ä–æ–ª–ª–∏—Ä—É–µ–º—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ."""
        print("   üìú –°–ö–†–û–õ–õ–ò–ú –í–°–ï –ö–û–ù–¢–ï–ô–ù–ï–†–´...")

        try:
            current_url = await tab.evaluate("window.location.href")
            print(f"      üìç –°—Ç—Ä–∞–Ω–∏—Ü–∞: {current_url}")

            # 1. –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
            container_count = await tab.evaluate("""
                document.querySelectorAll('[data-scroll], [tabindex], [overflow="auto"], [overflow="scroll"]').length
            """)

            # 2. –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –ö–ê–ñ–î–´–ô –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
            for i in range(container_count):
                await tab.evaluate(f"""
                    (function() {{
                        const containers = document.querySelectorAll('[data-scroll], [tabindex], [overflow="auto"], [overflow="scroll"]');
                        if (containers[{i}]) {{
                            const container = containers[{i}];
                            // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–Ω–æ –ª–∏ —Å–∫—Ä–æ–ª–ª–∏—Ç—å
                            if (container.scrollHeight > container.clientHeight) {{
                                console.log('–°–∫—Ä–æ–ª–ª–∏–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä', container.tagName, container.className);
                                container.scrollTop = container.scrollHeight;
                            }}
                        }}
                    }})()
                """)

                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å–∫—Ä–æ–ª–ª–∞–º–∏
                await asyncio.sleep(0.5)

            # 3. –ñ–¥–µ–º
            await asyncio.sleep(random.uniform(2, 3))

        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞: {str(e)[:100]}")

    async def _try_find_pagination_after_scroll(self, tab, current_page: int = 1):
        """–ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –∫–Ω–æ–ø–∫–∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            html = await tab.get_content()
            soup = BeautifulSoup(html, 'lxml')

            next_page_num = current_page + 1
            found_next_page = False

            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            for link in soup.find_all('a', href=True):
                href = link['href']
                match = re.search(r'/page/(\d+)', href)
                if match:
                    page_num = int(match.group(1))
                    if page_num == next_page_num:
                        print(f"      üñ± –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {next_page_num}")

                        try:
                            selector = f'a[href*="/page/{next_page_num}"]'
                            element = await tab.query_selector(selector)

                            if element:
                                await element.click()
                                await asyncio.sleep(random.uniform(4, 6))

                                await self._scroll_to_bottom(tab)

                                urls_page = await self._get_urls_from_current_page(tab)
                                if urls_page:
                                    before = len(self.all_parking_urls)
                                    self.all_parking_urls.update(urls_page)
                                    new_count = len(self.all_parking_urls) - before
                                    print(f"      üìä +{new_count} –Ω–æ–≤—ã—Ö URL")

                                await self._try_find_pagination_after_scroll(tab, next_page_num)
                                found_next_page = True
                                break

                        except Exception as e:
                            print(f"      ‚ùå –û—à–∏–±–∫–∞: {str(e)[:60]}")
                        break

            if not found_next_page:
                print(f"      ‚ö† –ù–µ—Ç –±–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü")

        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞: {str(e)[:60]}")

    async def _collect_from_remaining_pages(self, tab, start_page: int = 3, max_pages: int = 20):
        """–°–±–æ—Ä —Å—Å—ã–ª–æ–∫ —Å –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å—Ç—Ä–∞–Ω–∏—Ü."""
        print(f"      üîÑ –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä —Å –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å—Ç—Ä–∞–Ω–∏—Ü (—Å {start_page})...")

        for page_num in range(start_page, max_pages + 1):
            print(f"      üìÑ –ò—â–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num}...")

            # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            current_urls = await self._get_urls_from_current_page(tab)
            if current_urls:
                before = len(self.all_parking_urls)
                self.all_parking_urls.update(current_urls)
                new_count = len(self.all_parking_urls) - before
                print(f"      üìä –°–æ–±—Ä–∞–Ω–æ: {len(current_urls)} URL (+{new_count} –Ω–æ–≤—ã—Ö)")

            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            selector = f'a[href*="/page/{page_num}"]'
            element = await tab.query_selector(selector)

            if element:
                print(f"      ‚úÖ –ù–∞—à–ª–∏ —ç–ª–µ–º–µ–Ω—Ç –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")

                try:
                    href = await element.get_attribute('href')
                    print(f"      üîó HREF: {href}")
                except:
                    pass

                # –ö–ª–∏–∫–∞–µ–º
                print(f"      üñ± –ö–ª–∏–∫–∞–µ–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num}...")
                await element.click()
                print(f"      ‚úÖ –ö–ª–∏–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω")

                # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏
                await asyncio.sleep(random.uniform(3, 5))
            else:
                print(f"      ‚ùå –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ–º")
                break

        # –°–æ–±–∏—Ä–∞–µ–º —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        print(f"      üì• –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
        last_urls = await self._get_urls_from_current_page(tab)
        if last_urls:
            before = len(self.all_parking_urls)
            self.all_parking_urls.update(last_urls)
            new_count = len(self.all_parking_urls) - before
            print(f"      üìä –° –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: +{new_count} –Ω–æ–≤—ã—Ö URL")

        print(f"      ‚úÖ –°–±–æ—Ä —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∑–∞–≤–µ—Ä—à–µ–Ω")

    async def _get_urls_from_current_page(self, tab) -> Set[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ URL –ø–∞—Ä–∫–æ–≤–æ–∫ —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        try:
            await asyncio.sleep(1)
            html = await tab.get_content()

            urls = self._extract_urls_from_html(html)

            filtered_urls = set()
            for url in urls:
                if self._is_valid_parking_url(url):
                    clean_url = self._clean_parking_url(url)
                    if clean_url:
                        filtered_urls.add(clean_url)

            return filtered_urls

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ URL: {str(e)[:50]}")
            return set()

    def _extract_urls_from_html(self, html: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL –ø–∞—Ä–∫–æ–≤–æ–∫ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞."""
        soup = BeautifulSoup(html, 'lxml')
        urls = []

        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∏—Ä–º—ã
        firm_links = soup.select('a[href*="/firm/"]')

        for link in firm_links:
            href = link.get('href', '')
            if href and '/firm/' in href:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                if href.startswith('//'):
                    full_url = f"https:{href}"
                elif href.startswith('/'):
                    full_url = f"https://2gis.ru{href}"
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue

                # –û—á–∏—â–∞–µ–º URL
                clean_url = self._clean_parking_url(full_url)
                if clean_url:
                    urls.append(clean_url)

        # –¢–∞–∫–∂–µ –∏—â–µ–º –≤ data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        for elem in soup.select('[data-id]'):
            data_id = elem.get('data-id', '')
            if data_id and data_id.startswith('firm_'):
                firm_id = data_id.replace('firm_', '')
                url = f"https://2gis.ru/spb/firm/{firm_id}"
                if url not in urls:
                    urls.append(url)

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        return list(set(urls))

    def _is_valid_parking_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL –ø–∞—Ä–∫–æ–≤–∫–∏"""
        # –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å /firm/ –∏ ID
        if '/firm/' not in url:
            return False

        # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ URL
        exclude_patterns = [
            '/reviews',
            '/gallery',
            '/photos',
            '/menu',
            '/contacts',
            '/search/',
            'tab=',
            '#'
        ]

        for pattern in exclude_patterns:
            if pattern in url:
                return False

        return True

    def _clean_parking_url(self, url: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ URL –ø–∞—Ä–∫–æ–≤–∫–∏"""
        # –£–¥–∞–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —è–∫–æ—Ä—è
        url = url.split('?')[0].split('#')[0]

        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–ª—ç—à–∏
        url = url.rstrip('/')

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —ç—Ç–æ –ø–æ–ª–Ω—ã–π URL
        if url.startswith('//'):
            url = f"https:{url}"
        elif url.startswith('/'):
            url = f"https://2gis.ru{url}"
        elif not url.startswith('http'):
            url = f"https://{url}"

        return url

    def _short_url(self, url: str, max_length: int = 60) -> str:
        """–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ URL –¥–ª—è –≤—ã–≤–æ–¥–∞"""
        if len(url) <= max_length:
            return url
        return url[:max_length - 3] + "..."

    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
    async def _parse_all_parking_pages(self, urls: List[str]):
        """–ü–∞—Ä—Å–∏–Ω–≥ –í–°–ï–• —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞—Ä–∫–æ–≤–æ–∫"""
        print(f"\nüè¢ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {len(urls)} –ø–∞—Ä–∫–æ–≤–æ–∫ 2–ì–ò–°...")

        success_count = 0
        fail_count = 0

        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] –ü–∞—Ä–∫–æ–≤–∫–∞ {i}")
            print(f"   üîó {self._short_url(url, 60)}")

            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            data = await self._parse_single_parking_page(url)

            if data:
                normalized_data = self.normalize_data(data)
                self.results.append(normalized_data)
                success_count += 1

                # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                name = normalized_data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:40]
                address = normalized_data.get('–ê–¥—Ä–µ—Å', '')[:50]
                parking_type = normalized_data.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')

                print(f"   ‚úÖ {name}")
                print(f"      üìç {address}")
                print(f"      üöó –¢–∏–ø: {parking_type}")
            else:
                fail_count += 1
                print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if i % 10 == 0 or i == len(urls):
                progress = (i / len(urls)) * 100
                elapsed = time.time() - self.start_time
                estimated_total = (elapsed / max(1, i)) * len(urls)
                remaining = max(0, estimated_total - elapsed)

                print(f"\nüìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(urls)} ({progress:.1f}%)")
                print(f"‚è± –ü—Ä–æ—à–ª–æ: {elapsed:.0f}—Å | –û—Å—Ç–∞–ª–æ—Å—å: {remaining:.0f}—Å")
                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {success_count} | ‚ùå –û—à–∏–±–æ–∫: {fail_count}")

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < len(urls):
                delay = random.uniform(4, 7)
                print(f"   ‚è≥ –ó–∞–¥–µ—Ä–∂–∫–∞ {delay:.1f}—Å...")
                await asyncio.sleep(delay)

        print(f"\nüéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üìä –ò—Ç–æ–≥: –£—Å–ø–µ—à–Ω–æ {success_count}, –û—à–∏–±–æ–∫ {fail_count}")

    #
    # –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∫–æ–≤–∫–∏
    #

    async def _parse_single_parking_page(self, url: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∫–æ–≤–∫–∏."""
        max_retries = 2

        for attempt in range(1, max_retries + 1):
            try:
                if attempt > 1:
                    print(f"   üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}")
                    await asyncio.sleep(random.uniform(3, 5))

                # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–∞—Ä–∫–æ–≤–∫–∏ –≤ –Ω–æ–≤–æ–º —Ç–∞–±–µ
                tab = await self.browser.get(url)

                # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏
                await asyncio.sleep(random.uniform(3, 4))

                # –ü–æ–ª—É—á–∞–µ–º HTML
                html = await tab.get_content()

                # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ
                soup = BeautifulSoup(str(html), 'lxml')
                data = self.extract_data(url, soup, str(html))

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                if data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞') or data.get('–ê–¥—Ä–µ—Å'):
                    return data
                else:
                    print(f"   ‚ö† –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")

            except Exception as e:
                error_msg = str(e)
                print(f"   ‚úó –û—à–∏–±–∫–∞: {error_msg[:50]}...")

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
            if attempt < max_retries:
                retry_delay = random.uniform(5, 8)
                await asyncio.sleep(retry_delay)

        return None

    def extract_data(self, url: str, soup: BeautifulSoup, html: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã 2–ì–ò–°."""
        data = {}

        # –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è
        data['–°—Å—ã–ª–∫–∞'] = url
        data['–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã'] = self.extract_coordinates(url) or ""

        # –ù–∞–∑–≤–∞–Ω–∏–µ
        title_selectors = [
            'h1',
            '[itemprop="name"]',
            '.firm-card__title',
            '.business-card-title',
            'h1[data-qa="firm-card-header-name"]'
        ]

        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(' ', strip=True)
                if text and len(text) > 2:
                    data['–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞'] = text
                    break

        # –ê–¥—Ä–µ—Å
        address_selectors = [
            'address',
            '[itemprop="address"]',
            '.address',
            '.firm-card__address',
            '[data-qa="firm-card-address"]'
        ]

        for selector in address_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(' ', strip=True)
                if text and len(text) > 5:
                    data['–ê–¥—Ä–µ—Å'] = text
                    break

        # –¢–µ–ª–µ—Ñ–æ–Ω—ã
        phones = []
        phone_selectors = [
            'a[href^="tel:"]',
            '[data-qa="phone"]',
            '.contact__phone',
            '.phone',
            '.firm-card__phone'
        ]

        for selector in phone_selectors:
            for link in soup.select(selector):
                href = link.get('href', '')
                text = link.get_text(strip=True)

                if href.startswith('tel:'):
                    phone = href.replace('tel:', '').strip()
                elif text:
                    phone = text
                else:
                    continue

                if phone:
                    clean_phone = re.sub(r'[^\d\+\(\)\s\-]', '', phone)
                    clean_phone = ' '.join(clean_phone.split())
                    if clean_phone and clean_phone not in phones:
                        phones.append(clean_phone)

        data['–¢–µ–ª–µ—Ñ–æ–Ω'] = ', '.join(phones) if phones else ""

        # –°–∞–π—Ç
        site_selectors = [
            'a[href^="http://"]:not([href*="2gis.ru"])',
            'a[href^="https://"]:not([href*="2gis.ru"])',
            '.contact__website',
            '.website',
            '[data-qa="website"]'
        ]

        for selector in site_selectors:
            for link in soup.select(selector):
                href = link.get('href', '')
                if href and '2gis.ru' not in href:
                    data['–°–∞–π—Ç'] = href
                    break

        # –¢–∏–ø –æ–±—ä–µ–∫—Ç–∞ / –∫–∞—Ç–µ–≥–æ—Ä–∏—è
        type_selectors = [
            '[itemprop="category"]',
            '.category',
            '.firm-card__category',
            '.business-card-category'
        ]

        for selector in type_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                if text:
                    data['–¢–∏–ø –æ–±—ä–µ–∫—Ç–∞'] = text
                    break

        # –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã
        hours_selectors = [
            '[itemprop="openingHours"]',
            '.working-hours',
            '.schedule',
            '.hours',
            '[data-qa="opening-hours"]'
        ]

        for selector in hours_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(' ', strip=True)
                if ':' in text or '—á–∞—Å' in text.lower() or '–æ—Ç–∫—Ä—ã—Ç' in text.lower():
                    data['–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã'] = text
                    break

        # –†–µ–π—Ç–∏–Ω–≥
        rating_selectors = [
            '[itemprop="ratingValue"]',
            '.rating',
            '.business-rating-badge',
            '[data-qa="rating"]'
        ]

        for selector in rating_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                if text:
                    try:
                        rating_match = re.search(r'[\d\.]+', text)
                        if rating_match:
                            data['–û—Ü–µ–Ω–∫–∞'] = rating_match.group(0)
                    except:
                        data['–û—Ü–µ–Ω–∫–∞'] = text
                    break

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤
        reviews_selectors = [
            '[itemprop="reviewCount"]',
            '.reviews-count',
            '.review-count',
            '[data-qa="reviews-count"]'
        ]

        for selector in reviews_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                if text:
                    reviews_match = re.search(r'\d+', text)
                    if reviews_match:
                        data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫'] = reviews_match.group(0)
                    break

        # –¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏
        data['–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏'] = self.detect_parking_type(html, data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', ''))

        # –¶–µ–Ω—ã –∏ —Ç–∞—Ä–∏—Ñ—ã
        price_text = soup.get_text(' ', strip=True)
        price_patterns = [
            r'(\d+[\s\u00A0]*—Ä—É–±[–ª–µ–π\.]*)',
            r'(\d+[\s\u00A0]*‚ÇΩ)',
            r'(\d+[\s\u00A0]*—Ä\.)',
            r'(\d+[\s\u00A0]*–≤ —á–∞—Å)',
            r'(\d+[\s\u00A0]*–≤ —Å—É—Ç–∫–∏)',
            r'(\d+[\s\u00A0]*–≤ –º–µ—Å—è—Ü)',
        ]

        prices = []
        for pattern in price_patterns:
            matches = re.findall(pattern, price_text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    price = match[0]
                else:
                    price = match

                if price and price not in prices:
                    prices.append(price.strip())

        if prices:
            data['–¢–∞—Ä–∏—Ñ—ã'] = '; '.join(prices[:3])
            data['–¶–µ–Ω—ã'] = prices[0] if prices else ""

        # –í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
        capacity_patterns = [
            r'(\d+)[\s\u00A0]*–º–µ—Å—Ç[–∞-—è]*',
            r'(\d+)[\s\u00A0]*–º–∞—à–∏–Ω–æ–º–µ—Å—Ç',
            r'–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å[\s:]*(\d+)',
            r'(\d+)[\s\u00A0]*–∞–≤—Ç–æ–º–æ–±–∏–ª[–µ–π—è]'
        ]

        for pattern in capacity_patterns:
            match = re.search(pattern, price_text, re.IGNORECASE)
            if match:
                data['–í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å'] = match.group(1)
                break

        # –û–ø–∏—Å–∞–Ω–∏–µ
        desc_selectors = [
            '.firm-card__description',
            '[itemprop="description"]',
            '.description',
            '.firm-description'
        ]

        for selector in desc_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(' ', strip=True)
                if text and len(text) > 20:
                    data['–û–ø–∏—Å–∞–Ω–∏–µ'] = text[:200]
                    break

        # –ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä–∫–æ–≤–∫–∏ (–¥—É–±–ª–∏—Ä—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞)
        data['–ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä–∫–æ–≤–∫–∏'] = data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '–ü–∞—Ä–∫–æ–≤–∫–∞')

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫—É
        if data['–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏'] == '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ':
            page_text = soup.get_text(' ', strip=True).lower()
            name_text = data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '').lower()

            parking_keywords = ['–ø–∞—Ä–∫–æ–≤–∫', '—Å—Ç–æ—è–Ω–∫', 'parking', '–∞–≤—Ç–æ—Å—Ç–æ—è–Ω–∫', '–ø–∞—Ä–∫–∏–Ω–≥']
            for keyword in parking_keywords:
                if keyword in page_text or keyword in name_text:
                    data['–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏'] = '–ø–∞—Ä–∫–æ–≤–∫–∞'
                    break

        return data

    def _generate_parking_id(self, url: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID –¥–ª—è –ø–∞—Ä–∫–æ–≤–∫–∏."""
        match = re.search(r'/firm/(\d+)', url)
        if match:
            return f"2gis_{match.group(1)}"

        url_hash = hashlib.md5(url.encode()).hexdigest()[:10]
        return f"2gis_{url_hash}"

    def _remove_duplicates(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
        if not self.results:
            return

        unique_results = []
        seen_ids = set()

        for item in self.results:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á
            name = item.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '').strip()
            address = item.get('–ê–¥—Ä–µ—Å', '').strip()
            url = item.get('–°—Å—ã–ª–∫–∞ –Ω–∞ –æ–±—ä–µ–∫—Ç') or item.get('–°—Å—ã–ª–∫–∞', '')

            if url:
                parking_id = self._generate_parking_id(url)
                key = parking_id
            elif name and address:
                key = f"{name[:30]}_{address[:30]}"
            else:
                data_str = str(item)
                key = hashlib.md5(data_str.encode()).hexdigest()[:12]

            if key not in seen_ids:
                seen_ids.add(key)
                unique_results.append(item)

        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"üóë –£–¥–∞–ª–µ–Ω–æ {removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ 2–ì–ò–°")

        self.results = unique_results

    #
    # –í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    #

    def _print_final_stats(self, all_urls: List[str]):
        """–í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏."""
        print("\n" + "=" * 60)
        print("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê 2–ì–ò–°")
        print("=" * 60)

        elapsed = time.time() - self.start_time
        minutes = int(elapsed // 60)
        seconds = int(elapsed % 60)

        print(f"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è: {minutes} –º–∏–Ω {seconds} —Å–µ–∫")
        print(f"üîó –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {len(all_urls)}")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ: {len(self.results)}")

        if all_urls:
            efficiency = len(self.results) / max(1, len(all_urls)) * 100
            print(f"üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞—Ä—Å–∏–Ω–≥–∞: {efficiency:.1f}%")

        if self.results:
            closed_count = 0
            paid_count = 0

            for item in self.results:
                parking_type = item.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '').lower()
                if '–∫—Ä—ã—Ç' in parking_type or '–æ—Ö—Ä–∞–Ω—è' in parking_type:
                    closed_count += 1
                if '–ø–ª–∞—Ç–Ω' in parking_type:
                    paid_count += 1

            print(f"\nüöó –¢–∏–ø—ã –ø–∞—Ä–∫–æ–≤–æ–∫ 2–ì–ò–°:")
            print(f"   –ó–∞–∫—Ä—ã—Ç—ã—Ö/–æ—Ö—Ä–∞–Ω—è–µ–º—ã—Ö: {closed_count}")
            print(f"   –ü–ª–∞—Ç–Ω—ã—Ö: {paid_count}")

            if len(self.results) >= 3:
                print(f"\nüèÜ –ü—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∫–æ–≤–æ–∫:")
                for i, item in enumerate(self.results[:3], 1):
                    name = item.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:40]
                    address = item.get('–ê–¥—Ä–µ—Å', '')[:50]
                    parking_type = item.get('–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                    print(f"   {i}. {name}")
                    print(f"      –ê–¥—Ä–µ—Å: {address}")
                    print(f"      –¢–∏–ø: {parking_type}")

        print("=" * 60)