import asyncio
import random
import re
import hashlib
import time
from typing import List, Dict, Any, Optional, Set
from datetime import datetime

from bs4 import BeautifulSoup
import nodriver

from .base_parser import BaseParser


class TwoGisParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä 2–ì–ò–°."""

    def __init__(self, headless: bool = True):
        super().__init__(headless)
        self.processed_ids: Set[str] = set()
        self.session_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
        }

    @property
    def source_name(self) -> str:
        return "2gis"

    async def parse(self, max_pages: int = 30) -> List[Dict[str, Any]]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ 2–ì–ò–°"""
        print("=" * 60)
        print("üöÄ –ó–ê–ü–£–°–ö –ü–ê–†–°–ï–†–ê 2–ì–ò–°")
        print("=" * 60)

        self.start_time = time.time()
        self.results = []

        if not await self.init_browser():
            return []

        try:
            # 1. –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏
            print(f"\nüìÑ –≠–¢–ê–ü 1: –°–ë–û–† –í–°–ï–• –°–°–´–õ–û–ö –ù–ê –ü–ê–†–ö–û–í–ö–ò")
            print("-" * 50)

            await self._collect_all_parking_urls()

            if not self.all_urls:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫–∏")
                return []

            print(f"\n‚úÖ –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫–∏: {len(self.all_urls)}")

            # 2. –ü–∞—Ä—Å–∏–º –≤—Å–µ —Å–æ–±—Ä–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            print("\nüè¢ –≠–¢–ê–ü 2: –ü–ê–†–°–ò–ù–ì –í–°–ï–• –°–û–ë–†–ê–ù–ù–´–• –ü–ê–†–ö–û–í–û–ö")
            print("-" * 50)

            urls_list = list(self.all_urls)
            await self._parse_all_parking_pages(urls_list)

            # 3. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._remove_duplicates()
            self._print_final_stats(len(self.all_urls))

            return self.results

        except Exception as e:
            print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            import traceback
            traceback.print_exc()
            return self.results
        finally:
            await self.close()

    async def _collect_all_parking_urls(self) -> bool:
        """–°–±–æ—Ä URL –ø–∞—Ä–∫–æ–≤–æ–∫ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –¥–ª—è 2–ì–ò–°)"""
        print("üîç –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫...")

        # –ù–∞—á–∏–Ω–∞–µ–º —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        start_url = "https://2gis.ru/spb/search/parking"
        print(f"üìç –ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {start_url}")

        tab = await self.browser.get(start_url)
        await asyncio.sleep(random.uniform(5, 7))

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        print("   üì• –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
        initial_urls = await self._get_urls_from_current_page(tab)
        if initial_urls:
            self.all_urls.update(initial_urls)
            print(f"   üìä –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {len(initial_urls)} URL")
        else:
            print("   ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫–∏ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
            return False

        # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        print("   üìú –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–∫—Ä—É—Ç–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
        await self._scroll_2gis_to_bottom(tab)

        # –°–æ–±–∏—Ä–∞–µ–º –í–°–ï URL –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏
        current_urls = await self._get_urls_from_current_page(tab)
        if current_urls:
            previous_count = len(self.all_urls)
            self.all_urls.update(current_urls)
            new_urls = len(self.all_urls) - previous_count
            print(f"      üìé –í—Å–µ–≥–æ URL –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏: {len(self.all_urls)} (+{new_urls} –Ω–æ–≤—ã—Ö)")

        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–Ω–æ–ø–∫—É –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        print("      üîç –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–Ω–æ–ø–∫—É –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏...")
        await self._try_find_2gis_pagination_after_scroll(tab)

        print(f"\n‚úÖ –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω")
        print(f"üìä –ò—Ç–æ–≥: {len(self.all_urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL")
        return len(self.all_urls) > 0

    async def _scroll_2gis_to_bottom(self, tab):
        """–ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ—Ç –í–°–ï —Å–∫—Ä–æ–ª–ª–∏—Ä—É–µ–º—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ 2–ì–ò–°"""
        print("   üìú –°–ö–†–û–õ–õ–ò–ú –í–°–ï –ö–û–ù–¢–ï–ô–ù–ï–†–´...")

        try:
            # 1. –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
            container_count = await tab.evaluate("""
                document.querySelectorAll('[data-scroll], [tabindex], [overflow="auto"], [overflow="scroll"]').length
            """)

            # 2. –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –ö–ê–ñ–î–´–ô –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
            for i in range(container_count):
                await tab.evaluate(f"""
                    (function() {{
                        const containers = document.querySelectorAll('[data-scroll], [tabindex], [overflow="auto"], [overflow="scroll"]');
                        if (containers[{i}]) {{
                            const container = containers[{i}];
                            if (container.scrollHeight > container.clientHeight) {{
                                container.scrollTop = container.scrollHeight;
                            }}
                        }}
                    }})()
                """)
                await asyncio.sleep(0.5)

            await asyncio.sleep(random.uniform(2, 3))

        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞: {str(e)[:100]}")

    async def _try_find_2gis_pagination_after_scroll(self, tab, current_page: int = 1):
        """–ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –∫–Ω–æ–ø–∫–∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏ (2–ì–ò–°)"""
        try:
            html = await tab.get_content()
            soup = BeautifulSoup(html, 'lxml')

            next_page_num = current_page + 1
            found_next_page = False

            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            for link in soup.find_all('a', href=True):
                href = link['href']
                match = re.search(r'/page/(\d+)', href)
                if match:
                    page_num = int(match.group(1))
                    if page_num == next_page_num:
                        print(f"      üñ± –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {next_page_num}")

                        selector = f'a[href*="/page/{next_page_num}"]'
                        element = await tab.query_selector(selector)

                        if element:
                            await element.click()
                            await asyncio.sleep(random.uniform(4, 6))

                            await self._scroll_2gis_to_bottom(tab)

                            urls_page = await self._get_urls_from_current_page(tab)
                            if urls_page:
                                before = len(self.all_urls)
                                self.all_urls.update(urls_page)
                                new_count = len(self.all_urls) - before
                                print(f"      üìä +{new_count} –Ω–æ–≤—ã—Ö URL")

                            await self._try_find_2gis_pagination_after_scroll(tab, next_page_num)
                            found_next_page = True
                            break

            if not found_next_page:
                print(f"      ‚ö† –ù–µ—Ç –±–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü")

        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞: {str(e)[:60]}")

    async def _get_urls_from_current_page(self, tab) -> Set[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ URL –ø–∞—Ä–∫–æ–≤–æ–∫ —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (2–ì–ò–°)"""
        try:
            await asyncio.sleep(1)
            html = await tab.get_content()

            urls = self._extract_2gis_urls_from_html(html)

            filtered_urls = set()
            for url in urls:
                if self._is_valid_2gis_url(url):
                    clean_url = self._clean_2gis_url(url)
                    if clean_url:
                        filtered_urls.add(clean_url)

            return filtered_urls

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ URL: {str(e)[:50]}")
            return set()

    def _extract_2gis_urls_from_html(self, html: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL –ø–∞—Ä–∫–æ–≤–æ–∫ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞ (2–ì–ò–°)"""
        soup = BeautifulSoup(html, 'lxml')
        urls = []

        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∏—Ä–º—ã
        firm_links = soup.select('a[href*="/firm/"]')

        for link in firm_links:
            href = link.get('href', '')
            if href and '/firm/' in href:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                if href.startswith('//'):
                    full_url = f"https:{href}"
                elif href.startswith('/'):
                    full_url = f"https://2gis.ru{href}"
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue

                clean_url = self._clean_2gis_url(full_url)
                if clean_url:
                    urls.append(clean_url)

        # –¢–∞–∫–∂–µ –∏—â–µ–º –≤ data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        for elem in soup.select('[data-id]'):
            data_id = elem.get('data-id', '')
            if data_id and data_id.startswith('firm_'):
                firm_id = data_id.replace('firm_', '')
                url = f"https://2gis.ru/spb/firm/{firm_id}"
                if url not in urls:
                    urls.append(url)

        return list(set(urls))

    def _is_valid_2gis_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL –ø–∞—Ä–∫–æ–≤–∫–∏ (2–ì–ò–°)"""
        if '/firm/' not in url:
            return False

        # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ URL
        exclude_patterns = [
            '/reviews',
            '/gallery',
            '/photos',
            '/menu',
            '/contacts',
            '/search/',
            'tab=',
            '#'
        ]

        for pattern in exclude_patterns:
            if pattern in url:
                return False

        return True

    def _clean_2gis_url(self, url: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ URL –ø–∞—Ä–∫–æ–≤–∫–∏ (2–ì–ò–°)"""
        url = url.split('?')[0].split('#')[0]
        url = url.rstrip('/')

        if url.startswith('//'):
            url = f"https:{url}"
        elif url.startswith('/'):
            url = f"https://2gis.ru{url}"
        elif not url.startswith('http'):
            url = f"https://{url}"

        return url

    def _extract_page_data(self, url: str, soup: BeautifulSoup, html: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã 2–ì–ò–°"""
        data = {}

        # –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è
        data['–°—Å—ã–ª–∫–∞'] = url
        data['–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã'] = self.extract_coordinates(url) or ""

        # –ù–∞–∑–≤–∞–Ω–∏–µ
        title_selectors = [
            'h1',
            '[itemprop="name"]',
            '.firm-card__title',
            '.business-card-title',
            'h1[data-qa="firm-card-header-name"]'
        ]

        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(' ', strip=True)
                if text and len(text) > 2:
                    data['–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞'] = text
                    break

        # –ê–¥—Ä–µ—Å
        address_selectors = [
            'address',
            '[itemprop="address"]',
            '.address',
            '.firm-card__address',
            '[data-qa="firm-card-address"]'
        ]

        for selector in address_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(' ', strip=True)
                if text and len(text) > 5:
                    data['–ê–¥—Ä–µ—Å'] = text
                    break

        # –¢–µ–ª–µ—Ñ–æ–Ω—ã
        phones = []
        phone_selectors = [
            'a[href^="tel:"]',
            '[data-qa="phone"]',
            '.contact__phone',
            '.phone',
            '.firm-card__phone'
        ]

        for selector in phone_selectors:
            for link in soup.select(selector):
                href = link.get('href', '')
                text = link.get_text(strip=True)

                if href.startswith('tel:'):
                    phone = href.replace('tel:', '').strip()
                elif text:
                    phone = text
                else:
                    continue

                if phone:
                    clean_phone = re.sub(r'[^\d\+\(\)\s\-]', '', phone)
                    clean_phone = ' '.join(clean_phone.split())
                    if clean_phone and clean_phone not in phones:
                        phones.append(clean_phone)

        data['–¢–µ–ª–µ—Ñ–æ–Ω'] = ', '.join(phones) if phones else ""

        # –°–∞–π—Ç
        site_selectors = [
            'a[href^="http://"]:not([href*="2gis.ru"])',
            'a[href^="https://"]:not([href*="2gis.ru"])',
            '.contact__website',
            '.website',
            '[data-qa="website"]'
        ]

        for selector in site_selectors:
            for link in soup.select(selector):
                href = link.get('href', '')
                if href and '2gis.ru' not in href:
                    data['–°–∞–π—Ç'] = href
                    break

        # –¢–∏–ø –æ–±—ä–µ–∫—Ç–∞
        type_selectors = [
            '[itemprop="category"]',
            '.category',
            '.firm-card__category',
            '.business-card-category'
        ]

        for selector in type_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                if text:
                    data['–¢–∏–ø –æ–±—ä–µ–∫—Ç–∞'] = text
                    break

        # –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã
        hours_selectors = [
            '[itemprop="openingHours"]',
            '.working-hours',
            '.schedule',
            '.hours',
            '[data-qa="opening-hours"]'
        ]

        for selector in hours_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(' ', strip=True)
                if ':' in text or '—á–∞—Å' in text.lower() or '–æ—Ç–∫—Ä—ã—Ç' in text.lower():
                    data['–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã'] = text
                    break

        # –†–µ–π—Ç–∏–Ω–≥
        rating_selectors = [
            '[itemprop="ratingValue"]',
            '.rating',
            '.business-rating-badge',
            '[data-qa="rating"]'
        ]

        for selector in rating_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                if text:
                    try:
                        rating_match = re.search(r'[\d\.]+', text)
                        if rating_match:
                            data['–û—Ü–µ–Ω–∫–∞'] = rating_match.group(0)
                    except:
                        data['–û—Ü–µ–Ω–∫–∞'] = text
                    break

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤
        reviews_selectors = [
            '[itemprop="reviewCount"]',
            '.reviews-count',
            '.review-count',
            '[data-qa="reviews-count"]'
        ]

        for selector in reviews_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                if text:
                    reviews_match = re.search(r'\d+', text)
                    if reviews_match:
                        data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫'] = reviews_match.group(0)
                    break

        # –¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏
        data['–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏'] = self.detect_parking_type(html, data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', ''))

        # –¶–µ–Ω—ã –∏ —Ç–∞—Ä–∏—Ñ—ã
        price_text = soup.get_text(' ', strip=True)
        price_patterns = [
            r'(\d+[\s\u00A0]*—Ä—É–±[–ª–µ–π\.]*)',
            r'(\d+[\s\u00A0]*‚ÇΩ)',
            r'(\d+[\s\u00A0]*—Ä\.)',
            r'(\d+[\s\u00A0]*–≤ —á–∞—Å)',
            r'(\d+[\s\u00A0]*–≤ —Å—É—Ç–∫–∏)',
            r'(\d+[\s\u00A0]*–≤ –º–µ—Å—è—Ü)',
        ]

        prices = []
        for pattern in price_patterns:
            matches = re.findall(pattern, price_text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    price = match[0]
                else:
                    price = match

                if price and price not in prices:
                    prices.append(price.strip())

        if prices:
            data['–¢–∞—Ä–∏—Ñ—ã'] = '; '.join(prices[:3])
            data['–¶–µ–Ω—ã'] = prices[0] if prices else ""

        # –í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
        capacity_patterns = [
            r'(\d+)[\s\u00A0]*–º–µ—Å—Ç[–∞-—è]*',
            r'(\d+)[\s\u00A0]*–º–∞—à–∏–Ω–æ–º–µ—Å—Ç',
            r'–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å[\s:]*(\d+)',
            r'(\d+)[\s\u00A0]*–∞–≤—Ç–æ–º–æ–±–∏–ª[–µ–π—è]'
        ]

        for pattern in capacity_patterns:
            match = re.search(pattern, price_text, re.IGNORECASE)
            if match:
                data['–í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å'] = match.group(1)
                break

        # –û–ø–∏—Å–∞–Ω–∏–µ
        desc_selectors = [
            '.firm-card__description',
            '[itemprop="description"]',
            '.description',
            '.firm-description'
        ]

        for selector in desc_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(' ', strip=True)
                if text and len(text) > 20:
                    data['–û–ø–∏—Å–∞–Ω–∏–µ'] = text[:200]
                    break

        # –ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä–∫–æ–≤–∫–∏ (–¥—É–±–ª–∏—Ä—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞)
        data['–ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä–∫–æ–≤–∫–∏'] = data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '–ü–∞—Ä–∫–æ–≤–∫–∞')

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–∞—Ä–∫–æ–≤–∫—É
        if data['–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏'] == '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ':
            page_text = soup.get_text(' ', strip=True).lower()
            name_text = data.get('–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞', '').lower()

            parking_keywords = ['–ø–∞—Ä–∫–æ–≤–∫', '—Å—Ç–æ—è–Ω–∫', 'parking', '–∞–≤—Ç–æ—Å—Ç–æ—è–Ω–∫', '–ø–∞—Ä–∫–∏–Ω–≥']
            for keyword in parking_keywords:
                if keyword in page_text or keyword in name_text:
                    data['–¢–∏–ø –ø–∞—Ä–∫–æ–≤–∫–∏'] = '–ø–∞—Ä–∫–æ–≤–∫–∞'
                    break

        return data

    def extract_coordinates(self, url: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∏–∑ URL (2–ì–ò–°)"""
        patterns = [
            r'@([\d\.]+),([\d\.]+)',
            r'll=([\d\.]+)%2C([\d\.]+)',
            r'/([\d\.]+)%2C([\d\.]+)/',
        ]

        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                lon, lat = match.groups()
                return f"{lon},{lat}"

        return None

    def detect_parking_type(self, html: str, name: str = "") -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø–∞—Ä–∫–æ–≤–∫–∏ (2–ì–ò–°)"""
        text = (html + " " + name).lower()
        type_info = []

        if any(word in text for word in ['–ø–ª–∞—Ç–Ω', '–æ–ø–ª–∞—Ç', '—Ç–∞—Ä–∏—Ñ', '—Ü–µ–Ω–∞', '‚ÇΩ', '—Ä—É–±']):
            type_info.append('–ø–ª–∞—Ç–Ω–∞—è')
        elif any(word in text for word in ['–±–µ—Å–ø–ª–∞—Ç–Ω', 'free', 'gratis']):
            type_info.append('–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è')

        if any(word in text for word in ['–∫—Ä—ã—Ç', '–∑–∞–∫—Ä—ã—Ç', '–æ—Ö—Ä–∞–Ω—è', '–ø–æ–¥–∑–µ–º–Ω']):
            type_info.append('–∫—Ä—ã—Ç–∞—è')
            type_info.append('–æ—Ö—Ä–∞–Ω—è–µ–º–∞—è')
        elif any(word in text for word in ['—É–ª–∏—á–Ω', '–æ—Ç–∫—Ä—ã—Ç', '–≥–æ—Å—Ç']):
            type_info.append('—É–ª–∏—á–Ω–∞—è')

        return ", ".join(type_info) if type_info else "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

    def _generate_parking_id(self, url: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID –¥–ª—è –ø–∞—Ä–∫–æ–≤–∫–∏"""
        match = re.search(r'/firm/(\d+)', url)
        if match:
            return f"2gis_{match.group(1)}"

        url_hash = hashlib.md5(url.encode()).hexdigest()[:10]
        return f"2gis_{url_hash}"
